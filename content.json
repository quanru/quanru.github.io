{"posts":[{"title":"16宫格拖拽","text":"实现一个如图所示的16宫格页面，其中各个数字盒子之间是能相互拖拽，并交换位置的。而横纵各自的标题栏ABC与XYZ实现的功能则是，ABC（XYZ）之间两两互换位置，从而引起两列（行）一起调换位置。 16宫格拖拽实现一个如图所示的16宫格页面，其中各个数字盒子之间是能相互拖拽，并交换位置的。而横纵各自的标题栏ABC与XYZ实现的功能则是，ABC（XYZ）之间两两互换位置，从而引起两列（行）一起调换位置。 外观处理 设置外层容器与内部方块宽度，并向左浮动，从而构成16宫格; 每个盒子的位置以坐标(x,y)来表示，其中0&lt;x,y&lt;4; 1234567891011121314151617#container { position: relative; width: 400px; height: 400px; background: #eee;}.box { float: left; width: 70px; height: 70px; margin: 5px; font-weight: bold; line-height: 70px; text-align: center; border: 10px solid red; border-radius: 10px;} 由于拖拽使用的是绝对定位，因此首先获取当前各个盒子的偏移量，然后将盒子的position属性设为absolute，使用对应的偏移量设置top与left。 12345678910function absoluteThem(e) { $($(&quot;.box&quot;).toArray().reverse()).each(function(index, el) { $(this).css({ &quot;left&quot;: $(this).position().left, &quot;top&quot;: $(this).position().top, &quot;float&quot;: &quot;none&quot;, &quot;position&quot;: &quot;absolute&quot; }); });} 拖拽处理注意盒子越界检查以及标题栏的字母只能横向或纵向移动，并保存被拖拽盒子的坐标 12345678910111213141516171819202122232425262728293031323334353637function clickNum(e) {//点击数字 var targetEle = e.target, targetEleJQ = $(targetEle), oriX = e.clientX - targetEle.offsetLeft, //光标按下时相对本身的坐标 oriY = e.clientY - targetEle.offsetTop; if (targetEleJQ.hasClass(&quot;undraggable&quot;)) { return; } $(document).bind(&quot;mousemove&quot;, moveIt); $(document).bind(&quot;mouseup&quot;, mouseUp); function moveIt(e2) {//移动 var newX = e2.clientX - oriX, newY = e2.clientY - oriY, maxX = 400 - targetEle.offsetWidth - 10, maxY = 400 - targetEle.offsetHeight - 10; if (newX &lt; 100) { newX = 100; } else if (newX &gt; maxX) { newX = maxX; } if (newY &lt; 100) { newY = 100; } else if (newY &gt; maxY) { newY = maxY; } if(targetEleJQ.hasClass(&quot;num&quot;)){//如果是数字 targetEle.style.left = newX + &quot;px&quot;; targetEle.style.top = newY + &quot;px&quot;; } else if(targetEleJQ.hasClass(&quot;group1&quot;)){//如果是ABC栏 targetEle.style.left = newX + &quot;px&quot;; } else if(targetEleJQ.hasClass(&quot;group2&quot;)){//如果是XYZ栏 targetEle.style.top = newY + &quot;px&quot;; } } 放下盒子释放鼠标处的盒子坐标：将鼠标当前位置clientX与clientY分别对100求余，并向下取整，即可得到。 123456789101112131415161718192021222324252627282930313233343536373839404142434445function mouseUp(e3) { var boxLocX = Math.floor(e3.clientY/100),//放下光标时，所在的方格坐标 boxLocY = Math.floor(e3.clientX/100), oriBoxLocX = parseInt(targetEle.id.substr(4,1)),//原来的方格坐标 oriBoxLocY = parseInt(targetEle.id.substr(6,1)), boxNow = &quot;box-&quot; + boxLocX + &quot;-&quot; + boxLocY,//放下处的盒子id boxOri = &quot;box-&quot; + oriBoxLocX + &quot;-&quot; + oriBoxLocY; if(targetEleJQ.hasClass(&quot;group1&quot;)) { if(!$(&quot;#&quot; + boxNow).hasClass(&quot;letter&quot;) || $(&quot;#&quot; + boxNow).hasClass(&quot;group2&quot;)){//点击的是字母而释放的是非字母 resetLoc(boxOri); return; } changeLoc(boxNow, boxOri); for(var i = 1; i &lt; 4; i++){ boxLocX ++; oriBoxLocX ++; boxNow = &quot;box-&quot; + boxLocX + &quot;-&quot; + boxLocY;//放下处的盒子id boxOri = &quot;box-&quot; + oriBoxLocX + &quot;-&quot; + oriBoxLocY; changeLoc(boxNow, boxOri); } } else if (targetEleJQ.hasClass(&quot;group2&quot;)) { if(!$(&quot;#&quot; + boxNow).hasClass(&quot;letter&quot;) || $(&quot;#&quot; + boxNow).hasClass(&quot;group1&quot;)){//点击的是字母而释放的是非字母 resetLoc(boxOri); return; } changeLoc(boxNow, boxOri); for(var j = 1; j &lt; 4; j++){ boxLocY ++; oriBoxLocY ++; boxNow = &quot;box-&quot; + boxLocX + &quot;-&quot; + boxLocY;//放下处的盒子id boxOri = &quot;box-&quot; + oriBoxLocX + &quot;-&quot; + oriBoxLocY; changeLoc(boxNow, boxOri); } } else{ if(!$(&quot;#&quot; + boxNow).hasClass(&quot;num&quot;)){ resetLoc(boxOri); return; } changeLoc(boxNow, boxOri); } $(document).unbind(&quot;mousemove&quot;, moveIt); $(document).unbind(&quot;mouseup&quot;, mouseUp); 完整代码","link":"/2015/06/16/16%E5%AE%AB%E6%A0%BC%E6%8B%96%E6%8B%BD/"},{"title":"Building my second brain 🧠 with Obsidian","text":"This article will take Obsidian as an example to share my practice of using Obsidian to build a second brain! The content of this article is out of date. Please refer to the official website LifeOS for more information. This article is also available in 简体中文. PrefaceWhat is Obsidian? Here is how it introduces itself on the official website: Obsidian is the private and flexible note‑taking app that adapts to the way you think. What I especially appreciate is its rich plugin ecosystem. If you like Vscode, then you will probably like Obsidian as well. The only difference is that Vscode is for writing code, and Obsidian is for taking notes. First Brain VS Second BrainThe first brain is our actual brain. It never stops working as long as we are alive. It performs tasks such as knowledge management, task management, and goal management. Most of the time, we cannot multitask, thus the first brain acts more like a CPU, with various tasks competing for CPU cycles. When there are too many tasks to handle, the brain becomes overwhelmed. It has to deal with current tasks while keeping the context of other tasks to switch between them, so we cannot focus on executing the current task. This is when an external system is needed to assist the first brain, which is the second brain. The second brain is an external system. If we compare the first brain to a CPU, then the second brain is more like a storage system. It acts as a buffer between the first brain and the real world, reducing the load on the first brain so it can focus on the current matter. It can be compared to memory and hard disks, but memory is more frequent in communication with the CPU (the first brain) and has a faster read speed. This storage system stores things that the first brain doesn’t need to focus on all the time. Of course, it’s up to the first brain to decide whether these things need to be stored. The content could be notes, to-dos, processes, and the medium could be text, images, videos. For example, when we use the second brain for task management, important and urgent items are stored in memory, while unimportant and non-urgent items are stored on the hard disk; this week’s tasks are in memory, while this month’s tasks might be on the hard disk. Therefore, by utilizing the second brain, we can focus on the present without any pressure and switch contexts when necessary. This article will use Obsidian as an example to share my practice in building a second brain! You may call it a second brain, but looking at it from different angles, I could also refer to it as “LifeOS,” because I record everything concerning life and work upon it. Additionally, it could be termed a “programmable personal productivity system.” I have written considerable code on it to do some querying and automation, and it is also the productivity system that I use to manage tasks and goals. It could even be likened to a “Monorepo project,” where every folder represents a project, and the README.md within the project is like the Package.json, describing the metadata of the current project. 📢 Attention: This system is not a top-down one with pre-established processes to implement. It gradually formed as I used Obsidian and is still evolving. Let’s tentatively call the current version 1.0. The purpose of sharing it now is to inspire others to improve their systems! I have already written an Obsidian Periodic PARA plugin to support this system! With this plugin, you don’t need any programming background. You can easily create periodic and PARA notes by simple visual clicks! My PracticeI adopt two systems: one for knowledge management and another for periodic notes. The former manages knowledge with dimensions like project/area/resource, and the latter manages tasks/goals/time with time as the dimension. Core Systems Knowledge management: Using the PARA system Projects -&gt; Projects are a series of tasks related to a goal with a deadline. Areas -&gt; Areas are activity domains that need to maintain certain standards over time. Resources -&gt; Resources are topics or subjects of ongoing interest. Archives -&gt; Archives are inactive items from the above three categories. Periodic notes Long-term: Top-down, focusing on long-term goals. Three-year Review Annual Review Quarterly Review Short-term: Bottom-up, focusing on short-term tasks. Monthly Review Weekly Review Daily: Capturing thoughts and insights, achieving self-awareness; time consumption statistics, focusing on projects. Daily Note The closer to Projects the PARA component, the more actionable it is; the more long-term the periodic note, the less predictable it is; These two systems effectively create two contexts that keep me focused: One based on time (periodic notes), i.e., when I reach a certain time node, I work based on the corresponding periodic notes, and the notes have enough context; The other based on the topic (PARA), i.e., when I want to research a topic, I work based on the index of the corresponding topic (README.md), and the notes have already collected a lot of context; Aspect-oriented Subsystem Aspect-oriented programming - Wikiwand Beneath the two systems mentioned above, there’s a hidden subsystem for managing tasks/goals/time, primarily through “periodic notes”: Task management Collecting via daily/weekly notes. Organizing via weekly/monthly reviews. Goal management Planning annual goals with the annual review. Splitting annual goals with the quarterly review. Breaking down pending items with the monthly review. Top-down sorting (through goal decomposition). Bottom-up sorting (through collection decomposition -&gt; daily/weekly notes). Time management Manually tracking the time spent on various projects in the and its proportions, adjusting time investment. Using scripts in daily, weekly, monthly, quarterly, and annual reviews to automatically track the time spent on various projects and their proportions, reviewing time investment. You might be curious that the above subsystem seems to only use “periodic notes.” In fact, the two main systems are connected through two methods. Connection How systems are associated Tag Connection Treat the first-level folders under PARA as special tags (they don’t need to be exactly the same as the folder names), use them in “periodic notes,” then you can index uniformly in each PARA folder in the same way. This ensures that the README.md index in each PARA folder has all the context for the current topic: Project ConnectionGenerate a project in “knowledge management,” to enhance focus on the project, there’s a “main event list” or “project list” in every class of “periodic notes,” such as: The “project list” in the daily note is a snapshot of the current project list, used for calculating the time spent that day on various projects and their proportions, ensuring enough time is spent on projects. The “priority first dimension” in the weekly and monthly reviews automatically merge and deduplicate from the daily reviews of the week or month, used for arranging project dimension tasks and subsequent reviews. The “priority first dimension” in the quarterly review is a snapshot of the current domain list, used for arranging main event dimension goals and subsequent reviews. The “priority first dimension” in the annual review, automatically merged and deduplicated from the quarter’s priority first dimension, used for setting domain dimension goals and subsequent reviews. Search Tags For example, the holiday, vacation tags in the daily note. Index files For example, the README.md index of each project’s tasks, notes, and context. Folders For example, using a consistent directory structure for each PARA category. Review The review focuses on the projects of the current period while planning the tasks for the next period. Weekly reviews reflect on the week’s daily notes, monthly reviews on each week’s reviews, and quarterly reviews on each month’s reviews. Quick StartDownload Click here to download Open with obsidian and enjoy Creating Notes Quickly create daily, weekly, monthly, seasonal, and annual notes through the note creation module in the upper left corner. Quickly create PARA notes, i.e., projects, areas, resources, archives through the note creation module in the upper left corner. “Daily Note” and “Project README” Used for daily management, including project lists, daily records, habit tracking, energy allocation, today’s accomplishments, etc. The “project list” in the daily note is a snapshot of the current projects (i.e., under the Projects directory). “Weekly Review” and “Monthly Review” Used for arranging weekly and monthly tasks, including task and review modules. In the weekly and monthly reviews, the “priority first dimension” are a collection of snapshots of the project lists from daily notes of the period (generated automatically). The “reviews” in the weekly and monthly notes mainly focus on the projects of the period. “Quarterly Review” and “Annual Review” Used for setting quarterly and annual goals, including goals and review modules. In the quarterly review, the “priority first dimension” is a snapshot of the current domain (i.e., under the Areas directory). In the annual review, the “priority first dimension” is a collection of snapshots from the priority first dimension of the period’s quarterly reviews (generated automatically). The “reviews” in the quarterly and annual notes mainly focus on the domains of the period. “PARA Index” and “Task Index” “Capture” and “Express”First, let’s introduce a concept, the CODE model, where: C is for Capture: Collecting resonant information. O is for Organize: Sorting the collected information, i.e., PARA. D is for Distill: Extracting the essence of the content, selecting the most useful information. E is for Express: Sharing with others, applying your knowledge in practice. Those familiar with PARA will see that this model is actually proposed by Tiago Forte in “Building a Second Brain,” and it’s the higher-level model that includes the PARA organization method, with O indicating the PARA organization method. My practice is to temporarily store some marked articles in the “Capture” directory while using the “-1. Capture/README.md” file to index the notes tagged with #PARA/Capture scattered in the daily notes. This makes it convenient for me during specific time nodes, such as weekends, month-ends, and quarter-ends, to review and sort, first organizing marked articles into each PARA topical directory, then transferring some inspirational notes from the daily notes into some explicit to-dos; Next, let’s talk about “Express.” I place my blog in the express section and also record some fragmentary notes in the daily note tagged with PARA/Express. These are outputs after internalization. If this output needs to be further posted on a specific social platform, such as Zhihu or Xiaohongshu, I will conveniently record it as a task. When I review the tasks indexed in the “5. Express/README.md” file and find pending ones, just complete them one by one. Small Tips in PracticeBuffer Zone MechanismPlace less important and non-urgent items swiftly into a buffer zone (Inbox) by creating tasks, keeping the main focus on “Projects.” Task ListsRecording tasks shouldn’t be a mental burden – writing them down doesn’t mean they must be done. Having them written relieves your mind from having to continuously remember or fear forgetting them. I’ve recorded many tasks, many of which, upon reassessment, were indeed not completed. It’s important to have mechanisms in place that allow for review of recorded tasks, for example: Using a tasks plugin to create query views for task lists Each periodic note contains a collected tasks list for the current period Project index files contain a tasks list Task RemindersI consider there to be three types of task reminders: Strong reminders, such as snapping up Moutai liquor or concert tickets, which need an alarm to remind you when it’s time Soft reminders, for things that need to be done on a certain day, like credit card repayments or loan repayments, which can be set up through GTD software for reminders List-style, for recording tasks which will be arranged later; based on the need, these can be converted to strong or soft reminders, akin to the inbox in GTD systems Micro-Habits I list micro-habits in my journal, and remember, they are not tasks. It’s okay whether they are completed or not; they mainly serve as a reminder, “Do you consider practicing these micro-habits today?” These act as prompts when I have the “capability” and “motivation.” For example: Micro-Habits Get up and drink water as soon as the alarm goes off Put on headphones and listen to the Little Universe podcast right after getting off the vehicle Open WeChat Read as soon as I get on the subway Write down three to-dos as soon as I reach my workstation Start serious work as soon as it’s 10:30 am Easy to RefactorWithin each periodic note, the same feature modules use the same statement, such as “tasks collected this period,” all inserted through the following query statement. The “this period” variable is provided by the current file name, which makes it very convenient to refactor all periodic files en masse, needing only to perform a batch replacement: 123```PeriodicPARA TaskRecordListByTime``` Efficient Use of ShortcutsSet consistent global shortcuts, so that no matter in which software, you can invoke the same function with the same shortcut. Here are some of my configurations: Cursor movement Pattern: Control + directional initial/VIM direction Examples: A: Head of line E: End of line F/L: Forward B/H: Backward N/J: Next line P/K: Previous line W: Delete a word (Backward) D: Delete a character (Forward) Window management Pattern: Command + Option + initial Examples: L: Left half of the screen R: Right half of the screen C: Center M: Maximize [: Show/Hide left sidebar ]: Show/Hide right sidebar ‘: Show/Hide bottom bar T: New Tab (for more specific windows, the top level uses Command + T) W: Close Tab (for more specific windows, the top level uses Command + W) J: Next Tab K: Previous Tab Document editing Pattern 1: Command + Option + number/symbol Examples: 1: Markdown level-one heading 2: Markdown level-two heading 3: Markdown level-three heading 4: Markdown level-four heading 5: Markdown level-five heading 6: Markdown level-six heading Bulleted list: ~ Strikethrough: - Function category Pattern: Control + initial Examples: C: Copy link (Obsidian block link, Arc browser link, VScode git link) D: Download I: Add to inbox K: Quick Search","link":"/2023/07/08/Building%20my%20second%20brain%20%F0%9F%A7%A0%20with%20Obsidian/"},{"title":"Chrome 谜一样卡死的排查方法","text":"遇到莫名卡死问题，如何正确地排查 最近由于 Chrome 浏览器升级，触发了一个埋点 SDK 的潜在问题，目前埋点 SDK 方面已经跟进解决，详细原因可见文章《Chrome 83 下千帆工作台卡死的问题》。 一切好像没什么问题，但是陆续有团队同学反馈，线上页面的确不卡了，测试环境某些操作仍然会出现卡死，而且卡死时正好会有 golden 或 omega 埋点处于 pending 状态。于是开始了排查之路~ 一、线索排查查看 pending 状态的请求： 发现请求卡在了一个 “Stalled” 的状态，谷歌还贴心的给出了 “Explanation” 链接，解释如下： Here’s more information about each of the phases you may see in the Timing tab: Queueing . The browser queues requests when: There are higher priority requests. There are already six TCP connections open for this origin, which is the limit. Applies to HTTP/1.0 and HTTP/1.1 only. The browser is briefly allocating space in the disk cache Stalled. The request could be stalled for any of the reasons described in Queueing. DNS Lookup. The browser is resolving the request’s IP address. Initial connection. The browser is establishing a connection, including TCP handshakes/retries and negotiating an SSL. Proxy negotiation. The browser is negotiating the request with a proxy server. Request sent. The request is being sent. ServiceWorker Preparation. The browser is starting up the service worker. Request to ServiceWorker. The request is being sent to the service worker. Waiting (TTFB). The browser is waiting for the first byte of a response. TTFB stands for Time To First Byte. This timing includes 1 round trip of latency and the time the server took to prepare the response. Content Download. The browser is receiving the response. Receiving Push. The browser is receiving data for this response via HTTP/2 Server Push. Reading Push. The browser is reading the local data previously received. 也就是 Queueing 阶段的问题导致卡在 Stalled 状态，经查得知可以通过如下方式查看 Chrome 的运行日志，当然包括详细的网络日志。 各平台查看日志方式见 https://support.google.com/chrome/a/answer/6271282?hl=zh-Hans 以监听日志方式打开浏览器 /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --enable-logging --v=1 实时查看日志：tail -f ~/Library/Application\\ Support/Google/Chrome/chrome_debug.log 二、日志查看按上述方式打开浏览器，实时查看日志文件，一步一步复现步骤，日志打印如下： 所以绕了一圈还是 “ResizeObserver” 的问题，原因在 Chrome 83 下千帆工作台卡死的问题 中也有提到，这里列两个 issue 大家有兴趣查看下： https://github.com/ElemeFE/element/issues/12866 https://github.com/ElemeFE/element/issues/17642 三、推测那么，为什么测试环境有问题，线上环境却没有呢？目前解决 Omega sdk 和 Golden sdk 问题的方法是通过如下方式重写 EventTarget.prototype.addEventListener 方法，达到过滤发送埋点请求的目的（也正是循环发送埋点请求，导致页面卡死）。 123456789101112131415EventTarget.prototype.addEventListenerBase = EventTarget.prototype.addEventListener; EventTarget.prototype.addEventListener = function(type, baseListener, status) { if(type === 'error') { function myListener(e) { // 重写 addEventListener，为了过滤ResizeObserver 错误的上报。在 Chrome 83 无限上报会造成浏览器卡死 if (e.message &amp;&amp; e.message.indexOf('ResizeObserver') !== -1) { return; } baseListener.apply(this, arguments); } this.addEventListenerBase(type, myListener, status); } else { this.addEventListenerBase(type, baseListener, status); } }; 所以，还有除了 EventTarget.prototype.addEventListener 方法之外的监听没有被重写，我掐指一算，难道是 window.onerror，于是去当前卡死页面的调试控制台打印 window.onerror: 真有这个监听，而且还有『字符串 replace 操作』，这要是无限循环调用这个回调，分分钟卡死！顺手点击这个打印结果，直接跳转到引用它的代码： 竟然是 vConsole 监听的，前端同学都知道，这个 vConsole 是为了在移动端方便调试使用的，一般在测试环境使用，所以这端代码很可能是这个导致了测试环境卡死，而线上环境正常的罪魁祸首！ 四、验证接下来我们就验证下猜想，首先把 window.onerror 覆盖为 console.log，重复复现步骤，控制台便打印出： 果不其然，不过这样还是卡住了，因为 console.log 也是同步操作。接着直接置空 window.onerror，执行 window.onerror = undefined，再来一次复现： 页面已经不再卡死了！ 五、解决目前在 Chrome 和 ele-element 未跟进解决的情况下，解决方法有： 去除 vConsole，毕竟中台项目不需要 vConsole； 置空 window.onerror； 对 window.onerror 进行节流； 六、总结此处总结下遇到莫名卡死问题，排查步骤应简化为： 监听 window.onerror 事件，这样能把一些内部的报错（即不会在控制台显示的错误）直接打印到控制台中 使用监听 Chrome 日志方式打开浏览器，从日志中查看线索 合理推测和验证","link":"/2020/06/11/Chrome%20%E8%B0%9C%E4%B8%80%E6%A0%B7%E5%8D%A1%E6%AD%BB%E7%9A%84%E6%8E%92%E6%9F%A5%E6%96%B9%E6%B3%95/"},{"title":"HTML5 截图","text":"实现一个类似于QQ截图的小东西，点击载入按钮，则载入图片，长按图片，弹出截图框，截图框右下角能够调整大小，并在右边的截图预览区域实时显示 需求实现一个类似于QQ截图的小东西，点击载入按钮，则载入图片，长按图片，弹出截图框，截图框右下角能够调整大小，并在右边的截图预览区域实时显示，其最终效果图如下： HTML需要注意canvas的设置，主要结构如下： 123456789&lt;section class=&quot;clearfix&quot;&gt; &lt;div id=&quot;origin&quot;&gt; &lt;canvas id=&quot;originImg&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;&lt;/canvas&gt; &lt;div id=&quot;shotRect&quot;&gt; &lt;div class=&quot;resizeBR&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;canvas id=&quot;showPre&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;&lt;/canvas&gt;&lt;/section&gt; CSS截图框初始大小为50px50px,右下角设置了一个不可见的调节区域,此处大小设置为12px12px，并在之后为其注册调整大小的事件。 1234567891011121314151617181920#shotRect { position: absolute; display: none; width: 50px; height: 50px; opacity: .5; border: 2px solid #2d2d2d;}.resizeBR { position: absolute; right: -5px; bottom: -5px; width: 12px; height: 12px; cursor: nw-resize; opacity: 0; background: #000; background: #ff0;} 其中原始图片的canvas大小与预览区的大小一致。 JavaScript载入图片，并绘制canvas123456789function loadImg(event) { //载入图片 cOri = document.getElementById(&quot;originImg&quot;); imgOri = new Image(); ctxOri = cOri.getContext(&quot;2d&quot;); imgOri.src = &quot;vegetable.jpg&quot;; imgOri.onload=function(){ ctxOri.drawImage(imgOri,0,0,500,500);//将图片绘制到canvas上 };} 长按待截图区域,弹出截图框12345678910111213function longClick(event) { //长按弹出截图框 event = event || window.event; var shotRect = document.getElementById(&quot;shotRect&quot;); timeout = setTimeout(function() { shotRect.style.display = &quot;block&quot;; var disX = event.clientX - shotRect.offsetWidth + 10, disY = event.clientY - shotRect.offsetHeight + 10; shotRect.style.left = disX + 'px'; shotRect.style.top = disY + 'px'; initCanvas(); updateRect(disX, disY, shotRect.offsetWidth, shotRect.offsetHeight); }, 1000);} 释放鼠标时,需要清除timeout。 初始化预览canvas12345function initCanvas() {//初始化预览画布 cPre = document.getElementById(&quot;showPre&quot;); ctxPre = cPre.getContext(&quot;2d&quot;); img = document.getElementById(&quot;originImg&quot;);} 更新预览canvas根据原始图片,在预览区域上使用drawImage方法画出预览图,其中x,y为截图框左上角相对于原始图片左上角的坐标;而w,h为截图框的长与宽;这四个参数提取出了截图框内的图像数据,而之后(0,0)这个坐标代表在画布上放置该图像数据的坐标位置,(0,0)意味着将该图像数据的左上角与预览区域的左上角重合。 1234function updateRect(x, y, w, h) {//更新画布 ctxPre.clearRect(0, 0, 500, 500); //清空画布 ctxPre.drawImage(img, x, y, w, h, 0, 0, 500, 500);} 调整截图框大小计算截图框左上角的坐标，并根据调整大小后鼠标的坐标，并据此重新设置截图框的大小，然后调用更新截图预览的函数updateRect，注意限制截图框的边界不能超过原始图片的大小。 123456789101112131415161718192021222324252627282930313233343536function resizeDown(event) { event = event || window.event; var shotRect = document.getElementById(&quot;shotRect&quot;), //计算Rect左上角的坐标 x = shotRect.offsetLeft, y = shotRect.offsetTop ; //console.log(&quot;x=&quot; + x+ &quot; &quot; + &quot;y=&quot; + y); //绑定事件 document.addEventListener(&quot;mousemove&quot;, mouseMove); document.addEventListener(&quot;mouseup&quot;, mouseUp); //移动鼠标 function mouseMove(event) { event = event || window.event; var finalX = event.clientX, finalY = event.clientY; //防止超过边界 if (event.clientX &gt;= 488) { finalX = 488; } if (event.clientY &gt;= 488) { finalY = 488; } //console.log( (event.clientX) + &quot; &quot; + (event.clientY)); xy = (finalX - x + 10) &lt; (finalY - y +10) ? (finalX -x + 10) : (finalY - y + 10); //计算移动后的Rect新大小 shotRect.style.width = xy + 'px'; shotRect.style.height = xy + 'px'; updateRect(x, x, shotRect.offsetWidth, shotRect.offsetHeight); } //停止事件 function mouseUp() { //卸载事件 document.removeEventListener(&quot;mousemove&quot;, mouseMove); document.removeEventListener(&quot;mouseup&quot;, mouseUp); }} 移动截图框详看注释吧~ 12345678910111213141516171819202122232425262728293031323334353637function dragDown(event) { event = event || window.event; if (event.target !== event.currentTarget) return; //如果是从子元素冒上来的，返回 var shotRect = document.getElementById(&quot;shotRect&quot;), disX = event.clientX - shotRect.offsetLeft, // 光标按下时光标相对截图框的坐标 disY = event.clientY - shotRect.offsetTop; //绑定事件 document.addEventListener(&quot;mousemove&quot;, mouseMove); document.addEventListener(&quot;mouseup&quot;, mouseUp); function mouseMove(event) { event = event || window.event; var disL = event.clientX - disX, //截图框左边界与左侧边界的距离 disT = event.clientY - disY, //截图框上边界与上侧边界的距离 maxW = document.getElementById(&quot;originImg&quot;).clientWidth - shotRect.offsetWidth, //最大宽度 maxH = document.getElementById(&quot;originImg&quot;).clientHeight - shotRect.offsetHeight; //最大高度 //超过边界则重置 if (disL &lt; 0) { disL = 0; } else if (disL &gt; maxW) { disL = maxW + 1; } if (disT &lt; 0) { disT = 0; } else if (disT &gt; maxH) { disT = maxH + 1; } shotRect.style.left = disL + 'px'; //重新计算截图框的相对位置 shotRect.style.top = disT + 'px'; updateRect(disL, disT, shotRect.offsetWidth, shotRect.offsetHeight); } function mouseUp(event) { document.removeEventListener(&quot;mousemove&quot;, mouseMove); document.removeEventListener(&quot;mouseup&quot;, mouseUp); }} 保存图片由于跨域问题,保存在chrome无效,在firefox中有效: 12345function saveImg(event) { var image = cPre.toDataURL(&quot;image/png&quot;); var w = window.open('about:blank', 'image from canvas'); w.document.write(&quot;&lt;img src='&quot; + image + &quot;' alt='from canvas'/&gt;&quot;);} 完整代码","link":"/2015/07/19/HTML5%E6%88%AA%E5%9B%BE/"},{"title":"Javascript 语言精粹の笔记","text":"最近买了几本书，这本《JavaScript 语言精粹》果真精粹，篇幅很少，而附录较多，作者是 JSON 的发明者 Douglas Crockford ，听起来屌屌的，其中的干货也是不少的，看完受益匪浅！ JavaScript 语言精粹章四,五 函数、继承 如果在一个函数前面带上 new 来调用，则背地里会创建一个链接到该函数的 prototype 成员的新对象 a. 通过 proto 进行链接，使得新对象上的 proto 属性拥有 prototype 上的所有属性与方法 b. 同时 this 将会被绑定到这个新对象上，此时若返回值不是一个对象，则返回 this 123var that = Object.create(this.prototype);var other = this.apply(that, arguments);return (typeof outer === 'object' &amp;&amp; other) || that; new 运算符创建一个新对象（继承自其运算数原型），然后调用该运算数，把新创建的对象绑定到 this 上；使得运算符（即构造器函数）在返回给调用者前能够自定义新创建的对象。 因为 JavaScript 原型继承的动态本质，使得新的方法立刻被赋予到所有对象的实例上，哪怕在将方法增加到对应类的 prototype 属性之前就已经创建的对象实例。 function A() {}, 类 A 的构造函数位于 A 的原型 prototype 之上，因此使用如下方式继承时，最后一步需要重置子类的构造器为自身： 123456789101112131415161718192021// Shape - superclassfunction Shape() { this.x = 0; this.y = 0;}// superclass methodShape.prototype.move = function(x, y) { this.x += x; this.y += y; console.info('Shape moved.');};// Rectangle - subclassfunction Rectangle() { Shape.call(this); // call super constructor.}// subclass extends superclassRectangle.prototype = Object.create(Shape.prototype);Rectangle.prototype.constructor = Rectangle; 关于 Object.create The **Object.create()** method creates a new object with the specified prototype object and properties. 该方法根据指定的原型对象和属性生成一个新对象 1234567891011121314151617181920var o;// create an object with null as prototypeo = Object.create(null);o = {};// is equivalent to:o = Object.create(Object.prototype);// o -&gt; Object:{__proto__: Object}function Constructor() {}o = new Constructor();// is equivalent to:o = Object.create(Constructor.prototype);// Of course, if there is actual initialization code in the// Constructor function, the Object.create() cannot reflect it// o -&gt; Constructor:{__proto__: Object} 章六 数组 JavaScript 的数组是一种拥有一些类数组特性的对象 12345678var arr = [];var obj = {};typeof arr === typeof obj // true，皆为 'object'arr instanceof Array // truearr instanceof Object // true 为数组设置更大的 length 不会分配更多空间，而把 length 设小，则将所下标大于等于新 length 的属性删除 由于数组就是对象，因此可使用 delete 运算符移除元素 12345var arr = ['one', 'two', 'three'];delete arr[1]; // ['one', undefined, 'three']// 使用 splice 方法arr.splice(1,1); // ['one', 'three'] 数组是个对象，因此可以给其增添属性，当属性不为数字时，并不会增加 length 章七 正则 如果拿不准一个特殊字符是否需要转义，可统一加上转义符 ‘' \\d [0-9] \\D \\s Unicode 字符 \\S \\w [0-9A-Z_a-Z] \\W . 除结束符以外的任意字符 12345678910111213143. 正则表达式分组：()，正则表达式字符集：[]4. 量词：```js//如果只有一个量词(形如 {1,} ) 则表示进行贪婪性匹配let reg = /w{3,}/;'wwwww'.match(reg); // 'wwwww'// 通过 ？ 进行非贪婪性匹配let reg2 = /w{3,}?/;'wwwww'.match(reg2); // 'www' 章八 方法 array.pop 与 array.shift pop 的位置不同，一个在头部，一个在尾部 array.push 与 array.unshift push 的位置不同，一个在尾部，一个在头部 array.slice(start, end) 潜复制，array.splice(start, deleteCount, item…) 移除若干个元素，并使用 item 替换 regexp.exec(string)、regexp.test(string) string.lastIndexOf(searchString, position)、string.charAt(position)、string.match(regexp)、string.search(regex) 附录 1 + 0.2 // 0.30000000000000004 0.1 + 0.2 == 0.3 // false 12345672. ```js typeof NaN === 'number' // true Nan === NaN // false NaN !== NaN // true isNaN(NaN) // true ES6 将其归到 Number.isNaN 判断数组： 12arr &amp;&amp; typeof arr === 'object' &amp;&amp; arr.constructor === Array 加强版: 12Object.prototype.toString.apply(arr) === '[object Array]' 位运算符在 JavaScript 中执行效率非常慢 建议使用 var foo = function foo() {}; 而不是 function foo() {}; 避免使用类型包装器：new Boolean(false)、new Number(1)、new String(‘abc’) 避免使用 new Object 与 new Array HTML 中，字面上的 ‘&lt;’ 符号必须使用 ‘&amp;lt ;’ JSON JSON 有六种类型值： 对象、数组、字符串、数字、布尔值、null","link":"/2016/11/20/JavaScript%20%E8%AF%AD%E8%A8%80%E7%B2%BE%E7%B2%B9%E3%81%AE%E7%AC%94%E8%AE%B0/"},{"title":"JavaScript 权威指南の笔记","text":"当Javascript解释器启动时，它将创建一个新的全局对象，并给它一组定义的初始属性。 只要引用了字符串直接量的属性，就会将字符串值通过调用new String(). 方式转换成对象，一旦引用结束，则销毁这个临时对象。 “==”将运算符将原始值与其包装对象视为相等，而“===”不然。 原始值不可变，对象引用可变。 undefined转换为数字： NaN，而null转换为数字：0，空字符串转为0与false。 对象 当Javascript解释器启动时，它将创建一个新的全局对象，并给它一组定义的初始属性。 只要引用了字符串直接量的属性，就会将字符串值通过调用new String(). 方式转换成对象，一旦引用结束，则销毁这个临时对象。 “==”将运算符将原始值与其包装对象视为相等，而“===”不然。 原始值不可变，对象引用可变。 undefined转换为数字： NaN，而null转换为数字：0，空字符串转为0与false。 对象到布尔值的转换： 所有对象（包括数组和函数）都转为true。 全局作用域可以不写var，但局部变量的声明需要var，否则默认生成全局变量。 Javascript并无块级作用域，取而代之的是函数作用域，指在函数内声明的所有变量在函数体内始终可见，意. 着变量在声明之前已经可用，但是赋值操作还在原始位置执行，即“声明提前”。 var truevar = 1；//声明一个不可删除的全局变量 fakevar = 2; //创建全局对象的可删除的属性 x!=x来判断x是否为NaN。 in运算符： 若右侧对象拥有一个名为左操作数值的属性名，则返回true。12.typeof(null) -&gt; “object”;typeof(undefined) -&gt; “undefined”;常用语表达式中：(typeof value == “string”) ? “ ‘ “ + value + “ ‘ “ : value; 函数是对象的一种，但是typeof(func)的结果是“function”。 删除属性或数组不仅仅是设置了一个undefined的值，其属性将不存在。 不能删除通过var声明的变量，删除不存在的属性返回true，删除数组元素，不改变其长度。 void运算符： 忽略操作数的计算结果并返回undefined。 for(variable in object) statement;先计算variable的值，并将属性名赋值给它。 break不能越过函数的边界，而continue只能在循环体内使用。 &amp;&amp;短路： 当左操作数是真值时，该运算符将计算右操作数的值并将其返回作为整个表达式的计算结果。 hasOwnProperty()用来检测给定的名字是否是对象的自有属性。 存取器属性： getter与setter，可继承，get property(){},set property(){}。 数据属性：值，可写性，可枚举性，可配置性； 存取器属性：get，set，可枚举性，可配置性。 转换为不可扩展性后，不可转回，Object.preventExtensions()，给一个不可扩展的对象的原型添加属性，它照样会继承新属性。 1, Object.getOwnPropertyDescriptor()可获得某个对象特定属性的属性描述符：({x:1},”x”)。 2, Object.defineProperty(o,”x”,{value:1,writable:true,enumerable:false,configurable:true}); 3, 以上两个函数都不包括继承属性。 数组 数组是JS对象的特殊形式，数组索引实际上和碰巧是整数的属性名差不多。 数组遍历：1. 排除null与defined、不存在的元素： if(!a[i]) continue; 排除undefined和不存在的元素： if(a === undefined) continue; 排除不存在仍处理undefined： if(!(i in a)) continue;或：for(var index in sparseArray){}。 数组方法： join()：将所有元素转成字符串连接在一起，默认分隔符为逗号，可指定分隔符； reverse()：将元素颠倒顺序； sort()：字母表顺序排列，数值顺序：a.sort(function(a,b) { return a - b; } ); //若第一个参数应该在前，返回小于零的数值，反之亦然； concat()：创建并返回一个新数组，包括原始数组与concat()的每个参数； slice()：返回的数组包含第一个参数指定的位置到第二个参数指定的位置之间的所有数组元素，不包括第二个参数；若指定一个参数，则该参数为起始位置，结束位置在最后；负数：-1代表最后一个元素； splice()：不同于slice与concat，它会修改调用的数组；并返回被删元素组成的数组； push与pop； unshift与shift在数组头部操作； Javascript 5： forEach()：遍历数组，为每个元素调用指定的函数； map()：需要返回值，返回的是新数组，不修改调用的数组，而forEach()修改； filter()：压缩稀疏数组：var dense = sparse.filter(functin(){return true;});//filter会跳过稀疏数组中缺少的元素，返回的数组总是稠密的; 同时删除undefined和null元素：a = a.filter(function(x){return x !== undefined &amp;&amp; x != null;}); every()：当且仅当针对数组中的所有元素调用判定函数都返回true，它才返回true； some()：至少有一个为true，则返回true； reduce()：var sum = a.reduce(function(x,y) { return x + y }, 0);//其中第二个参数0是初始值； reduceRight()：从右到左处理； indexOf()：搜索整个数组中具有给定值的元素，返回找到的第一个元素的索引，没找到返回-1； lastIndexOf()：反向搜索，第二个参数指定开始查找的位置。 判定是否为数组：Array.isArray()。 函数 嵌套函数不会从调用它的函数中继承this，如果嵌套函数作为方法调用，其this的值指向调用它的对象，作为函数调用，其this值不是全局对象（非严格模式下）就是undefined（严格模式下）。 new o.m()调用上下文（this）并不是o，而是构造函数调用创建的一个新的空对象。 &amp;&amp;： 若左操作数为假，返回左操作数值，不计算右操作数；若左操作数为真，计算右操作数并返回右操作数的值； ||： 若左操作数为真，返回左操作数值，不计算右操作数；若左操作数为假，计算右操作数并返回右操作数的值； arguments： 指向实参对象的引用，包含一个length属性，却不是真正的数组。 arguments的属性： callee：指代当前正在执行的函数。 函数可以有自己的属性，因为它是一种特殊的对象。 函数可以作为命名空间： ( function(){ //codes } () );//结束函数定义并立即调用它。 闭包： JS函数执行用到了作用域链，此链是函数定义的时候创建的，不管在何时何地执行内部嵌套函数f()，f中的变量一定是局部变量。 每次调用JS函数的时候，都会为之创建一个新的对象用来保存局部变量，把这个对象添加至作用域链中，当函数返回时，就从作用域链中将这个绑定变量的对象删除。 嵌套函数里是无法访问this的，除非在外部函数将其转换为一个变量： var self = this; 函数属性： length：形参个数； prototype属性： 当函数用作构造函数时，新创建的对象会从原型对象上继承属性。 以对象o的方法来调用函数f： f.call(o)或者f.apply(o); bind()方法： var g = f.bind(o);//通过调用g(x)来调用o.f(x)。 Function()构造函数所创建的函数并不使用词法作用域，函数体代码的编译总是会在顶层函数（全局作用域）执行。 不完全函数与记忆函数。 类和模块 调用构造函数： 构造函数的prototype属性被用作新对象的原型，从而继承了prototype的属性。 工厂函数方法与构造函数方法。 原型对象是类的唯一标识，当且仅当两个对象继承自同一个原型对象时，它们才属于同一个类的实例。 对于任意函数F.prototype.constructor==F;一个对象的constructor属性指代这个类。 构造函数与原型对象之间的关系： 任何添加到构造函数对象（不是指添加到构造函数内部）的属性都是类字段和类方法，属于类而不属于类的某个实例； 原型对象的属性被类的所有实例所继承，若原型对象的属性值是函数的话，这个函数就作为类的实例方法来调用，实例方法由所有实例所共享； 直接给类的每个实例定义非函数属性，实际上就是实例的字段。 即使创建对象之后，原型的属性发生改变，也会影响到继承这个原型的所有实例对象。 检测对象的类：instanceof，isPrototypeOf()，这里的继承可以不是直接继承； constructor属性； 以上两个方法不适用于多窗口和多框架子页面，因此可以使用构造函数的名称； 工厂方法： 构造函数方法： toJSON()用于序列号对象，如果一个对象有toJSON()方法，则JSON.stringify()并不会对传入的对象做序列号操作，而会调用toJSON()来执行序列号操作，JSON.parse()是其逆过程。 forEach： 私有方法： 通过将变量（或参数）闭包在一个构造函数内来模拟实现私有实例字段： 创建子类的关键： B.prototype = inherit(A.prototype);//子类派生自父类 B.prototype.constructor = B;//重载继承来的constructor属性 用组合代替继承的集合的实现： Object.preventExtensions():将对象设置为不可扩展的，即不能给对象添加任何新属性； Object.seal与Object.freeze:将对象的所有属性设置为只读和不可配置的。 Object.create(null);//创建一个不包含原型的对象，使之能够直接对它使用in运算符 作为私有命名空间的函数： 创建屏外图像： new Image(80,20).src = “images/***.gif”; this： 1,指向函数执行时的当前对象；2,没有明确的当前对象时，指向全局对象window。3,在事件处理程序的代码中，关键字this引用触发该事件的文档元素。 表单元素的属性： type，form（对包含该元素的form对象的只读引用），name，value；使用this.form引用Form对象，this.form.x引用名为x的兄弟表单元素。 cookie的性质： expires，path，domain，secure。 函数直接量与Function()构造函数创建函数的方法：适用于只用一次，无需命名。 属性callee： 用来引用当前正在执行的函数。 f.call(o,1,2);等价于： o.m = f; o.m(1,2); delete o.m; 删除一个属性： delete book.chapter2。 在方法主体的内部，this的值就变成了调用该方法的对象。 JS对象都“继承”原型对象的属性。 属性的继承只发生于读属性值时，而在写属性值时不发生。 实例属性有自己的副本，而实例方法是一个类共享的。 若要生成类Complex的一个子类，只需确保新类的原型对象是Complex的一个实例；这样它就能继承Complex.prototype的所有属性： MoreComplex.prototype = new Complex(0,0)； MoreComplex.prototype.constructor = MoreComplex; Object.property 等价于 Object[“property”] -&gt;关联数组。 只有那些真正存储在数组中的元素才能够分配内存。 正则表达式直接量： var pattern = /S$/;即：var pattern = new RegExp(“S$”); 在复制和传递时使用的是引用，但在比较它们时使用的却是值。 通过设置class.prototype属性来定义所有类实例共享的方法和常量。 Function()构造函数 函数直接量 允许运行时动态地创建和编译JS代码 静态 每次调用都会解析函数体并创建一个新的函数对象 相反 不使用词法作用域作为顶级函数 null == undefined -&gt;true; null === undefined -&gt; false; web浏览器中的JS 如果两个窗口包含的脚本把Document的domain属性设置成相同的值，则这两个窗口就不再受同源策略的约束，能够相互读取对方的属性。 1, 当HTML解析器遇到script标签元素时，它默认必须先执行脚本，然后再恢复文档的解析和渲染； 2, script标签的defer属性：使得浏览器延迟脚本的执行，直到文档的载入和解析完成；按顺序执行。 3, async属性：使得浏览器尽快的执行脚本，而不用在下载脚本时阻塞文档的解析；有可能无序执行。 window对象 Window对象的location属性引用的是Location对象： window.location === document.location //true Location对象的toString方法返回的是它的href属性的值。 A窗口中调用B窗口中的函数，此函数在定义它的作用域中执行，而不是在调用它的作用域中执行。 对于内置的类，每个窗口都有构造函数的一个独立副本和构造函数对应原型对象的一个独立副本。 脚本化文档 为某些HTML元素设置name或id属性值，将自动为window与document对象创建对应的属性，其属性值指向表示该文档元素的HTMLElement对象。 通过CSS选择器选取元素： querySelectorAll()，querySelector()；接受一个包含CSS选择器的字符串参数。 Document、Element、Text对象都是Node对象，其属性： parentNode，childNodes，firstChild，lastChild，nextSibling，previoursSibling，nodeType，nodeValue，nodeName，textContent。 Element属性： attributes，innerHTML; DocumentFragment是一种特殊的Node，作为其他节点的一个临时容器。 滚动条位置： window.pageXOffset/pageYOffset; 查询视口尺寸： windows.innerWidth/innerHeight; HTML5中，input标签的placeholder属性指定了用户输入前在输入域显示的提示消息。 Document类型定义了创建Element和Text对象的方法： document.createTextNode(“text node content”); Node类型定义了在节点树中插入、删除和替换的方法： parent.appendChild(child);//插入parent元素的最后； parent.insertBefore(child,parent.childNodes[n]; n.parentNode.removeChild(n); n.parentNode.replaceChild(document.createTextNode(“[ REDACTED ]”), n); 脚本化CSS 内联样式： e.style.position=”relative”; 计算样式： window.getComputedStyle(element,null); 事件处理 通过HTML属性来注册事件处理程序是一个例外，它们被转换为能存取全局变量的顶级函数而非任何本地变量。 在一个对象上触发某类事件（比如单击onclick事件），如果此对象定义了此事件的处理程序，那么此事件就会调用这个处理程序，如果没有定义此事件处理程序或者事件返回true，那么这个事件会向这个对象的父级对象传播，从里到外，直至它被处理（父级对象所有同类事件都将被激活），或者它到达了对象层次的最顶层，即document对象（有些浏览器是window）。 jQuery each()只能遍历jQuery对象，而jQuery.each()可以遍历数组元素或对象属性。 focus与blur事件不支持冒泡，而focusin与focusout支持； mouseover与mouseout支持冒泡，mouseenter与mouseleave不支持；","link":"/2015/06/15/JavaScript%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%81%AE%E7%AC%94%E8%AE%B0/"},{"title":"Javascript 函数式编程の书摘","text":"章一1，确定抽象，并为其构建函数；2，利用已有的函数来构建更为复杂的抽象；3，通过将现有的函数传给其他的函数来构建更加复杂的抽象。 章一1，确定抽象，并为其构建函数；2，利用已有的函数来构建更为复杂的抽象；3，通过将现有的函数传给其他的函数来构建更加复杂的抽象。 章二1，函数时一等公民： 函数式编程语言应该是促进创造和使用函数的； 2，applicative 编程： 函数A作为参数提供给函数B； 3，JavaScript对象的键只能是字符串； 章三词法作用域： 指一个变量的可见性，及其文本表述的模拟值；变量查找从最内层范围向外扩展。 动态作用域： 1，维护一张”值“的全局表（维护一个命名绑定栈的全局映射）；2，并使用一个函数来查询绑定的值；3，缺点：任何给定的绑定值，在确定调用其函数前，都是不可知的。 闭包： 闭包就是一个函数，捕获作用域内的外部绑定（例如，不是自己的参数）。这些绑定是为了之后使用（即使在该作用域已经结束）而被定义的。如果一个变量的引用同时存在于闭包的内外部，则它的变化可以跨越看似私有的界限，因此JavaScript经常使用如下模式，把捕获的变量作为私有数据： 12345678910111213var pingpong = (function() { var private = 0; return { inc: function(n) { return private += n; } dec: function(n) { return private -= n; } }; })(); pingpong.inc(10); pingpong.dec(7); 章四高阶函数： 1，它是一等公民；2，以一个函数作为参数；3，以一个函数作为返回结果； 闭包： 1，闭包会捕获一个值（或引用），并多次返回相同的值；2，每一个新的闭包都会捕获不一样的值； 章五柯里化函数： 逐渐返回消耗参数的函数，直到所有参数耗尽； 部分应用函数： 是一个部分执行，等待接收剩余的参数立即执行的函数； 组合函数： _.compose: 从右往左执行，最右边的函数的结果会被送入其左侧的函数，一个接着一个。 章六尾递归： 函数（除了停止条件返回值）的最后一个动作是递归调用； 递归和组合函数： andify与orify； 相互关联函数： 两个或多个函数相互调用会被称为相互递归，通过相互递归调用来回反弹彼此之间递减某个绝对值，直到一方或另一方达到零； 蹦床原理： 通过打包调用，而不是直接调用；trampoline函数：不断调用函数的返回值，直到它不再是一个函数。 章七纯函数： 1，其结果只能从它的参数的值来计算；2，不能依赖于能被外部操作改变的数据；3，不能改变外部状态。 幂等性： 对同一的参数运行一次函数应该与连续两次运行是一个结果。 不变性： 例如字符串； 对象的不变性： 1，不可变对象应该在构造时固定它们的值之后不能再修改；2，不可变对象操作并返回新对象。 章八惰性链： 在调用value之前，_.chain是惰性的，什么都不会发生； thunk： 封装了一些行为的函数； 管道；","link":"/2015/07/19/Javascript%20%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E3%81%AE%E4%B9%A6%E6%91%98/"},{"title":"Monorepo 下 Git 工作流的最佳实践","text":"没有哪一种 Git 工作流是银弹，合适的 Git 工作流往往取决于项目的代码规模、协作人数、应用场景等；本次分享先从适合小型 Monorepo 的 Feature branch 工作流开始分享，接着分享适用于中大型 Monorepo 的 Trunk-based 工作流，并给出一些选型标准供同学们参考，希望通过本次分享，大家能找到合适自己 Monorepo 工程的 Git 工作流！ 背景没有哪一种 Git 工作流是银弹，合适的 Git 工作流往往取决于项目的代码规模、协作人数、应用场景等；本次分享先从适合小型 Monorepo 的 Feature branch 工作流开始分享，接着分享适用于中大型 Monorepo 的 Trunk-based 工作流，并给出一些选型标准供同学们参考，希望通过本次分享，大家能找到合适自己 Monorepo 工程的 Git 工作流！ 前置知识我们最熟悉的 Git 工作流莫过于 Git flow, Gitlab flow, Github flow，而对于 feature branch 和 trunk-based 比较陌生，那么以上几种 flow 有什么关系呢？ Feature branch 和 Trunk-based 工作流是比较新晋的概念，二者是相对的、互斥的，它们组成一个全集； Git flow, Gitlab flow, Github flow 都属于 feature branch development，它们有一个共同点：都采用『功能驱动式开发』，即：需求是开发的起点，先有需求再有功能分支（feature branch）或者补丁分支（hotfix branch）； 适用场景在 Monorepo 工程中，使用 feature branch development 开发模式时，随着代码库复杂性和团队规模的增长，并行的『长期分支』也会越来越多，这些分支在合入主干时，将会频繁遇到冲突或者不兼容的情况，而手动解决代码冲突往往会引入 Bug。 而 trunk-based development 鼓励开发者可以通过一些小的提交创建『短期分支』，从而大大缓解冲突问题，有助于保持生产版本的流畅。 总的来说，选择一个工作流不仅仅是一系列操作工具的流程，我们往往还需要对它背后的思想买单；下面的表格是两种工作流模式在各个维度的适用情况： 目前大部分业务场景使用的都是 feature branch 的开发模式，如果你的业务是多人开发一个巨型应用（如抖音主站、飞书文档等），应该尝试使用 Trunk based 开发模式，这会提高仓库整体工程质量和管理水平。 展开说说Feature branch development什么是 feature branch development？ 定义 『功能分支开发模式』的核心思想是所有特性开发都应该在专用的分支，而不是主分支中进行。这种封装使多个开发人员，可以轻松地在不干扰主代码库的情况下处理特定功能。这也意味着主分支永远不会包含损坏的代码，这对于持续集成环境来说是一个巨大的优势。– Git Feature Branch Workflow | Atlassian Git Tutorial 上线模式 从 master 分支创建一个功能分支（Feature Branch） 开发者们在功能分支中完成开发工作 构建功能分支，并通知 QA 进行验证 如果发现任何问题 开发者创建一个基于功能分支的修复 MR 经过代码审阅和合并过程将修复 MR 合入功能分支 再重新构建部署，并通知 QA 进行验证 QA 验证通过后，将功能分支发布至线上，然后将其合并入主干后删除 为什么使用 feature branch development？ 多功能并行开发 使多个开发人员可以轻松地在不干扰主代码库的情况下处理特定功能。 保持主分支稳定 主分支永远不会包含损坏的代码，这对于持续集成环境来说是一个巨大的优势。 心智负担低 仅需了解简单的操作即可实践，无需了解 cherry-pick, feature flag 等概念。 Trunk-based development什么是 trunk-based development？ 定义 『基于主干的开发模式』是一种版本控制管理实践，开发者将小而频繁的更新合并到核心『主干』（通常是 master 分支）。 这是 DevOps 团队中的一种常见做法，也是 DevOps 生命周期的一部分，因为它简化了合并和集成阶段。事实上，它也是 CI/CD 的必备实践。 与其它存在『长期分支』的功能分支策略相比，开发者可以通过一些小的提交创建『短期分支』。随着代码库复杂性和团队规模的增长，『基于主干的开发模式』有助于保持生产版本的流畅。– Trunk-based Development | Atlassian 上线模式 从部署分支上线半自动化流程，适用于低频率部署，以及自动化测试不全面的项目 (A dot represents an MR merged into master. Green dots means good commits that passed e2e tests, and red dot means a buggy commit which should be avoided when deploying/rollback) 从 master 分支创建一个部署分支（RC） 构建部署分支（RC），并通知 QA 进行验证 如果发现任何问题 开发者创建一个基于 master 分支的修复 MR 经过代码审阅和合并过程将修复 MR 合入 master 将 commits cherrypick 到部署分支（RC），再重新构建部署，并通知 QA 进行验证 QA 验证通过后，将部署分支（RC）发布至线上，然后删除该分支（RC） 从主干分支上线全自动化流程，适用于需要高频率部署，以及自动化测试较为全面的项目 (A dot represents an MR merged into master. Green dots means good commits that passed e2e tests, and red dot means a buggy commit which should be avoided when deploying/rollback) 定时部署： 每天或者每小时到了特定时间，部署机器人自动找到当前最新通过全部端到端测试的代码(特定的 commit hash)，然后将之部署上线。 持续部署： 每当有新代码合并进主干分支时，部署机器人自动验证新代码是否通过所有端到端测试，以及是否与该项目相关，如果是则自动部署上线 为什么使用 trunk-based development？ 允许持续的代码集成（CI） 在『基于主干的开发模式』中，源源不断的提交合入主干分支。为每个提交添加自动化测试套件和代码覆盖率监控可以实现持续集成。当新代码合并到主干中时，会运行自动集成和代码覆盖测试以验证代码质量。 确保持续的代码审查（CR） 『基于主干的开发模式』的快速、小型提交使代码审查成为一个更有效的过程。借助小型分支，开发人员可以快速查看和审查小的更改。与评审者阅读大面积代码变更的长期功能分支相比，这要容易得多。 支持连续的生产代码发布（CD） 团队应该每天频繁地合并到主分支。『基于主干的开发模式』努力使主干分支保持“绿色”，这意味着它可以在每次提交合并后进行部署。自动化测试、代码收敛和代码审查，保证了基于主干的项目可以随时部署到生产环境中。 更适用于大型 Monorepo 下的多人协作场景（scalable） 大型 Monorepo 下的多人协作场景更易出现代码冲突，不仅消耗的大量的人力解决冲突，还增加了『长期分支』合入『主干分支』引入 bug 的可能性。与其它存在『长期分支』的功能分支策略相比，开发者可以通过一些小提交创建『短期分支』进行快速迭代。因此，随着代码库复杂性和团队规模的增长，『基于主干的开发模式』也能保证顺畅的多人协作。 线性的提交历史（Linear history） Trunk-based development 更容易做到线性的 commit 历史，它有如下几个好处： 方便查看和跟踪历史记录 方便回溯变更，比如：Feature A 是在 Bugfix B 之前或者之后引入的？ 方便排查 bug，比如：使用 Git bisect 二分排查，而非线性历史则难以操作 撤销变更，比如：当你发现一个有问题的 commit，简单的 revert 对应的 commit 即可，而非线性的历史会有很多跨分支的合并，使 revert 变得困难 有效的两个前提 持续集成和测试 在每次代码合并前后，开发者都需要知道自己的代码对主干带来了什么影响，因此持续集成和测试的能力必不可少。 功能开关 因为在基于主干开发时，大的功能被分解为小改动，因此对于还未完成而之后部分合并进主干的功能，我们需要功能开关来不让他们过早地暴露给用户。 功能开关通常是一套独立的控制系统，线上的代码有两套逻辑，然后通过实时读取功能开关的取值来决定是否隐藏或暴露某个功能。通常，我们在部署完一个功能相关的所有代码之后打开某个功能开关。然后当此功能已经稳定并且被永久加入产品后，会把功能开关和相关的逻辑代码删除掉。 参考 A tidy, linear Git history：https://www.bitsnbites.eu/a-tidy-linear-git-history/","link":"/2022/12/31/Monorepo%20%E4%B8%8B%20Git%20%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"title":"Node.js 开发指南の书摘","text":"章三1，单次加载： require不会重复加载模块，无论调用多少次require，获得的模块都是同一个； 2，覆盖exports： 当将一个对象封装到模块中时，exports.Hello = Hello,则需使用require(‘./singleobject’).Hello来获取对象，可简化如下：module.exports = Hello; 此时就可以直接获取这个对象了，var Hello = require(‘./hello’); hello = new Hello(); 3，创建全局链接： npm link express；从而在当前目录使用全局安装的express； 4， 通过使用npm init交互式初始化一个符合标准的package.json；发布包：npm publish；更改json文件中的version字段后，重新发布，达到更新版本的目的；取消发布：npm unpublish； 章三1，单次加载： require不会重复加载模块，无论调用多少次require，获得的模块都是同一个； 2，覆盖exports： 当将一个对象封装到模块中时，exports.Hello = Hello,则需使用require(‘./singleobject’).Hello来获取对象，可简化如下：module.exports = Hello; 此时就可以直接获取这个对象了，var Hello = require(‘./hello’); hello = new Hello(); 3，创建全局链接： npm link express；从而在当前目录使用全局安装的express； 4， 通过使用npm init交互式初始化一个符合标准的package.json；发布包：npm publish；更改json文件中的version字段后，重新发布，达到更新版本的目的；取消发布：npm unpublish； 章四1，process： process.argv: 命令行参数数组，第一个元素是node，以此类推；process.nextTick(callback): 为事件循环设置一项任务，Node.js会在下次事件循环响应时调用callback（一般用于拆分事件，从而减少每个事件的执行时间）； 2，console： console.trace(): 向标准错误流输出当前的调用栈； 3，util： util.inherits(constructor, superConstructor): 仅仅继承superConstructor在其原型中定义的函数，而构造函数内部创造的属性和函数都没有被继承；util.inspect(object, [showHidden], [depth], [colors]): 将任意对象转换为字符串的方法，通常用于调试和错误输出； 4，events： 1234567events.EventEmitter: var events = require(&quot;events&quot;); var emitter = new events.EventEmitter(); emitter.on(&quot;someEvent&quot;, function(arg1, arg2) { console.lgo(&quot;listener1&quot;, arg1, arg2); });emitter.emit(&quot;someEvent&quot;, &quot;byvoid&quot;, 1991); result: listener1 byvoid 1991 emitter.emit(“error”);退出程序并打印调用栈。只要是支持事件响应的核心模块都是EventEmitter的子类，例如fs、net、http。 5，fs: fs.readFile(filename, [encoding], [callback(err, data)];fs.readFileSync(filename, [encoding]);与同步I/O函数不同，node.js中异步函数大多没有返回值。 6, HTTP服务器: var http = requitr(‘http’); http.Server是一个基于事件的HTTP服务器,主要有以下几个事件: 1,request: 当客户端请求到来时触发,提供两个参数req与res,分别是http.ServerRequest和http.ServerResponse的实例.2,connection: 当TCP连接建立时触发,提供一个参数socket,是net.Socket的实例。该事件粒度大于request。3,close: 当服务器关闭时触发,而不是用户连接断开时。由于最常用的是request，因此http提供了一个捷径：http.createServer([requestListener])，其功能是创建一个HTTP服务器并将requestListener作为request事件的监听函数。 而http.ServerRequest提供了以下三个事件用于控制传输： 1，data：当请求体到来时触发，并提供一个参数chunk，表示接收到的数据。2，end：当请求体数据传输完成时，该事件被触发。3，close：用户当前请求结束时触发。 http.ServerResponse是返回给客户端的信息： 1，response.WriteHead(statusCode, [headers]): 向客户端发送响应头。2，response.write(data, [encoding]): 向请求的客户端发送响应内容。3，response.end([data], [encoding]): 结束响应，告知客户端所有发送已经完成。如果不调用，客户端将永远处于等待状态。 章五1，建立网站基本结构： express -t ejs microblog 2，创建应用实例： express.createServer() 3，控制权转移： express在处理路由规则时，会优先匹配先定义的路由规则，后面的规则将会被屏蔽，可使用next()进行转移。 4，路径匹配： app.get(‘/user/:username’, function(req, res) { res.send(‘user: ‘ + req.params.username); 5，模板引擎ejs，有如下三种标签： 123&lt;% code %&gt;：JavaScript代码；&lt;%= code %&gt;：显示替换过html特殊字符的内容；&lt;%- code %&gt;：显示原始html内容。 6，关闭layout： app.set(‘view options’, { layout: false });指定layout：function(req, res) { res.render(‘userlist’, { title: ‘后台管理系统’, layout: ‘admin’ }); }; 从而指定admin.ejs作为页面布局。 7，片段视图： partials。 8，视图助手： 静态视图助手可以是任何类型的对象，包括接受任意参数的函数，并且访问到的对象必须是与用户请求无关的；动态视图助手只能是一个函数，这个函数不能接受参数，但可以访问req合res对象； 章六1，在不显式指定文件模块扩展名的时候，Node.js会分别试图加上.js、.json和.node扩展名。.js是JavaScript代码，.json是JSON格式的文本，.node是编译好的C/C++代码。 2，当require遇到一个既不是核心模块，又不是以路径形式表示的模块名称时，会试图在当前目录下的node_modules目录中来查找。如果没有找到，则会在当前目录的上一层中的node_modules目录中继续查找，反复执行这一过程，直到遇到根目录为止。 3，加载缓存： Node.js模块不会被重复加载，这是因为Node.js通过文件名缓存所有加载过的文件模块，所以以后再访问到时就不会重新加载了。注意，Node.js是根据实际文件名缓存的，加载两次，也不会重复加载，解析到的文件是同一个。 4，控制流问题： 循环陷阱，可用forEach、闭包、let解决。","link":"/2016/03/18/Node.js%20%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E4%B9%A6%E6%91%98/"},{"title":"Node.js 微信公众号开发小试","text":"申请服务器: 微信公众号的开发需要使用一台用于接收并处理消息的服务器, 此处推荐申请腾讯的免费云主机, 点我去申请吧, 每天九点半开抢, 我选择的服务器镜像是Ubuntu, 关于如何在服务器上配置Node环境, 可参考我另一篇博客使用 Linux 系统开发Web前端. 公众号开发的原理就是通过设置一个接收接口, 一旦开启开发者模式, 微信服务器将转发消息至该接口. 申请服务器微信公众号的开发需要使用一台用于接收并处理消息的服务器, 此处推荐申请腾讯的免费云主机, 点我去申请吧, 每天九点半开抢, 我选择的服务器镜像是Ubuntu, 关于如何在服务器上配置Node环境, 可参考我另一篇博客使用 Linux 系统开发Web前端. 公众号开发的原理就是通过设置一个接收接口, 一旦开启开发者模式, 微信服务器将转发消息至该接口. 接入开发步骤填写服务器配置 验证服务器地址的有效性完成配置后, 服务器将收到来自微信的GET验证请求, 该请求包括如下参数: signature 微信加密签名, 使用开发者填写的token参数和请求中的timestamp参数、nonce参数进行加密 timestamp 时间戳 nonce 随机数 echostr 随机字符串, 当验证通过时, 返回该字符串给微信服务器, 从而完成验证 验证流程 将token、timestamp、nonce三个参数进行字典序排序 将三个参数字符串拼接成一个字符串进行sha1加密 开发者获得加密后的字符串可与signature对比，标识该请求来源于微信 验证有效性的代码 123456789101112131415161718192021app.get('/wechat', (req, res) =&gt; { var token = &quot;quanru&quot;; var signature = req.query.signature; var timestamp = req.query.timestamp; var echostr = req.query.echostr; var nonce = req.query.nonce; var oriArray = [nonce, timestamp, token]; oriArray.sort(); var original = oriArray.join(''); var shaObj = new jsSHA(original, 'TEXT'); var scyptoString = shaObj.getHash('SHA-1', 'HEX'); if (signature == scyptoString) { //验证成功 res.send(echostr); } else { //验证失败 res.send(false); }}); 依据接口文档实现业务逻辑以文本消息为例, 如下为微信转发至服务器的文本消息: 12345678&lt;xml&gt;&lt;ToUserName&gt;&lt;![CDATA[toUser]]&gt;&lt;/ToUserName&gt;&lt;FromUserName&gt;&lt;![CDATA[fromUser]]&gt;&lt;/FromUserName&gt;&lt;CreateTime&gt;1348831860&lt;/CreateTime&gt;&lt;MsgType&gt;&lt;![CDATA[text]]&gt;&lt;/MsgType&gt;&lt;Content&gt;&lt;![CDATA[this is a test]]&gt;&lt;/Content&gt;&lt;MsgId&gt;1234567890123456&lt;/MsgId&gt;&lt;/xml&gt; 消息处理: 123456789101112 app.post('/wechat', (req, res) =&gt; { res.writeHead(200, {'Content-Type': 'application/xml'}); var content = req.body.xml.content; turingRobot(encodeURI(content)).then((data) =&gt; { var response = JSON.parse(data); var resMsg = autoReply(req.body.xml, response.text);console.log(resMsg); res.end(resMsg); });}); 其中turingRobot用于向图灵机器人发送用户消息: 1234567891011121314151617181920const request = require('request');function getTuringResponse(info) { if(typeof info !== 'string') { info = info.toString(); } let options = { method:'GET', url: 'http://www.tuling123.com/openapi/api?key=13a74dbd0f6b45d69ac49334e7027742&amp;info='+info }; return new Promise((resolve, reject) =&gt; { request(options, (err, res, body) =&gt; { if (res) { resolve(body); } else { reject(err); } }); })}module.exports = getTuringResponse; 自动回复模块autoReply: 12345678910111213function autoReply(requestData, info) { if(requestData.msgtype == 'text') { var resMsg = '&lt;xml&gt;' + '&lt;ToUserName&gt;&lt;![CDATA[' + requestData.fromusername + ']]&gt;&lt;/ToUserName&gt;' + '&lt;FromUserName&gt;&lt;![CDATA[' + requestData.tousername + ']]&gt;&lt;/FromUserName&gt;' + '&lt;CreateTime&gt;' + parseInt(new Date().valueOf() / 1000) + '&lt;/CreateTime&gt;' + '&lt;MsgType&gt;&lt;![CDATA[text]]&gt;&lt;/MsgType&gt;' + '&lt;Content&gt;&lt;![CDATA['+info+']]&gt;&lt;/Content&gt;' + '&lt;/xml&gt;'; } return resMsg;}module.exports = autoReply; One More Thing使用微信的官方的Node中间件, 可更方便高效地开发公众号传送门 123456789101112131415161718192021222324const turingRobot = require('./turingRobot');const autoReply = require('./autoReply');const wechat = require('wechat');module.exports = function(app) { //使用中间件 //传入这三个配置, 自动帮你验证 var config = { token: 'quanru', appid: 'wxdc6410f1001d787b', encodingAESKey: 'IJwymet3h2KzGSTPxMnITc25pGiSzSlWCXHUhcvQRzc' }; app.use('/wechat', wechat(config, (req, res, next) =&gt; { //用户的消息以对象的形式返回到该变量 var message = req.weixin; var content = message.Content; turingRobot(encodeURI(content)) .then((data) =&gt; { var response = JSON.parse(data); //默认回复文本消息, 支持多种格式回复, 如图像, 音乐 res.reply(response.text); }); }));} 附上精美PPT地址附上精美代码地址 参考nodejs微信开发—自动回复的实现微信公众平台开发概述—接入指南(需要登录)使用 Linux 系统开发Web前端","link":"/2016/08/14/Node.js%20%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E5%BC%80%E5%8F%91%E5%B0%8F%E8%AF%95/"},{"title":"Node.js 的 Morgan 模块与 Cluster 模块","text":"这段时间鼓捣Node.js，跟着《Node.js 开发指南》把 Microblog 给实现了一下，由于该书撰写于 2012 年，Node 版本目前最新已更新至 v5.9.0，且 Express 也发生了相当大的变化，导致很多书中代码已经不符合当前的版本了。关于实现部分，可参考这篇文章：《nodejs开发指南》微博实例express4.x版 。BTW，目前 express 启动命令改为 npm start，该命令执行 bin 目录下的 www 文件，相当于直接执行 node ./bin/www。此处简要记录下该书第六章提到的关于日志与多核CPU的优化问题，并介绍一款 debug tool。整个 microblog 的代码已放至 github 上：摸我 一、前言这段时间鼓捣Node.js，跟着《Node.js 开发指南》把 Microblog 给实现了一下，由于该书撰写于 2012 年，Node 版本目前最新已更新至 v5.9.0，且 Express 也发生了相当大的变化，导致很多书中代码已经不符合当前的版本了。关于实现部分，可参考这篇文章：《nodejs开发指南》微博实例express4.x版 。BTW，目前 express 启动命令改为 npm start，该命令执行 bin 目录下的 www 文件，相当于直接执行 node ./bin/www。 此处简要记录下该书第六章提到的关于日志与多核CPU的优化问题，并介绍一款 debug tool。 整个 microblog 的代码已放至 github 上：摸我 二、Morgan 模块の日志生成 安装 morgan 与 file-stream-rotator，后者用于生成 daily log： npm i morgan file-stream-rotato –save-dev 编辑 app.js 文件，将其引入，此外还有 node 自带模块 fs： 123var FileStreamRotator = require('file-stream-rotator');var logger = require('morgan');var fs = require('fs'); daily log 生成： 123456789var logDirectory = __dirname + '/log';//日志文件存放目录fs.existsSync(logDirectory) || fs.mkdirSync(logDirectory);//检查目录是否存在，若不存在则新建var accessLogStream = FileStreamRotator.getStream({ date_format: 'YYYYMMDD',//日期格式 filename: logDirectory + '/access-%DATE%.log',//日志文件命名方式 frequency: 'daily',//生成频率，可以是每两小时:2h，每分钟：1m verbose: false//详细与否});app.use(logger('combined', {stream: accessLogStream}));//其中 combined 为生成格式 该插件的具体使用方法参见： morgan ，包括生成格式与配置选项等，也可自行组合配置生成格式。 三、Cluster 模块の生成子进程该模块为 Node.js 自带的核心模块，用于生成与当前进程相同的子进程，允许父子进程共享端口，可充分利用当代服务器的多核CPU。 www 文件中存放服务器创建代码，因此编辑该文件，添加如下代码：12345678910111213141516&quot;use strict“;//后面用到了箭头函数，属于es6，因此设定严格模式，否则不支持var cluster = require('cluster');//模块引入var numCPUs = require('os').cpus().length;//获得当前系统的 cpu 数量if(cluster.isMaster) {//若当前进程为主进程，则 fork 新进程，数目为cpu数量 for(let i = 0; i &lt; numCPUs; i++) { cluster.fork(); } cluster.on('exit', (worker, code, signal) =&gt; { console.log(`worker ${worker.process.pid} died`); }); } else {//否则，创建服务器主进程 //share TCP connection server.listen(port); server.on('error', onError); server.on('listening', onListening); } 此时查看进程管理器，发现有多个 node 进程，而原来的代码则只有两个： 四、Node.js 除虫工具我就不搬运了，它基于 electron，模仿 chrome 的 devtool，会用 chrome 的 devtool 就会这个，反正很吊：devtool ，github 地址为：devtool 安装： npm install -g devtool 此处启动命令为： devtool bin/www –watch 断点之后，按 ctrl + r，重启服务器即可。 进入断点后，点击 esc 可打开一个执行在当前作用域内的控制台，可以修改一些变量然后继续执行。 当使用 cluster 时，无法进入 get 与 post 请求函数内部，我提了个 issue。 高级使用方法见上述网址咯。","link":"/2016/03/18/Node.js%20%E7%9A%84%20Morgan%20%E6%A8%A1%E5%9D%97%E4%B8%8E%20Cluster%20%E6%A8%A1%E5%9D%97/"},{"title":"Node.js 调试方式集锦","text":"本文讨论了几个流行的 Node.js 调试方式，包括 Chrome，VSCODE，ATOM，WebStorm Chrome：优势 不依赖编辑器； 不依赖配置； Node.js 默认已经集成，官方推荐； 与前端代码调试工具一致； dev-tools 工具强大 劣势 无法在源文件上进行断点 VSCODE：优势 可在源文件上进行断点 劣势 需要一定的配置 WebStorm：优势 可在源文件上进行断点 劣势 需要一定的配置 ATOM：优势 可在源文件上进行断点 劣势 功能比较弱，不如 VSCODE 个人建议：如果本身就使用 VSCODE，WebStorm 的话，建议继续使用对应的 Node.js 调试方式；如果使用 ATOM，Sublime 调试不便的编辑器，推荐使用 Chrome 进行调试。 CHROME 调试配置步骤一：安装 nodemon，监听文件变化以重启 node 服务 $&gt; npm install nodemon -g 步骤二：以 —inspect 参数启动调试 $&gt; NODE_ENV=development nodemon --inspect -w config -w server -x node server/server.js 注： NODE_ENV=development // 传入环境变量 —inspect // 配合 Chrome 进行调试，当同时调试多个 node 服务时，可设置端口以避免冲突：—inspect=9291 -w config -w server // 监听 config 与 server 目录，当上述两个目录内文件发生变化时，自动重启服务 -x node // 默认为 node，可不配置，必要时可配置为 babel-node：-x babel-node server/server.js // node 项目入口文件 步骤三：启动前端调试服务(如果有的话) 步骤四：建议将上述命令配置为 scripts 脚本（此时可在项目内单独安装 nodemon）： 1234567&quot;scripts&quot;: { &quot;start&quot;: &quot;npm run server &amp; npm run dev&quot;, &quot;server&quot;: &quot;NODE_ENV=development nodemon -w config -w server server/server.js&quot;, &quot;start-debug&quot;: &quot;npm run server-debug &amp; npm run dev&quot;, &quot;server-debug&quot;: &quot;NODE_ENV=development nodemon --inspect -w config -w server server/server.js&quot;, &quot;dev&quot;: &quot;nodemon -w config -w scripts scripts/server.js&quot; } 步骤五：打开 Chrome，并打开 dev tool，当你的 Chrome 足够新（60 以上），可点击如下 node.js 图标，进入调试 步骤六：如果当前 Chrome 不支持，请更新，或者使用插件 NIM VSCODE 调试配置步骤一：打开项目目录步骤二：切换到 debug tab： 步骤三：打开 launch.json 文件 步骤四：配置 launch.json12345678910111213141516171819{ &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;type&quot;: &quot;node2&quot;, // 新增类型，方便调试 await/async 语法 &quot;request&quot;: &quot;launch&quot;, &quot;name&quot;: &quot;启动程序&quot;, &quot;verboseDiagnosticLogging&quot;: false, &quot;program&quot;: &quot;${workspaceRoot}/server/server.js&quot;, // 此为 node 工程的入口文件 &quot;cwd&quot;: &quot;${workspaceRoot}&quot;, &quot;runtimeExecutable&quot;: &quot;${workspaceRoot}/node_modules/.bin/babel-node&quot;, // 若未使用 babel-node，例如使用 node8 以上版本，请移除本行 &quot;env&quot;: { &quot;NODE_ENV&quot;: &quot;development&quot; }, &quot;sourceMaps&quot;: true } ]} 步骤五：启动前端调试服务(如果有的话) 步骤六：使用 VSCode 打开源码文件，点击行号左侧以添加断点 步骤七：切回 debug tab，启动 node.js 服务，即可调试 WebStorm步骤一：打开项目目录，展开 Run 菜单，选择 Edit Configurations 步骤二：新建 Node.js 调试配置 步骤三：配置参考 注： Node interpreter：node 可执行文件路径，若使用 babel-node 则需要添加 babel-node 路径； Working directory：工作目录； JavaScript file：项目入口文件； Environment variables：环境变量； 步骤四：启动前端调试服务(如果有的话) 步骤五：使用 WebStorm 打开源码文件，点击行号左侧以添加断点 步骤六：点击启动调试服务 ATOM 调试配置步骤一：安装插件：a. xatom-debug b. xatom-debug-nodejs 步骤二：打开 node 项目入口文件，并在箭头处切换需要调试的项目目录，之后点击箭头右侧的 Node.js 按钮进入配置 步骤三：配置调试 步骤四：启动前端调试服务(如果有的话) 步骤五：使用 ATOM 打开源码文件，点击行号左侧以添加断点 步骤六：点击启动调试服务","link":"/2017/08/15/Node.js%20%E8%B0%83%E8%AF%95%E6%96%B9%E5%BC%8F%E9%9B%86%E9%94%A6/"},{"title":"React 状态的逻辑复用","text":"关于 React 状态的逻辑复用史！","link":"/2020/05/30/React%20%E7%8A%B6%E6%80%81%E7%9A%84%E9%80%BB%E8%BE%91%E5%A4%8D%E7%94%A8/"},{"title":"Schema 与下载条","text":"由于我司有个网站还没有对应的移动端版本，因此需要在移动端访问该网站时，在底部显示一条下载条，而且点击该下载条时需要满足以下两个需求： 点击下载时，如果本机已经安装该软件，则尝试打开对应软件； 点击下载时，如果本机未安装该软件，则跳转到对应系统的下载地址。 需求由于我司有个网站还没有对应的移动端版本，因此需要在移动端访问该网站时，在底部显示一条下载条，而且点击该下载条时需要满足以下两个需求： 点击下载时，如果本机已经安装该软件，则尝试打开对应软件； 点击下载时，如果本机未安装该软件，则跳转到对应系统的下载地址。 schema协议 schema类似自定义url协议，我们可以通过自定义的协议来打开自己的应用，形如： 代码如下 复制代码myapplink:// 例如 facebook的fb:// itunes的itms-apps:// 还有短信也是类似的sms://如果要打开一个app，最简单的方式是通过一个链接，如我们在html中这样写： 代码如下 复制代码打开我的app 下载条配置由于该下载条在PC页面中使用，因此无法使用rem单位，我将下载条统一做成了png，并通过引入js文件来配置不同的下载条： 12&lt;script src=&quot;dlBar.js&quot; data-imgurl=&quot;下载条图片地址&quot; data-itunesurl=&quot;苹果下载地址&quot; data-androidurl=&quot;安卓下载地址&quot; data-schemaurl=&quot;schema url&quot;&gt;&lt;/script&gt; 生成考虑到兼容性，其中下载条使用原生原生的方法生成对应的css与html部分，其代码如下： 123456789101112function creatHtml () { var st = document.createElement(&quot;style&quot;); var cssText = document.createTextNode(&quot;.bottom-bar { display: none; position: fixed; left: 0; bottom: 0; width: 100%; } .dl-btn { position: absolute; right: 17.5%; width: 18.75%; } .cl-btn { position: absolute; right: 3.4375%; width: 4.6875%; } .dl-img { width: 100%; display: block; }&quot;); st.setAttribute(&quot;type&quot;, &quot;text/css&quot;); st.appendChild(cssText); var heads = document.getElementsByTagName(&quot;head&quot;); if(heads.length) { heads[0].appendChild(st); } document.body.insertAdjacentHTML(&quot;beforeEnd&quot;, '&lt;div id=&quot;barab&quot; style=&quot;display: none; position: fixed; left: 0; bottom: 0; width: 100%;&quot;&gt;&lt;img style=&quot;width: 100%; display: block;&quot; src=&quot;'+ imgSrc +'&quot; alt=&quot;下载条&quot;&gt;&lt;span style=&quot;position: absolute; right: 14.8%; bottom: 20%; width: 18.55%; height: 63%&quot;&gt;&lt;/span&gt;&lt;span style=&quot;position: absolute; right: 3.4375%; bottom: 35%;width: 4.6875%; height: 30%;&quot;&gt;&lt;/span&gt;&lt;/div&gt;');} 配置使用的下载条图片、APP store跳转地址、Android的下载地址以及schema地址。 主要代码1234567891011121314151617181920212223242526272829303132333435function downloadApp () { var dlNow = document.querySelector(&quot;#barab span&quot;); var closeBtn = dlNow.nextElementSibling; closeBtn.addEventListener(&quot;touchend&quot;, function (event) { downBar.style.display = &quot;none&quot;; }); dlNow.addEventListener(&quot;touchend&quot;, function (event) { var t; var clickTime = new Date(); var ifr = document.createElement('iframe'); ifr.src = schemaSrc; ifr.style.display = 'none'; document.body.appendChild(ifr); if(ios) { t = window.setTimeout(function () { var timeOutTime = new Date(); if (!clickTime || timeOutTime - clickTime &lt; 600 + 200) { window.location = itunesSrc; } }, 600); } if(android) { t = window.setTimeout(function() { var endTime = Date.now(); if (!clickTime || endTime - clickTime &lt; 600 + 200) { window.location = androidSrc; } }, 600); } window.onblur = function() { clearTimeout(t); }; });}})(); 代码思路 首先创建一个iframe，将schema赋值给它的src属性； 当点击下载按钮时，记录点击时的时间戳，并将iframe添加到body中，紧接着设置一个setTimeout； 如果该程序已经安装，便会尝试使用schema协议打开app； 如果该程序没有被安装，则所设置的定时器会在600ms后执行，并记录定时器触发的时间，与点击的时间如果相差200ms，则跳转到对应系统的下载地址。 原理 如果schema地址跳转成功，说明程序已经安装，则后续代码如何执行已不重要； 否则，没跳转成功的话，说明程序尚未安装，因此程序跳转到对应下载地址； 其中时间差就是为了等待schema地址打开软件的，如果不设置等待时间的话，程序会马上执行跳转部分，也就没有schema什么事了。 完整代码","link":"/2015/08/16/Schema%E4%B8%8E%E4%B8%8B%E8%BD%BD%E6%9D%A1/"},{"title":"git checkout 与 git reset 备忘录","text":"一直没搞清 git checkout 与 git reset, 昨晚特地建了一个仓库进行实验, 特此备忘. git 的三块区域与仓库 工作区 (working directory) 即当前编辑的文件所处的区域 暂存区 (stage index) 即当你执行 git add 命令之后, 对应的文件便加入暂存区 历史记录区 (history) 即当你执行 git commit 之后, 暂存区的修改便会作为一个 commit 记录, 存入历史记录区 以上都发生在本地仓库, 直到执行 git push 之后, 便将本地仓库的修改 (历史记录区) 推送到远程仓库. 此外还有个 HEAD 的概念, 它默认指向当前分支的最新的一个 commit. git checkout git checkout 平时主要用于切换分支 git checkout – 用于丢弃工作区的改动. 其中 ‘–’ 用于区分切换分支的 git checkout, 假设如下一种情况: 当前有个分支叫做 ‘ master’, 此时当前目录下也有一个叫做 ‘master’ 的文件, 此时你想撤销对工作区中对 ‘master’ 文件的修改, 便执行 git checkout master, 但是该命令只是让你切换到 ‘master’ 分支. 如图, 当我们修改一个文件后, 查看 git status, 给出了如下提示, 要么使用 git add 将修改过的文件提交到暂存区, 要么使用 git checkout – 丢弃当前工作区对该文件的修改. 可以发现执行 git checkout – 之后, README.md 恢复到修改之前. git reset git reset HEAD 丢弃暂存区的修改, 使改动退回到工作区 如下图, 当在添加 ‘second modify’ 到 README.md 文件后, 将此次修改通过 git add 命令将其添加到暂存区, 终端提示使用 git reset HEAD 用于取消暂存区的修改, 其中 git reset HEAD 代表将当前 HEAD 指向当前 commit. 其中 HEAD 可省略, 即默认指向当前 commit. 如下图, 继续修改 README.md, 在文件中追加 ‘ third modify’, 此时查看 git status, 发现除了原有的暂存区修改 (绿色文字) 之外, 还有第二次修改的当前工作区修改 (红色文字), 接着执行 git reset HEAD, 终端提示 ‘重置后取消暂存的变更’, 此时查看 git status, 发现暂存区不见了, 只剩当前工作区, 查看工作区中的 README.md 文件, 发现原有的暂存区修改被撤销了, 且不影响工作区文件. ‘重置后取消暂存的变更’如果 git reset ‘commitID’ 呢, 此时所谓 ‘重置’ 指的是将当前指针指向对应的 ‘commitID’, 而 ‘取消暂存的变更’ 指的是撤销最近一次 ‘git add’; 这样分开看, 就很好理解了. git reset –soft/–mixed/–hard如图, 带参数的 git reset 不能 (或不建议) 作用于某个路径, 因此只要用于操作 commitID. –soft重置 HEAD 的指针到指定的 commitID, 同时保持暂存区与工作区不变; 如图, 将 HEAD 指针重置到两个 commit 之前, 发现当前分支落后远程分支两个 commit, 此时查看暂存区 (git diff –cached) 和 工作区 (cat README.md), 可以发现这两个区域的文件相比 git reset –soft 之前都没有变化. –mixed重置 HEAD 的指针到指定的 commitID, 同时撤销暂存区, 而工作区则不变; 如图, 此处与 git reset soft,hard,mixed之区别深解 中所述的 –mixed 有出入, 并没有 ‘并且重置index以便和HEAD相匹配’; –hard重置 HEAD 的指针到指定的 commitID, 同时重置暂存区与工作区与其对应的 commitID 相匹配; 如图, 执行 –hard 重置之后, 暂存区与工作区全都消失了, 此时可以选择 git pull, 重新与远程分支一致, 也可使用 git push –force, 强行将远程分支的 commit 记录与本地仓库保持一致. 不过一般建议使用 git revert, 回退的同时保存 commit 的历史记录. 到底哪个才是默认参数有些文章说 –mixed 是默认参数, 也有些说 –soft 是默认参数 如图, 可知默认参数是 –mixed","link":"/2016/10/04/git%20checkout%20%E4%B8%8E%20git%20reset%20%E5%A4%87%E5%BF%98%E5%BD%95/"},{"title":"一种实用新型 Obsidian 实践之构建我的第二大脑 🧠","text":"本文将以 Obsidian 为例，分享我使用 Obsidian 构建第二大脑的实践！ 本文内容已经过时，更多内容请参考官网 LifeOS！ This article is also available in English. 前言什么是 Obsidian？ 官网上它是这么自我介绍的： Obsidian is the private and flexible note‑taking app that adapts to the way you think. Obsidian 是一款私密且灵活的笔记应用程序，可以适应您的思维方式。 我主要看中它丰富的插件生态，你如果喜欢 Vscode，那你大概率也会喜欢 Obsidian，只不过 Vscode 用于写代码，而 Obsidian 用于记笔记。 第一大脑 VS 第二大脑第一大脑即我们的真实大脑，只要我们还活着，那这个大脑就不停地在运转，执行的任务比如知识管理、任务管理、目标管理等，大多数时候我们并不能一脑多用，因此第一大脑更像是一个 CPU，各式各样的任务在抢占 CPU 分片。当需要处理的任务多起来的时候，大脑将不堪重负，因为大脑既要处理当前的任务，又要保持其它任务的上下文，用以切换任务，使得我们无法专注于当前任务的执行，此时需要一个外置的系统来辅助第一大脑，它就是第二大脑。 第二大脑即一个外置的系统，如果把第一大脑比作 CPU 的话，第二大脑更像是存储系统，它就像第一大脑与真实世界之间的一道缓存，减轻了第一大脑的负担，使其能够专注于当前事项。它可以类比为内存和硬盘，只不过内存相对于硬盘，它与 CPU（第一大脑）沟通更加频繁，读取速度也更快。这个存储系统存放当前第一大脑无需时刻关注的事物，当然，这些事物得由第一大脑思考决 - 定是否有存放的必要，内容可以是记录、待办、流程，载体可以是文本、图片、视频。 举一个例子，当我们使用第二大脑进行任务管理的时候，重要紧急的事项存放在内存，不重要不紧急的事项存放在硬盘；本周任务存放在内存，本月任务可能就存放在硬盘；因此通过借助第二大脑，我们就能够无压力专注在当下，在有必要的时候再切换上下文。 本文将以 Obsidian 为例，分享我构建第二大脑的实践！你说它是第二大脑，但是从不同角度审视这个大脑，我也可以称它为『LifeOS』，因为无论从生活还是工作，我都记录在上面；我也可以称它为『可编程个人生产力系统』，我在上面写了不少代码，用来做一些查询和自动化的事情，也是我用来管理任务和目标的生产力系统；甚至它还有点像『Monorepo 工程』，每个文件夹就是一个项目，项目中的 README.md 就像是 Package.json 一样描述了当前项目的元信息。 📢 注意：这套系统不是那种自上而下、先有这套流程而去实现的，是我在使用 Obsidian 过程中逐渐形成的，而且也一直在迭代中，姑且把当前的版本定为 1.0，现在分享出来是想给大家一点点灵感，去完善自己的系统！目前已经编写了一个 Obsidian Periodic PARA 插件来支持这套系统！借助这个插件，你不需要有任何编程基础，通过简单的可视化点击就能轻松地创建周期笔记和 PARA 笔记！ 我的实践 我采用两套系统，一个是知识管理系统，另一个是周期笔记，前者以项目/领域/资源为维度，进行知识管理，后者以时间为维度，进行任务/目标/时间管理。 两套系统 知识管理：采用 PARA 系统 Projects -&gt; 项目是与目标相关的一系列任务，有截止日期 Areas -&gt; 领域是一种活动领域，需要在一定时间内保持一定的标准 Resources -&gt; 资源是持续感兴趣的话题或主题 Archives -&gt; 存档是来自上述三个类别的非活动条目 周期笔记 长期：自顶向下，专注于长期的目标 三年记 年记 季记 短期：自底向上，专注于短期的任务 月记 周记 日常：捕捉想法洞见，实现自我觉察；耗时统计，确保聚焦于项目 日记 其中 PARA 越靠近 Projects，它的可操作性就越高；周期笔记越长期，它的可预测性就越低； 这两套系统相当于制造了两个上下文，让我保持聚焦 一个是基于时间的（周期笔记），即我到达某个时间节点，我就基于对应周期笔记作业，且笔记中有足够的上下文； 另一个是基于主题的（PARA），即我想对某个主题进行调查研究的时候，我就基于对应主题的索引（README.md）作业，且笔记中已经收集了不少上下文； 切面子系统 面向切面程序设计 - Wikiwand 在上述两套系统之下，隐藏着任务/目标/时间管理子系统，我主要通过『周期笔记』来管理： 任务管理 通过日记/周记来收集 通过周记/月记来整理 目标管理 通过年记规划年度目标 通过季记分拆年度目标 通过月记拆解待办事项 自上而下整理（通过目标拆解） 自下而上整理（通过收集拆解 -&gt; 日记/周记） 时间管理 通过日记手动统计各个项目的耗时和占比，反馈和调整时间开销 通过日记、周记、月记、季记、年记使用脚本自动统计各个项目的耗时和占比，用于复盘时间开销 你也许会好奇，上述子系统似乎只使用了『周期笔记』，实际上两个父系统之间通过两种方式将各个子系统连接起来。 连接 系统之间如何关联 标签连接 将 PARA 下的一级文件夹作为一种特殊的标签（不一定要与文件名完全一致），在『周期笔记』中使用，那么便可在各个一级文件夹中，以相同的方式进行统一的索引。这样能保证每个 PARA 文件夹下的 README.md 索引有当前主题的所有上下文： 项目连接通过在『知识管理』中立项来生成项目，为了增加对项目的关注，在每类『周期笔记』中均设置有『要事列表』或『项目列表』，比如 日记中的『项目列表』，它是一份当前项目列表的快照，用于统计当天花费在各个项目中的耗时及其占比，确保把足够的时间花在项目上 周记和月记中的『要事维度』，是从本周和本月的日记中自动合并去重得到的一个列表，用于安排项目维度的任务和后续的复盘 季记的『要事维度』，它是一份当前领域列表的快照，用于安排要事维度的目标和后续的复盘 年记中的『要事维度』，是从本年的季记中自动合并去重得到的一个列表，用于设置领域维度的目标和后续的复盘 检索 标签 比如，日记的 节日、休假 标签 索引文件 比如，每个项目的 README.md 索引本项目的任务、日志、上下文 文件夹 比如，每个 PARA 目录使用一致的目录结构 复盘 复盘主要针对本周期内的项目，复盘的同时规划下个周期的任务 周记复盘本周日记，月记复盘每周复盘，季记复盘每月复盘 快速上手下载 点我下载 使用 obsidian 打开即可享用 创建笔记 通过左上角笔记创建模块，快速创建日记、周记、月记、季记、年记 通过左上角笔记创建模块，快速 PARA 笔记，即项目、领域、资源、存档 『日记』与『项目 README』 用于日常管理，包括项目列表、日常记录、习惯打卡、精力分配、今日完成等模块 日记中的『项目列表』是一份当前项目（即 Projects 目录下）的快照 『周记』与『月记』 用于安排周度和月度任务，包括任务和复盘模块 在周记和月记中，『要事维度』是一份本周期日记『项目列表』的快照合集（自动生成） 在周记和月记中，『复盘』主要针对本周期内的项目进行 『季记』与『年记』 用于制定季度和年度目标，包括目标和复盘模块 在季记中，『要事维度』是一份当前领域（即 Areas 目录下）的快照 在年记中，『要事维度』是一份本周期季记『要事维度』的快照合集（自动生成） 在季记和年记中，『复盘』主要针对本周期内的领域进行 『PARA 索引』与『任务索引』 『捕获』与『表达』首先介绍一个概念，即 CODE 模型，其中： C 是 Capture - 捕获：把引发共鸣的信息收集起来 O 是 Organize- 组织：对收集的这些信息进行整理，即 PARA D 是 Distill - 提炼：提炼这些内容的精髓，筛选最有用的信息 E 是 Express - 表达：和他人分享，在实践中去应用你的知识 熟悉 PARA 的朋友肯定看出来了，这个模型其实就是 Tiago Forte 在 《打造第二大脑》中提出的，它是包含了 PARA 组织法的上层模型，其中的 O 即指的 PARA 组织法。 而我的实践就是在『捕获』目录中，暂时存放一些标记过的文章，同时借助『-1. 捕获/README.md』文件去索引分散在日记中，被打上 #PARA/Capture 标签的记录，这样方便我在特定的时间节点，比如每周末、每月末、每季度末进行回顾梳理，首先将标记文章梳理到各个 PARA 主题目录，接着将日记中的某些灵感记录转化成一些明确的待办事项； 接下来说说『表达』，我会把自己的博客放到表达中，也会在日记中记录一些零碎的记录并打上 PARA/Express 标签，这些都是自己内化后的输出，如果这个输出需要进一步发布到指定的社交平台，比如知乎、小红书，我会顺手把它记录为任务，当我在回顾『5. 表达/README.md』文件索引的时候，发现有待办任务，直接一项一项的完成即可。 实践中的小 Tips缓存区机制把不重要不紧急的事情，通过创建任务，快速放到缓存区（Inbox），把大部分注意力保持在『项目』中 任务列表任务的记录不要有太大的心里压力，记下的不代表一定要做；记下了能够减轻你的心里负担，不用老想着这个事，也不怕忘记这个事；我有非常多的任务记下来了，后续经过评估也确实没实现。 我们只要保证一定的机制能回顾到这些被记下的任务即可，比如 使用 tasks 插件来做一些任务列表的 查询视图 每份周期笔记中都有当前周期收集的 任务列表 项目索引文件中的 任务列表 任务提醒我认为任务有三种提醒方式 强提醒，比如抢茅台、抢演唱会门票这种，到点就要进行，就需要强提醒，通过手机设置闹钟提醒即可 弱提醒，某天需要完成，比如信用卡还款、贷款还款之类的，通过 GTD 软件设置提醒即可 列表类，用于记录任务，让你后续统筹安排的，根据需要可以转成强提醒或者弱提醒事件，有点类似 GTD 里的收件箱 微习惯 我会在日记中列一些微习惯，切记不是任务，完不完成都行，主要用来提醒『这些微习惯，你今天考虑做一下吗？』，即在我有『能力』和『动机』的时候，起到『提示』的作用，比如： 微习惯 一听到闹钟响就起床喝水 一下车就戴耳机听小宇宙 一上地铁就打开微信读书 一到工位就写下三件待办 一到十点半就开始干正事 易于重构在每份周期笔记中，相同功能的模块都使用同一个语句，比如『本周期收集的任务』，都是通过插入如下查询语句，而『本周期』的变量是当前的文件名提供，这就使得批量重构所有周期文件变得十分方便，只需要批量替换即可： 123```PeriodicPARATaskRecordListByTime``` 善用快捷键设置全局一致的快捷键，使得无论在哪个软件都能使用同一个快捷键唤起同一个功能，如下是我的部分设置： 光标移动 规律：Control + 方向首字母/VIM 方向 示例： A: Head of line E: End of line F/L: Forward B/H: Backward N/J: Next line P/K: Previous line W: Delete a word(Backward) D: Delete a character(Forward) 窗口管理 规律：Command + Option + 首字母 示例： L: 左侧半屏 R: 右侧半屏 C: 居中 M: 最大化 [: 显示/隐藏左侧栏 ]: 显示隐藏右侧栏 ‘: 显示/隐藏底栏 T: 新建 Tab(更具体的窗口下，顶层 Tab 使用 Command + T) W: 关闭 Tab(更具体的窗口下，顶层 Tab 使用 Command + W) J: 下一个 Tab K: 上一个 Tab 文档编辑 规律 1：Command + Option + 数字/符号 示例： 1: Markdown 一级标题 2: Markdown 二级标题 3: Markdown 三级标题 4: Markdown 四级标题 5: Markdown 五级标题 6: Markdown 六级标题 无序列表: ~ 删除线: - 功能类 规律：Control + 首字母 示例： C: Copy link(Obsidian block link, Arc browser link, VScode git link) D: Download I: Add to inbox K: Quick Search","link":"/2023/06/18/%E4%B8%80%E7%A7%8D%E5%AE%9E%E7%94%A8%E6%96%B0%E5%9E%8B%20Obsidian%20%E5%AE%9E%E8%B7%B5%E4%B9%8B%E6%9E%84%E5%BB%BA%E6%88%91%E7%9A%84%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91%20%F0%9F%A7%A0/"},{"title":"不要叫我程序猿,我是前端开发工程狮","text":"上学期期末开始了解到前端开发，当时忙于做实验、写论文、投文章，也就没深入了解，当时觉得挺好玩的，所以屁颠屁颠的去借了一本《HTML与CSS入门经典》，当时那个迷茫啊。一开始是借了《Java编程思想》和《Android 4.X从入门到精通》，想着复习下Java，然后动手写写安卓，然后毕业搞安卓开发，后来也没看下去。那本入门经典是很基础的那种，看起来轻轻松松地，兴趣也慢慢培养起来。 引言上学期期末开始了解到前端开发，当时忙于做实验、写论文、投文章，也就没深入了解，当时觉得挺好玩的，所以屁颠屁颠的去借了一本《HTML与CSS入门经典》，当时那个迷茫啊。一开始是借了《Java编程思想》和《Android 4.X从入门到精通》，想着复习下Java，然后动手写写安卓，然后毕业搞安卓开发，后来也没看下去。那本入门经典是很基础的那种，看起来轻轻松松地，兴趣也慢慢培养起来。之后隔了一个寒假回来，论文却被拒了。差不多也是那时开始决心好好学习前端，所以借了一些特别经典的前端书籍，例如：《CSS权威指南》、《精通CSS:高级Web标准解决方案》、《JavaScript编程精解》、《JavaScript权威指南》。 学习当时满腔热情的学着，一开始先看的《JavaScript编程精解》，看起来真是费劲，后来看犀牛书就通顺多了，本来有编程基础的程序猿，上手JavaScript是挺快的。然后配合网上的视频，动手做一些小例程，进步非常快，好在导师也没烦我，完全放羊状态，更能全心全意地学习。期间还写了一篇论文，跟之前被拒绝的一起再投出去了，好在这两篇都中了，毕业要求达到了，就更有底气学自己喜欢的东西。 2048网页版 面试&amp;笔试 一直想出去实习，俗话说：“纸上得来终觉浅”，之前从来没有找过工作，没什么经验。注册了智联招聘，隔三差五就有人打电话，不是了解情况就是让我去面试，可见前端需求还是相当大的。其中我去了两家，一家是GXG男装，想想也好玩，我竟然去了，我同学问我是不是去应聘模特？哈哈。面试官问我有做过什么项目没，我说没有，都是自己在看书；还各种给我灌输，打杂是应该的，而且几乎不问前端的知识点，真心觉得好坑。另一个公司我忘了叫什么，从事体检方面的公司，也是找人打杂，也不问我学到的知识点。 跟同学一起去杭州参加了蘑菇街的宣讲会，笔试被虐得很惨，都是基础的算法题，太久没接触，考完研都忘光了。在杭州呆了一晚，跟爹娘会合去乌镇玩了。回来报名参加了一个网上的在线面试，50RMB。一共有两个面试官跟我视频，简直一问三不知，打击真的太大了，之后针对他问的一些缺漏，有针对的研究了一阵子，进步跟打击一样大哈。 实验室的同学介绍了一个打算出来创业做旅游网站的程序员给我，说是需要前端工程师，联系之后才知道，项目还没启动呢。不过他给我布置了个小任务，实现一个网页版的微信，写了一周。又说要适配移动端，用响应式媒体查询语句，真是麻烦到死，适配移动端这种需求应该一开始就要考虑到，反正改得很难受。之后也没联系了，不过通过这次也学到了很多。 网页版微信 本人作为知乎脑残粉，在一个关于前端入门的回答里，偶然发现了百度推出的一个公益性质的前端学院。该项目托管在github上，第一次看到github，挺惊奇的，之前备份都是直接备份整个目录，然后才敢改代码。在百度的这个项目上，做了三个任务，学了很多细致入微的知识点，虽然最后一个“待办事项”的任务由于一开始没发现要本地存储的这个需求，就屁颠屁颠的写起来，后来这个需求也就不了了之了。 百度前端学院任务 短信收到挖财的在线笔试通知，这在线笔试还要求摄像头常开，这逼格也是够了。笔试做起来很顺利，开放题也是惊奇。笔试自我感觉挺好的，可就是一直等不到面试通知，某一天又在各大招聘网站闲逛，突然挖财就来电话通知面试了，真是“没有一点点防备，也没有一丝丝…”。订了隔天的票就奔向省城了，面试也挺顺利的，都是基础问题，期间换了一个面试官。这位面试官问了稍微深入的一些问题，末了，给了两个任务让我回去做，一个是实现截图实时显示，另一个任务则是给你一个psd，让你照着做，要求适配移动端的网页。 任务一、任务二也遇到了一些问题，不过后来都解决了，面试官挺满意的，说是确定要我。又问我有没有兴趣再做一题，“又是没有一点点防备”。我能说没兴趣么？当然，任务三也完成了。 任务一 任务二 任务三 实习打算端午过后去杭州实习，好喜欢杭州啊，也算是正式入了前端这个坑了，对这几个月认真学习有了个交代。","link":"/2015/06/14/%E4%B8%8D%E8%A6%81%E5%8F%AB%E6%88%91%E7%A8%8B%E5%BA%8F%E7%8C%BF-%E6%88%91%E6%98%AF%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E7%8B%AE/"},{"title":"为前端应用开发一个本地接口 mock 服务","text":"无法忍受没有 mock 服务的日子 背景本地开发时，目前通过直连测试环境和修改代码两种方式调试接口，前者十分依赖后端的数据状态和服务稳定，使得本地开发调试容易受外部影响；后者修改代码容易造成问题，如果涉及多处代码改动，还需要对这些分散的代码进行撤销改动，从而造成不必要的心智负担。因此，以上两种方式都不能良好地做到前后端分离开发。理想情况下，前端应在开发阶段使用 api mock 服务，当流程走通之后，再切到测试环境与后端联调。 有现成的工具吗？有的，目前公司现有 mock 平台 能支持这个需求，但是由于我们的应用使用的是 gateway 的方式调用后端方法，即所有接口都是访问同一个 api 地址，并通过区分 query 参数标识不同 dubbo 方法，因此该 mock 平台不能很好地支持这种场景，需要通过对同一 api 地址添加多种 http method 的接口，接着通过设置不同『期望』以达到根据不同 query 参数返回不同数据的效果。此外，目前项目没有编写 mock 接口配置的习惯，不可能对百来个接口进行一一配置调试，因此接入 mock 不能做到一蹴而就。 怎么办？针对以上情况，我觉得可以通过开发一个本地接口 Mock 服务如下方式解决： 在本地启动一个 mock 服务，当本地存在对应接口的 mock json 文件时，优先使用 mock 数据，否则调用测试环境。这样既保证了项目在开发调试阶段不受外部影响，做到完全地前后端分离开发（通过切换环境决定连接 mock 接口还是测试环境接口），又能够逐步实现上百个接口的 mock。 为什么使用本地 mock 服务，而不是在线 mock 服务？无论是前公司，还是现公司，都存在多个在线 mock 服务，有的是前端团队主导的，有的是测试团队主导的，这些服务要么仅有少数项目使用，要么几乎没人用。这些服务大多没有解决如下两个问题： 老项目如何快速接入？每个老项目涉及几十上百个接口，几乎不可能主动去接入这些平台。 接口直接如何避免相互影响？同一个工程，A 同学修改了 M 接口返回 N 以测试流程 X，而 B 同学需要修改 M 接口返回 P 用以测试流程 Y，这就造成了冲突（开发阶段应尽量避免外界的影响，开发一个功能实在不应该因为 mock 接口返回的修改或者测试环境的出错而阻塞） 而本地 mock 服务可以较好地通过缓存测试环境或线上环境的接口数据解决问题1；通过将接口数据缓存在本地解决问题2，也就是说 A 同学在本地的操作是不会影响到 B 同学的调试的。 不过，我认为本地 mock 和在线 mock 二者并不冲突，二者可以相互同步接口数据，这样既保证了在线维护接口的便利，又能隔离开发时，在线编辑接口返回值导致的相互影响。 还能怎么玩？对于这种缓存测试数据到本地的模式，我想到一个有趣的使用方式，当与后端联调时，通过特定参数启动本地开发服务，使得当前每个接口都强制同步测试环境数据；接着将联调的页面跳转路径点击一遍，便可将此次流程涉及的接口悉数缓存；重启本地开发服务，这样同一个操作流程皆使用了刚刚缓存的接口，做到『流程回放』，从而不需要依赖后端重复造数据，非常方便 B 端这种流程多，步骤长（这往往十分依赖状态，而状态在每个步骤之后往往会改变）的联调。","link":"/2019/07/21/%E4%B8%BA%E5%89%8D%E7%AB%AF%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E6%9C%AC%E5%9C%B0%E6%8E%A5%E5%8F%A3%20mock%20%E6%9C%8D%E5%8A%A1/"},{"title":"从 line-height 到 0.5 px","text":"前几天发现写的一段代码，其中line-height（1.7rem）与height（1.7rem）相等，font-size（1.1rem）；在ios设备上文字垂直居中，而在android设备上竟上下缝隙相差1px（其实我看不出来是不是一像素，我猜的）。 问题前几天发现写的一段代码，其中line-height（1.7rem）与height（1.7rem）相等，font-size（1.1rem）；在ios设备上文字垂直居中，而在android设备上竟上下缝隙相差1px（其实我看不出来是不是一像素，我猜的）。 为什么我试了好久，我以为是字体的原因，毕竟ios用了自己设计的字体，虽然我的大魅族也是自己设计（白永祥：这并没有什么卵用）。后来我发现当我把字体设为1rem时，android上的字体看起来就居中了，而ios无论如何都是居中的，百思不得其解啊； 机智如我，获取了魅族MX5的设备宽度（360），所以根据我的页面计算出来的rem对应为18px，也就是说line-height实际高度为1.7rem=1.718px=30.6px，font-size实际大小为1.1rem=1.118px=19.8px； 而由于1px是最小单位，那么这个带小数点的单位到底是向上取整还是向下取整，亦或是四舍五入，我在我的MX5上设置了“林”这个字的如下字号：70、70.4、70.5、70.6、71px；接着屏幕截图，放到ps里量了下，高度分别是71、71、71、71、72；事实证明是向下取整的，不要问我为什么是71而不是70，因为我不告诉你。 那么，就有了一个很好的解释了：line-height - font-size = (30 - 19)px = 11px，11px上下没法均摊，结果上面5px，而下面的空隙就是6px，导致了整体向上偏移，然而这没法解释ios设备为什么就可以上下均摊。 再试试，字号改为1rem的情况：line-height - font-size = (30 - 18)px = 12px，上下各自为6px，因此能居中。 0.5px今天在搜寻如何在retina设备上实现1px，无意中发现ios8以上的版本，是支持0.5px的，也就是说ios8设备上的精度是0.5px，所以11px除以2，各自都是5.5px，难怪可以垂直居中。 ios8中的近似那么，70.4、70.5、70.6这些字号，在ios8这种精度为0.5px的设备上是如何近似的呢？答案就是：只要不是整数，通通设为70.5px，这能叫居中近似吗？哈哈~ 那么问题来了1px边框到底如何在非ios8的retina屏幕下实现呢？今天看到一篇博客里的一句话，我激动了： 要是没有最后一句话该多好啊~ 网上搜了好久，找到一个不错的方法： 伪类 + transform 原理是把原先元素的 border 去掉，然后利用 :before 或者 :after 重做 border ，并 transform 的 scale 缩小一半，原先的元素相对定位，新做的 border 绝对定位 单条 border: 12345678910111213141516.hairlines li{ position: relative; border:none;}.hairlines li:after{ content: ''; position: absolute; left: 0; background: #000; width: 100%; height: 1px; -webkit-transform: scaleY(0.5); transform: scaleY(0.5); -webkit-transform-origin: 0 0; transform-origin: 0 0;} 四条 border: 1234567891011121314151617181920.hairlines li{ position: relative; margin-bottom: 20px; border:none;}.hairlines li:after{ content: ''; position: absolute; top: 0; left: 0; border: 1px solid #000; -webkit-box-sizing: border-box; box-sizing: border-box; width: 200%; height: 200%; -webkit-transform: scale(0.5); transform: scale(0.5); -webkit-transform-origin: left top; transform-origin: left top;} 样式使用的时候，需要结合 JS 代码，判断是否 Retina 屏 123if(window.devicePixelRatio &amp;&amp; devicePixelRatio &gt;= 2){ document.querySelector('ul').className = 'hairlines';} 那么问题又来了有没有更好的方法啊？","link":"/2015/09/14/%E4%BB%8Eline-height%E5%88%B00-5px/"},{"title":"使用 Jest 测试 Node.js","text":"使用 Jest 测试你的 Node.js 应用 目的 增强代码的健壮性 及时发现未被覆盖的代码逻辑 项目交接或重构更加放心 工具1. 安装1npm install --save-dev jest supertest 2. 配置 package.json123456789101112131415161718192021222324&quot;scripts&quot;: { &quot;test&quot;: &quot;NODE_ENV=development jest&quot;, &quot;test-watch&quot;: &quot;npm test -- --watch&quot;,},&quot;jest&quot;: { &quot;verbose&quot;: true, &quot;notify&quot;: true, &quot;collectCoverage&quot;: true, &quot;testEnvironment&quot;: &quot;node&quot;, &quot;modulePaths&quot;: [ &quot;&lt;rootDir&gt;/server&quot; ], &quot;roots&quot;: [ &quot;&lt;rootDir&gt;/__tests__&quot; ], &quot;testPathIgnorePatterns&quot;: [ &quot;__tests__/(fixtures|__mocks__)/&quot; ], &quot;coverageReporters&quot;: [ &quot;html&quot;, &quot;text&quot;, &quot;text-summary&quot; ] } 3. 添加 gitignore 在 .gitignore 配置文件中增加忽略 coverage 目录 4. 运行12npm test # 全部测试npm run test-watch # 开启 watch 模式, 只运行修改的测试文件 5. jest 命令的实用参数 npm test -- fileName 文件名支持正则，比如 npm test -- server/*；支持部分匹配，比如 npm run test -- controllers/login npm test --bail [-- fileName] 当遇到失败的用例时，立马退出，方便查看报错信息 npm test --watch [-- fileName] 监听测试文件修改，仅重新执行所修改的测试用例 npm test --watchAll [-- fileName] 监听测试修改，重新执行所有测试用例 6. 目录结构约定 测试文件：__tests__ mock 模块：__mocks__ 辅助工具：__test__/fixtures 123456789__tests__├── fixtures├── __mocks__│ └── request.js└── server ├── controllers │ └── thread │ └── index.test.js └── server.test.js 测试维度 正向测试：这个函数是否按照其声明的那样实现了非常基本的功能？ 负向测试：代码是否可以处理非期待值？ 测试覆盖率源代码被测试的比例, 有四个测量维度 行覆盖率（line coverage）：是否每一行都执行了？ 函数覆盖率（function coverage）：是否每个函数都调用了？ 分支覆盖率（branch coverage）：是否每个if代码块都执行了？ 语句覆盖率（statement coverage）：是否每个语句都执行了？ 1234567891011-----------|----------|----------|----------|----------|----------------|File | % Stmts | % Branch | % Funcs | % Lines |Uncovered Lines |-----------|----------|----------|----------|----------|----------------|All files | 100 | 85.71 | 100 | 100 | | logger.js | 100 | 85.71 | 100 | 100 | |-----------|----------|----------|----------|----------|----------------|Test Suites: 1 passed, 1 totalTests: 9 passed, 9 totalSnapshots: 0 totalTime: 0.836s, estimated 1sRan all test suites. 附：单元测试准则 文档较长，建议饭后查看 测哪些东西 server - 启动是否正常 middlewares - 加载正常，请求时正常工作 controllers - 请求特定路由，看响应是否是符合预期 services - 调用特定方法，返回结果符合预期，边界情况 routes、lib - 普通测试 测试用例撰写一个普通且完备的单测文件 1234567891011121314151617181920212223242526272829303132333435363738394041describe('api 映射模块', () =&gt; { // 在所有单测运行前执行，用于准备当前 describe 模块所需要的环境准备，比如全局的数据库； beforeAll(() =&gt; { }) // 在每个单测运行前执行，用于准备每个用例（it）所需要的操作，比如重置 server app 操作 beforeEach(() =&gt; { }) // 在每个单测运行后执行，用于清理每个用例（it）的相关变量，比如重置所有模块的缓存 afterEach(() =&gt; { jest.resetModules() }) // 在所有单测运行后执行，用于清理环境，比如清理一些为了单测而生成的“环境准备” afterAll(() =&gt; { }) // 注：以上四个方法均支持返回一个 Promise，此时 Jest 将等待该 Promise resolve 后继续 it('当 env 为默认的 development 环境时，返回 localhost 地址', async() =&gt; { process.env.NODE_ENV = '' const API = require('lib/api') expect(API).toThrow() // 期望 API 抛错 expect(API('')).toMatch(/localhost/) // 期望返回包含 'localhost' 字段 }) it.only('当 env 为测试环境时，返回测试环境地址', async() =&gt; { // 仅执行本测试用例，常用于调试当前用例 process.env.NODE_ENV = 'test' const API = require('lib/api') expect(API('get_items')).toMatch(/test.baidu.info/) })}) ​ 附：expect 常用语句，更多请查看官方 expect 文档 12345678910111213.toBe(value) // 期望值为 value.toEqual(value) // 期望两个对象内容完全相等.toBeDefined() // 期望被定义.toBeFalsy() // 期望为 Falsy.toBeTruthy() // 期望 Truthy.toMatch() // 期望符合，支持字符串和正则对象.toThrow() // 期望抛错.toHaveBeenCalled() // 方法被调用.toHaveBeenCalledWith(arg1, arg2, ...) // 方法被以参数 arg1, arg2, ... 调用.toHaveBeenCalledTimes(number) // 方法被调用次数为 number 次// 以上 expect 语句均可取非，形式如下：not.toBe() mock 示例jest 中 mock 主要有两种作用：屏蔽外部影响：123456789101112131415161718192021222324// number-add.js...const debug = require('debug')module.exports = (a, b) =&gt; { debug('value a: ', a) debug('value b: ', b) return a + b}...// number-add.test.js// mock debug 模块，使得每次 require 该模块时，返回自动生成的 mock 实例jest.mock('debug')...it('返回 a 和 b 的和', () =&gt; { const add = require('utils/number-add') const total = add(1, 2) expect(total).toBe(3)})... 模拟外部调用：123456789101112131415161718192021222324252627// string-add-async.jsconst fetch = require('node-fetch')module.exports = async (apiA, apiB) =&gt; { const stringA = await fetch(apiA) const stringB = await fetch(apiB) return stringA + stringB}// string-add-async.test.jsdescribe('测试 string-add-async 模块', () =&gt; { it('返回接口 a 和 接口 b 所返回的字符串拼接', async () =&gt; { // mock node-fetch 模块 jest.mock('node-fetch', () =&gt; { return jest .fn() .mockImplementationOnce(async () =&gt; 'Hello ') // 首次调用时返回 'Hello ' .mockImplementationOnce(async () =&gt; 'world!') // 第二次调用时返回 ' world!' }) const addAsync = require('utils/string-add-async') const string = await addAsync('apiA', 'apiB') expect(string).toBe('Hello world!') })}) 如何正确的 mock 一个模块 此处以 string-add-async 模块为例 12345678910111213141516171819202122232425262728293031323334353637383940414243// 方式一describe('测试 string-add-async 模块', () =&gt; { it('返回接口 a 和 接口 b 所返回的字符串拼接', async () =&gt; { // mock node-fetch 模块 jest.mock('node-fetch', () =&gt; { return jest .fn() .mockImplementationOnce(async () =&gt; 'Hello ') // 首次调用时返回 'Hello ' .mockImplementationOnce(async () =&gt; 'world!') // 第二次调用时返回 ' world!' }) const addAsync = require('utils/string-add-async') const string = await addAsync('apiA', 'apiB') expect(string).toBe('Hello world!') })})// 方式二describe('测试 string-add-async 模块 2', () =&gt; { it('返回接口 a 和 接口 b 所返回的字符串拼接', async () =&gt; { // mock node-fetch 模块，使得每次 require 该模块时，返回 mock 实例 jest.mock('node-fetch') const fetch = require('node-fetch') fetch .mockImplementationOnce(async () =&gt; 'Hello ') // 首次调用时返回 'Hello ' .mockImplementationOnce(async () =&gt; 'world!') // 第二次调用时返回 ' world!' const addAsync = require('utils/string-add-async') const string = await addAsync('apiA', 'apiB') expect(string).toBe('Hello world!') })})// 方式三// __tests__/__mocks__/node-fetch.jsmodule.exports = async apiUrl =&gt; { return apiUrl} 注：强烈不建议使用方式三，因为该方式影响范围比较大，不过适合 屏蔽外部影响 的情况 mock 实例 当一个模块被 mock 之后，便返回了一个 mock 实例，该实例上有丰富的方法可以用来进一步 mock；且还给出了丰富的属性用以断言 mockImplementation(fn) 其中 fn 就是所 mock 模块的实现 mockImplementationOnce(fn) 与 1 类似，但是仅生效一次，可链式调用，使得每次 mock 的返回都不一样 mockReturnValue(value) 直接定义一个 mock 模块的返回值 mockReturnValueOnce(value) 直接定义一个 mock 模块的返回值（一次性） mock.calls 调用属性，比如一个 mock 函数 fun 被调用两次：fun(arg1, arg2); fun(arg3, arg4);，则 mock.calls 值为 [['arg1', 'arg2'], ['arg3', 'arg4']] 附：更多 mock 实例属性与方法详见官方文档 测试示例完整代码暂不提供工具模块的测试方法参看本文档 mock 示例 部分服务启动的测试方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748const supertest = require('supertest')describe('server 服务', () =&gt; { let app, server beforeEach(async () =&gt; { app = await require('server') // 禁用 koa-logger 日志输出 app.log.level('fatal') }) afterEach(() =&gt; { if (server) { server.close() } app = null server = null }) const request = () =&gt; { if (!server) { server = app.listen(0) } return supertest(server) } it('启动正常', async () =&gt; { expect(request).not.toThrow() }) it('app 抛出异常处理', async () =&gt; { app.use(async ctx =&gt; { app.emit('error', new Error('app error'), ctx) ctx.body = 'ok' }) await request() .get('/throw-error') .expect(200) .then(res =&gt; { expect(res.text).toBe('ok') }) })}) 中间件测试的方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758const supertest = require('supertest')describe('错误中间件', () =&gt; { let app, server beforeEach(async () =&gt; { app = await require('server') // 可以试试取消注释这一句，可以发现由于没有重置模块缓存，导致测试用例 3 使用了用例 2 中的 server 实例 jest.resetModules() }) afterEach(() =&gt; { if (server) { server.close() } app = null server = null }) const request = () =&gt; { if (!server) { server = app.listen(0) } return supertest(server) } it('抛出异常-中间件出错（自定义错误）', async () =&gt; { app.use(async (ctx, next) =&gt; { await Promise.reject(new Error('中间件出错')) await next() }) await request() .get('/throw-error') .expect(200) .then(res =&gt; { expect(res.body.error).toBe('中间件出错') }) }) it('app 抛出异常-系统异常，请稍后再试（默认错误）', async () =&gt; { app.use(async (ctx, next) =&gt; { await Promise.reject(new Error('')) await next() }) await request() .get('/throw-error') .expect(200) .then(res =&gt; { expect(res.body.error).toBe('系统异常，请稍后再试') }) })}) 接口测试的方法// add-api.js 1234567891011121314151617181920const AddService = require('./add-service')module.exports = async router =&gt; { router.get('/add', async ctx =&gt; { const { a, b } = ctx.query const numberA = Number(a) const numberB = Number(b) if (Number.isNaN(numberA) || Number.isNaN(numberB)) { throw new Error('参数必须为数字！') } const projectService = new AddService(ctx) const ret = await projectService.add(numberA, numberB) // 处理请求成功后的数据 ctx.body = `接口计算结果：${ret}` })} // add.test.js 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566jest.mock('./add-service')const Service = require('./add-service')const addApi = require('add-api')const Router = class { constructor (ctx) { return new Proxy({}, { get (target, name) { return async (path, callback) =&gt; { callback(ctx) } } }) }}describe('测试 add 接口', () =&gt; { it(`当 a=1 且 b=2，返回 '接口计算结果：1 + 2 = 3'`, async () =&gt; { const mockedAdd = jest.fn(async () =&gt; '1 + 2 = 3') const ctx = { query: { a: '1', b: '2' } } Service.mockImplementation(() =&gt; { return { add: mockedAdd } }) const router = new Router(ctx) await addApi(router) expect(mockedAdd).toBeCalledWith(1, 2) // or expect(mockedAdd.mock.calls).toEqual([[1, 2]]) expect(ctx.body).toBe('接口计算结果：1 + 2 = 3') }) it(`当 a=1 且 b=xxx，接口报错`, async () =&gt; { const mockedAdd = jest.fn(async () =&gt; '1 + 2 = 3') const ctx = { query: { a: '1', b: 'xxx' } } Service.mockImplementation(() =&gt; { return { add: mockedAdd } }) const router = new Router(ctx) try { await addApi(router) } catch (error) { expect(error).toBeEqual(new Error('参数必须为数字！')) } expect(mockedAdd).not.toBeCalled() })}) 服务层的测试方法// project-service.js 12345678910const add = require('utils/number-add')module.exports = class { add (a, b) { const ret = add(a, b) return `${a} + ${b} = ${ret}` }} // project-service.test.js 12345678910111213141516describe('测试 project service', function() { it('测试 service 的 add 方法', async () =&gt; { jest.mock('utils/number-add') const add = require('utils/number-add') const Service = require('project-service') const service = new Service() add.mockImplementation(() =&gt; 100) const ret = await service.add(1, 2) expect(ret).toBe('1 + 2 = 100') })}) FAQconsole.log 有时无效 试试 console.warn mock 没起作用 mock 模块是否在多个测试用例中相互影响了； mock 操作是否在 require 之后； 是否需要在 beforeEach 中执行 jest.resetModules() 或 jest.resetAllMocks()； 是否需要单独执行 mock 的实例方法mockReset； 参考 Jest - Getting Started","link":"/2018/02/22/%E4%BD%BF%E7%94%A8%20Jest%20%E6%B5%8B%E8%AF%95%20Node.js/"},{"title":"使用 Linux 系统开发Web前端","text":"为什么使用 Linux? Mac纵有千千万万的好，作为学生党来说，毕竟其较高的价格让的确让许多我”党“人士望而却步，去年刚进公司实习的时候，使用的是Win7，对于Windows下的命令行体验真是无力吐槽，特别是对于Web前端来说，非常多的工具都运行在终端内，比如不计其数的Node.js工具，后来改用Linux就舒畅多了。 为什么使用 LinuxMac纵有千千万万的好，作为学生党来说，毕竟其较高的价格让的确让许多我”党“人士望而却步，去年刚进公司实习的时候，使用的是Win7，对于Windows下的命令行体验真是无力吐槽，特别是对于Web前端来说，非常多的工具都运行在终端内，比如不计其数的Node.js工具，后来改用Linux就舒畅多了。 发行版本选择发行版本个人还是推荐主流之一的Ubuntu，建议使用Gnome Flavor版本，简洁优雅的Gnome3桌面，使用起来相当顺手，Ubuntu自带的Unity界面丑到我想哭。也可使用Fedora，它默认就是Gnome3，二者主要是包管理器不一样，前者使用apt-get，后者使用yum。 系统安装建议使用U盘刻录安装，推荐刻录软件UNetbootin，将下载好的ISO文件通过UNetbootin烧进U盘，安装前记得空出一块磁盘，系统本身占用很小，10G虽然够，还是建议20G吧。安装过程不细说了，网上教程一大堆，建议第一次安装还是对着教程来吧，记得备份重要文件。这是我安装后的桌面： 开发软件前端开发所需的软件大都有对应的Linux版本，比如Sublime、Atom、Charles、WebStorm、Chrome，大家可自行Google下载。 安装Git：sudo apt-get install git 配置github(如果你使用的话，否则可略过)： 配置git用户名和邮箱 git config user.name &quot;用户名&quot; git config user.email &quot;邮箱&quot; 在config后加参数 –global 可设置全局用户名和邮箱。 生成ssh key ssh-keygen -t rsa -C &quot;邮箱&quot; 然后根据提示连续回车即可在~/.ssh目录下得到id_rsa和id_rsa.pub两个文件，id_rsa.pub文件里存放的就是公钥。 上传公钥到github 复制公钥内容，接着登录github，进入Settings，选择 SSH and GPG keys，点击 New SSH key。 测试是否配置成功 ssh -T git@github.com 如果配置成功，则会显示： Hi username! You’ve successfully authenticated, but GitHub does not provide shell access. 安装Node.js：方法零：使用apt安装 sudo apt-get install nodejs npm ln -s nodejs /usr/bin sudo apt-get install openjdk-9-jdk sudo npm i -g wnpm sudo wnpm i -g wac-cli 方法一：使用包管理器安装（推荐新手使用）安装 5.x 版本： sudo apt-get install curl curl -sL https://deb.nodesource.com/setup_5.x | sudo -E bash - sudo apt-get install nodejs 方法二：使用 NVM 安装并管理 node，建议有一定 Linux 命令行和 Node.js 经验的人使用： https://github.com/creationix/nvm 方法三：也可采用编译源码的方式安装，打开终端，其步骤如下。 安装 build-essential，即软件编译工具集，用于从源代码编译和安装软件。 sudo apt-get update sudo apt-get install build-essential 克隆分支并进入node目录： git clone https://github.com/nodejs/node.git &amp;&amp; cd node 切换到一个你需要的稳定分支： git checkout v4.0.0-rc 编译并安装： ./configure make sudo make install 查看node安装位置并添加软连接： whereis node sudo ln -s /usr/local/bin/node /usr/bin/node sudo ln -s /usr/local/bin/npm /usr/bin/npm 查看node版本 node -v PlayOnLinux &amp;&amp; PhotoShop令人头疼的PhotoShop，Adobe没有对应的Linux版本，此处使用Wine方案，推荐安装PlayOnLinux，你只要有exe文件就好了，Wine的环境配置不需要你操心，PlayOnLinux帮你搞定。安装PlayOnLinux只需输入一条命令搞定： sudo apt-get install playonlinux 装好后，打开PlayOnLinux，如何安装请看下图： 再往后就是下一步、下一步、下一步、完成。 wine软件可能出现中文乱码，可参看彻底消除wine中文乱码 Zsh终端： 首先安装zsh： sudo apt-get install zsh 切换shell： chsh -s which zsh 重启系统即可生效 安装主题，本人推荐主题bullet-train-oh-my-zsh-theme 其它软件越过长城 搜狗输入法 WPS QQ easystroke鼠标手势： sudo apt-get install easystroke audacious听歌： sudo apt-get install audacious","link":"/2016/04/17/%E4%BD%BF%E7%94%A8%20Linux%20%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91Web%E5%89%8D%E7%AB%AF/"},{"title":"使用 Node.js 将珍藏的 bash 脚本封装成命令行工具","text":"阐述如何将一个常用的 bash 脚本融入 npm 生态之中，此处以最近遇到的一个 CR 提交脚本为例。 背景作为程序猿，大家或多或少地都用过 GitHub 上的 merge request 功能。当然，除了这类 Code Review 方式，不少公司都有自己的 Code Review 平台，我司也不例外，我们使用了类似 Gerrit 的工具，此处我们暂且以 Gerrit 指代。由于最近在治理工程时，需要全面切（要）到（求）使用 Gerrit 进行 CR 提交。发现 Gerrit 提交命令不是那么好记，经常需要先 git push，接着被拦截报错之后，再根据提示复制命令行，再次执行方可成功提交 Gerrit 。作为攻城狮，这有点难以忍受了！！！ 诉求一、有没有单独的命令让我直接提交 Gerrit？二、有没有命令行工具，我直接安装就能使用？三、在 git push 后，经由 git hook 拦截后按需提交 Gerrit？解决一、有没有单独的命令让我直接提交 Gerrit？ 答：有的，有次同事见我提交 Gerrit 不顺畅，转发了一个 bash 脚本给我：你把它复制到 /usr/local/bin 目录下，就能直接使用 gerrit 执行了，珍藏脚本如下（gerrit）： 12branch=$(git symbolic-ref --short -q HEAD)git push origin HEAD:refs/for/${branch} 二、有没有命令行工具，我直接安装就能使用？ 答：有的，既然都有脚本了，作为前端开发，必须用心爱的 Node.js 封装一个命令行工具，只需两步即可使用：首先执行 npm i @dd/gerrit-cli -g ；接着在工程目录下执行 gerrit 即可使用。 三、在 git push 后，经由 git hook 拦截后按需提交 Gerrit？ 答：有的，如果你还觉得全局安装命令行太麻烦，或者害怕新人来了一脸懵逼。那么，还可以借助 git hook 进行拦截，用户只需要『无脑地』执行 git push 即可。当然前端这块有现成的 git hook 神器，它就是人见人爱的哈士奇，至于其它语言生态，大家找找应该有的。 我们来看看如何封装上述脚本吧！ 实现方式1.配置命令如何能让别人安装你的 npm 包时，就能在终端中执行命令行呢？只需对你的 npm 包的 package.json 添加 bin 字段： 123456789{ &quot;name&quot;: &quot;your-first-cli-package&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;你的第一个命令行工具&quot;, &quot;main&quot;: &quot;index.js&quot;, &quot;bin&quot;: { &quot;yourCommand&quot;: &quot;index.js&quot; },} 之后别人使用 npm i -g your-first-cli-package 时，即可在终端中执行 yourCommand 调用你的 index.js 的逻辑啦。如果使用局部安装的方式，即 npm i your-first-cli-package，命令行将被安装到 node_modules/.bin/yourCommand 下，其内容正是 index.js 的内容。此时可编辑 npm scripts 调用。 2.调用声明由于我们使用 Node.js 实现，因此命令行对应的入口 js 文件（此处即 index.js）需要声明当前文件使用 node 执行： 12#!/usr/bin/env node// 此处编写 yourCommand 命令的逻辑 3.编写逻辑此处实现得比较粗糙，目前就一个命令，因此未引入 args 这类包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#!/usr/bin/env nodeconst execa = require('execa')const chalk = require('chalk')const run = async () =&gt; { let branch = '' let result = '' try { console.log(chalk.gray(`获取当前分支...`)) const { stdout } = await execa.command('git symbolic-ref --short -q HEAD') branch = stdout console.log(chalk.gray(`当前分支为: ${branch}`)) } catch (error) { console.log(chalk.red(`获取分支失败：${error.message}`)) process.exit(1) // 以失败码退出，用于 git hooks 拦截识别 } try { console.log(chalk.gray(`检查当前分支是否推送过远程仓库...`)) await execa.command(`git rev-parse --abbrev-ref ${branch}@{upstream}`) console.log(chalk.gray(`当前分支存在于 ${branch} 远程仓库...`)) } catch (error) { console.log( chalk.yellow(`当前分支 ${branch} 未推送远程仓库 ${error.message}`), ) try { console.log(chalk.green(`尝试推送分支 ${branch} 至远程仓库`)) const { stderr } = await execa.command( `git push --set-upstream origin ${branch} --no-verify`, ) result = stderr } catch (error) { console.log(chalk.red(`提交 gerrit 失败：${error.message}`)) process.exit(1) } } try { console.log(chalk.gray(`对分支 ${branch} 提交 gerrit ...`)) const { stderr } = await execa.command( `git push origin HEAD:refs/for/${branch} --no-verify`, ) result = stderr } catch (error) { console.log(chalk.red(`提交 gerrit 失败：${error.message}`)) process.exit(1) } console.log(chalk.green(`${branch} 提交 gerrit 成功，信息如下：\\n${result}`)) process.exit(0) // 以成功码退出，用于 git hooks 通过识别}run() 使用方式全局使用（非前端工程推荐使用）安装npm i @dd/gerrit-cli -g 执行 确保在 git 工程目录下 gerrit 示例 JavaScript 工程局部使用（前端工程推荐使用）安装npm i @dd/gerrit-cli --save-dev 在 package.json 中新增 gerrit scripts12345&quot;scripts&quot;: { ... &quot;cr&quot;: &quot;gerrit&quot; ... }, 执行 确保在 git 工程目录下 npm run cr 示例 和 husky 配合使用在 package.json 中新增 gerrit scripts12345678910&quot;scripts&quot;: { ... &quot;cr&quot;: &quot;gerrit&quot; ...},&quot;husky&quot;: { &quot;hooks&quot;: { &quot;pre-push&quot;: &quot;npm run cr&quot; }}, 执行 确保在 git 工程目录下 git push 示例 TODO 新增子命令支持生成 gerrit 的配置文件 打印对应的 CR 规范的文档链接，否则新人会懵逼 封装成 SDK 供其它工具调用 总结我们多多少少会遇到类似的场景，以工程化的视角去封装它，让原本 npm 生态之外的 bash 脚本也能融于无形！","link":"/2020/10/02/%E4%BD%BF%E7%94%A8%20Node.js%20%E5%B0%86%E7%8F%8D%E8%97%8F%E7%9A%84%20bash%20%E8%84%9A%E6%9C%AC%E5%B0%81%E8%A3%85%E6%88%90%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/"},{"title":"函数式编程中的柯里化到底有什么用？","text":"函数式编程最近越来越活跃，去年实习的时候买了一本 《JavaScript 函数式编程》，囫囵吞枣的看了一遍，似懂非懂的， 今年重新看了一遍，现写下这篇博客，谈谈我对柯里化的理解吧。 柯里化柯里化函数为每一个逻辑参数返回一个新函数。(《JavaScript 函数式编程》) 简单说，函数柯里化就是对高阶函数的降阶处理。举个例子，就是把原本：function(arg1,arg2)变成function(arg1)(arg2)function(arg1,arg2,arg3)变成function(arg1)(arg2)(arg3)function(arg1,arg2,arg3,arg4)变成function(arg1)(arg2)(arg3)(arg4)……function(arg1,arg2,…,argn)变成function(arg1)(arg2)…(argn) 作者：小蝶惊鸿链接：https://www.zhihu.com/question/40374792/answer/86268208来源：知乎著作权归作者所有，转载请联系作者获得授权。 举个例子一个参数 强制只接收一个参数 123456789101112131415161718// 接收一个参数自动柯里化function curry (fun) { return function (arg) { return fun(arg); }}// es6 装逼版function curry (fun) { return arg =&gt; fun(arg);}[1, 2, 3, 4, 5].map(parseInt)//[1, NaN, NaN, NaN, NaN][1, 2, 3, 4, 5].map(curry(parseInt))//[1, 2, 3, 4, 5] 两个参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// 普通二参数的加法function normalAdd(x, y) { return x + y;}// 柯里化版本function add(y) { return function(x) { return x + y; }}let add2 = add(2);add2(3);// 5// 普通二参数乘法function normalMultiply(x, y) { return x * y;}// 柯里化版本function multiply(y) { return function(x) { return x * y; }}let multiply2 = multiply(2);multiply2(3);// 6// 自动柯里化function curry2 (fun) { return function (arg2) { return function (arg1) { return fun(arg1, arg2); } }}// es6 装逼版function curry2 (fun) { return arg2 =&gt; arg1 =&gt; fun(arg1, arg2);}let curryAdd = curry2(normalAdd);let curryAdd2 = curryAdd(2);let curryMultiply = curry2(normalMultiply);let curryMultiply2 = curryMultiply(2);curryAdd2(3);// 5curryMultiply2(3);// 6 三个参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 普通版本function normalAddThenMultiply(arr, factor, increase) { let tempArr = arr.map(function(ele, index) { return normalAdd(ele, increase); }); return tempArr.map(function(ele, index) { return normalMultiply(ele, factor); });}normalAddThenMultiply([1, 2, 3], 3, 2);// [9, 12, 15]// 柯里化版本function addThenMultiply(increase){ return function(factor) { return function(arr) { let addStep = curry2(normalAdd); let multiplyFactor = curry2(normalMultiply); let tempArr = arr.map(addStep(increase)); return tempArr.map(multiplyFactor(factor)); } } }let add2Multiply = addThenMultiply(2);let add2Multiply3 = add2Multiply(3);add2Multiply3([1, 2, 3]);// [9, 12, 15]// 自动柯里化function curry3 (fun) { return function (last) { return function (middle) { return function (first) { return fun(first, middle, last); } } }}// es6 装逼版function curry3(fun) { return last =&gt; middle =&gt; first =&gt; fun(first, middle, last);}let curryAddMultiply = curry3(normalAddThenMultiply);let curryAdd2Multiply = curryAddMultiply(2);let curryAdd2Multiply3 = curryAdd2Multiply(3);curryAdd2Multiply3([1, 2, 3]);// [9, 12, 15] 柯里化到底有什么用 每个步骤都是显性调用（消耗一个参数），同时将该步骤的结果缓存（返回匿名闭包，该闭包等待下一个参数），从而暂缓调用，待时机成熟时便可传入下一个参数以便继续调用； 两个参数情况下的应用12345678910111213141516171819202122232425262728293031323334353637383940414243444546 // 用于定义一系列 action actionList = [{ &quot;action&quot;: &quot;isLogin&quot;, &quot;hasCallback&quot;: true }, { &quot;action&quot;: &quot;doLogin&quot;, &quot;hasCallback&quot;: false }, { &quot;action&quot;: &quot;setTitle&quot;, &quot;hasCallback&quot;: true }]; // 批量生成 API 的工厂函数 factory(actionList) { for (let value of actionList) { this[`${value.action}`] = this.generator(value); } } // 简化版本的 API 生成函数 generator(action) { return function(params) { let MyPromise = es6Promise.Promise; action['params'] = params; return new MyPromise((resolve, reject) =&gt; { let callbackId = this.generateId(); this.responseCallbackList[callbackId] = (data) =&gt; { resolve(data); } this.sendAction(action, callbackId); }); } }// 最终的调用方式， 其中 params 是用户调用时才传入的 bridge.setTitle({skin: 'red', color: '#666'}) .then((data) =&gt; { alert(data); }) .catch((err) =&gt; { alert(err); }); 三个参数情况下的应用12345// redux-thunk 中间件export default function thunkMiddleware({ dispatch, getState }) { return next =&gt; action =&gt; typeof action === 'function' ? action(dispatch, getState) : next(action);} 该中间件期待一个第一个参数 { dispatch, getState }, 并返回一个期待一个 next 参数的匿名函数， 由于 next 的值由上一个中间件决定， 因此暂缓调用， 直至传入 next 参数， 最终返回一个新的函数（即加入中间件的 dispatch 函数）， 该函数期待一个 action 参数。 具体调用过程及原理详见：理解 redux 中间件 参考：专属前端坑的函数式编程深入到源码：解读 redux 的设计思路与用法理解 redux 中间件","link":"/2016/10/06/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E6%9F%AF%E9%87%8C%E5%8C%96%E5%88%B0%E5%BA%95%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8%EF%BC%9F/"},{"title":"函数式编程之 Functor","text":"万万没想到，Promise 也属于函数式编程？","link":"/2018/02/05/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E4%B9%8B%20Functor/"},{"title":"前端 Monorepo 在字节跳动的实践","text":"受邀参加第十一届 Top100 峰会，更多详情见文章年底了，看看这100位技术创新带头人如何做复盘？ 分享会背景受邀参加第十一届 Top100 峰会，更多详情见文章年底了，看看这100位技术创新带头人如何做复盘？ 本主题简介 PPT 正文 大家早上好，我是林宜丙，今天我带来的分享主题是《前端 Monorepo 在字节跳动的实践》 简单介绍下我自己，我来自字节跳动的 Web Infra 部门，拥有多年前端工程化的经验，主要帮助前端工程师更好地管理和治理工程 目前负责前端 Monorepo 解决方案的设计及其落地工作，有丰富的 Monorepo 实践和治理经验 今天的分享分为以下五个部分 首先分析『现代前端工程开发』的趋势，并引出字节跳动，前端工程开发面临的痛点 其次简单介绍下 Monorepo，以及它为什么能解决我们面临的痛点 接着针对我们在落地过程中遇到的问题及其实践 再接着分享下自研的方案在字节跳动的落地情况 最后总结当前方案的问题和针对这些问题的展望 OK，我们先来看看现代前端工程开发的几个趋势 首先是前端工种的趋势 第一个趋势是涉及的平台越来越多，Web，Node，客户端和跨平台等 第二个趋势是所能支撑的业务越来越多，复杂度越来越大，特别是近年来前端侧涌现出不少重前端交互的应用，比如搭建类的 figma，文档类的飞书等 第三个趋势是随着上述两个趋势而来地、不可避免地使得前端团队的规模不断增大 上述三个趋势又客观上造成了前端工程的四个趋势，即： 代码规模增大，内部已经出现代码量超过 10G 的大型工程 维护人数增多，一个工程少则十来人，多则四五十人 研发工具增加，不断出现的新工具在一个工程上堆叠，构建方面比如 webpack，rollup，vite 等，测试方面比如 jest，vitest 等 依赖关系复杂，各类项目安装依赖后的 lock 文件大小，足以说明一个工程的复杂依赖关系 那么，在上述这些个趋势下，我们的前端工程开发面临了哪些痛点呢？主要有三个： 其一，项目基建重复，每次新增项目都需要重复配置 Git，构建平台，CI/CD 配置等 其二，代码复用困难，跨项目的代码复用和调试极其繁琐，往往通过发布 npm 包来复用，而这种复用方式又不可避免地遇到更新触达率的问题 其三，工作流程割裂，一个功能往往涉及到多个模块，这时需要分别在各个模块的工程上开发、合码、上线和验证，这是繁琐和割裂的 面对上述痛点，我们该如何更好地组织工程，以提高工程的『可维护性』和『开发效率』？我们给出的答案是 Monorepo！ 接下来我们来简单介绍下 Monorepo 首先我们来看看，什么是 Monorepo 呢？Monorepo 是一种源代码管理的模式，其形式就是将多个项目集中到一个仓库中管理； 与 Monorepo 相对的是 Polyrepo 模式，这种模式各个项目都有独立的仓库。 这里需要注意，Monorepo 的子项目不仅仅是简单地放到一个仓库中维护而已，处理子项目间的关系也十分重要。 子项目也可以是任意的类型，可以是 web 项目，也可以是 node 项目和 Lib 库。 总而言之，Monorepo 就是将多个不同的项目以良好的组织关系放到单个仓库中维护。 在前端领域，大家可能对 Lib 型的 Monorepo 更加熟悉，知名的开源项目比如 React、Vue、Babel 等都采用 Monorepo 方式管理源码，大家看左边这个图，通过将整个系统拆分成多个 package，便于抽象和复用，并且这些 package 往往不需要走线上部署流程，只需要发布到 npm registry 即可。我相信大家在公司中接触和使用到的大部分 Monorepo 项目就是这个类型，但是这类项目往往在商业公司中并不是主流。 除了 Lib 型以外，我们看右边这张图，多个 App 也是可以放到一个仓库中维护的，这就是 App 型的 Monorepo，它包含多个 App 项目，以及项目共享的组件、工具函数等等，App 类型的项目需要走完整的部署流程，App 及其依赖的 Lib 一般不需要发布到 npm registry，这类项目呢，才是商业公司的主流。 在字节跳动，大部分的项目都是 App 应用，我们侧重在这个类型上建设自研的 Monorepo 方案，它的覆盖的人群和覆盖的应用是最多的，因此它的收益也是巨大的；当然支持了 App 型的 Monorepo，自然也就支持了 Lib 型的 Monorepo。 那为什么要采用 Monorepo 呢？接下来我们看看使用 Monorepo 的收益都有哪些？它是如何解决我们遇到的痛点的？ 首先，Monorepo 可以降低多项目的维护成本，从而解决项目基建重复的痛点。 Polyrepo 下，每个项目都需要有同学创建和维护，当创建更多项目的时候，需要更多同学，或者更多精力去创建和维护。 而在 Monorepo 中，只需要少数几个同学负责设立起 Monorepo，所有的项目以及将来的项目都能够在一个仓库中统一维护，从而降低多项目维护成本； 此外，Polyrepo 下，多个项目的基建有很多重复，当团队有多个项目的时候，需要频繁创建 git 仓库，配置 CI、Lint 规则、构建等等，而且为每个项目创建的基建后续都需要有人来维护。 同时将一个项目的调整，同步到其他项目的成本也很高，比如想在 CI 流程中为所有项目加入类型检查，来提高下 ts 项目的质量，那么需要修改每个项目，提交代码，跑 CI，这样成本其实是很高的。而在 Monorepo，只需要创建一套基建，所有子项目，以及未来的子项目都能够接入现有的基建。这些基建的调整和维护，也能够很容易地应用到多个项目。 在 Polyrepo 中，如果要开发多个项目的话，还可能需要来回切换开发环境、切换仓库，link 代码，而在 Monorepo 中可以一键启动多个子项目的调试、构建，从而提高研发效率。 其次，在 Monorepo 下可以很方便的共享代码，从而解决代码复用困难的痛点。 Polyrepo 中，复用公用代码比较困难，需要为公用代码单独维护一个仓库，此外升级、调试流程也十分繁琐低效。 首先是调试很繁琐，公用模块的调试需要手动执行 link，与当前调试的项目关联起来，如果公用模块较多的话，link 步骤将非常繁琐低效。 其次公用模块的升级很繁琐，需要手动管理这种依赖关系，先升级底层的模块，然后发布，最后再升级顶层模块，如果升级完之后发现有问题，这些步骤还得重来一次。此外，推动上层模块更新也不能及时触达。 而在 Monorepo 中，可以直接一键创建公用模块，顶层的模块一键引入公用模块进行开发、调试，底层模块的更改能够直接被上层感知，甚至不需要经过 link 和 npm 发布，即可在本地调试或者部署平台发布，降低了很多重复的工作。 因此，在 Polyrepo 中复用代码比较困难，导致代码复用率比较低，而 Monorepo 中能够很方便的复用代码，抽离新的工具库的成本非常低，这使得大家更愿意做这类抽离工作，提高了代码复用率。 再次，在 Monorepo 里能够实现自动化的多项目工作流，从而解决工作流程割裂的痛点。 如果我的业务需求要涉及到多个项目，在 Polyrepo 模式中，我需要修改多个项目，对多个项目各自提交 commit，每个项目单独跑 CI 流程，如果项目间有依赖关系的话，还需要手动升级项目的依赖版本。例如需要修改图中的三个项目，需要先修改提交底层模块，跑每个模块的 CI 流程，在处理顶层模块时，还得更新底层依赖，接着再跑一次 CI 流程，这一套流程非常繁琐且不连续。 而在 Monorpeo 中，我们可以直接修改多个子项目，只需一次 commit 提交，多个子项目的 CI 和发布流程也是一次性的。从而将整个多项目的工作流自动化，连续化。 我们简单的做一个总结：在 Polyrepo 的模式下，每一个项目都有自己的地址、仓库，这会导致共享代码很困难，基建比较重复，同时工作流也比较割裂。 但是在 Monorepo 里面，我们可以将多个项目的工作流进行规范统一。比如说统一的 CI 流程、 code review 等等。同时，它的代码共享也比较方便，我们可以直接将多个项目共享的组件或者工具函数，抽离出来，便可以在多个项目间复用，当然，多个项目的基建也可以复用。 很多时候一个团队、或者一块业务，他们的项目之间并不是完全割裂的，而是相互联系的，Monorepo 可以很方便的将这些项目组织到一起，进行维护。当团队有多项目需求的时候，推荐使用 Monorepo。 当然，Monorepo 并不是银弹，Polyrepo 也能符合大部分的场景，我们选择 Monorepo 是基于我们遇到的痛点考虑和权衡的，使用 Monorepo 虽然很好地解决我们遇到的痛点，但是也给我们来了不少额外成本，接下来的部分我会着重分享，在采用 Monorepo 方案后，所带来的问题及其实践。 接下来，我们来看看引入 Monorepo 方案后遇到的问题，以及我们在解决这些问题方面的实践 我们的自研方案在实践过程中，主要遇到如下四个方面的问题： 第一个，因为大部分用户习惯于 Polyrepo 的开发，有些甚至是第一次使用 Monorepo，因此存在用户教育，上手门槛的问题 第二个是随着业务的迭代，Monorepo 工程的规模，即子应用数量迅速增多，而一次调试和构建往往涉及多个子应用，从而导致调试、构建性能下降 第三个是 Monorepo 工程的生命周期往往比 Ployrepo 长很多，且参与开发的同学也非常多，如何确保合入工程的代码保持良好的可维护性，也是一个挑战 第四个，字节的业务线众多，号称 App 工厂，那么面对这些不同业务的需求，如何提供丰富的扩展能力呢？ 此处简述我们自研方案的整体架构，主要分为三层，底层是开源工具层，即我们依赖的开源方案，有 npm 包管理器，changesets 版本管理工具，子应用集成工具，比如 jest 和 webpack 等； 顶层是各类业务场景的支撑，有微前端、跨端、BFF、H5、Library 等场景； 中间层是自研的解决方案，方案也包含三个层次，底层是各类基础能力的封装，比如缓存管理、任务管理、依赖管理、子项目管理等；中间是面向用户的功能模块封装，比如 Checker、Builder、Pipeline 等；顶层则是命令行工具，提供各种命令用于调用 Monorepo 的能力； 右侧则是自研方案提供的，基于插件的扩展能力。 架构设计不是本次分享的重点，接下来才是我们的重点，重点分享这套自研方案，在面对上述四个问题时，它是如何实践的。 因为相当多的用户对 Monorepo 项目的开发方式了解不足，习惯于 Polyrepo 的开发，甚至此前都没有接触过 Monorepo 项目的开发。 因此在加强用户教育，降低上手门槛方面，我们紧密贴合字节基础设施，实践有 提供『内置脚手架』工具，使得用户能够一键创建字节常见的应用类型 提供『CI/CD 能力』，适配字节基建的发包和构建流水线 提供『可视化扩展』，通过界面教育和引导用户使用 接下来我们展开说说 内置脚手架方面，我们提供了一种生成器的模版机制来应对子项目的初始化，我们集成字节常见应用类型的模版（Lynx、Gulu、Garfish、组件库/工具库等），此外也能实现自己的模版，模版既可以是项目级别的，也可以是组件级别的。 如图就是生成器的架构图，它由 5 个 stage 组成，比如 prompt 阶段用于收集用户的命令行输入，render 阶段是渲染模版文件阶段，emit 阶段是生成模版文件，然后通过插件机制，使得每个插件能够在上述 5 个 stage 中处理插件的逻辑，我们提供的插件有 Git 仓库创建插件、SCM 创建插件、Npm 依赖安装插件等。 基于生成器的设计，用户能够定制合适自己业务线的、开箱即用的模版，一键创建项目及其 Git 仓库、配置 SCM 构建等，降低了创建项目的成本。 接下来看看 CI/CD 能力方面的实践，如下是架构图，它也是通过插件的机制来实现的，比如 EntryPackagePlugin 用于入口应用探测，BuildDependenciesPlugin 用于构建依赖，RunSubPackageBuildShPlugin 用于执行子应用的 shell 脚本； 为了实现多环境支持，它会对每一种场景都有对应的 Pipeline，比如 GitlabPipeline 和 SCMPipeline，可以简单的理解为，为每个场景维护了一份插件集合，然后在对应的场景执行时会调用对应场景的 Pipeline，每个 Pipeline 都会执行相同的 stage； stage 列表如右边所示，其中 analyzeEntries stage 会分析依赖，得出哪些代码变更了，collect tasks stage 会去调度子项目的任务流水线。 基于插件集的定制，使得当前的 Pipeline 机制在 SCM 和 Gitlab CI 场景下开箱即用，也方便为未来新的场景进行扩展。 这套机制支持了 Gitlab CI 自动发版流水线与 SCM 构建流水线等场景，大大降低了应用接入发版平台和构建平台的成本。 可视化扩展方面，我们通过界面教育和引导用户，提供快捷操作创建子应用、调用各类命令、搜索子应用、配置 Monorepo 等； 从而降低新手用户的上手成本、降低记住命令的心智负担。 因为 Monorepo 势必会让子应用迅速增多，而一次调试和构建往往涉及多个子应用，从而导致调试、构建性能下降，所以我们需要有特别的方案来保障调试和构建的性能。 因此在规模增大导致性能下降方面，我们通过多种方式提高构建效率，实践有 支持『任务并行能力』，采用最大限度的批量任务并行加速 支持『多级缓存能力』，对依赖安装、构建产物、测试结果等实现了多级缓存 支持『按需构建能力』，根据依赖图和代码更改的影响面来构建和测试 接下来看看我们是如何实践任务并行能力的！ 如图所示，根据子项目的依赖关系转成一个任务依赖图，就可以根据这个依赖图进行任务的调度。 看左下角的任务依赖图，它的构建顺序必须符合这样一种要求，即上层的项目构建依赖于底层项目构建的完成。 OK，我们看方式一，一种最朴素的方式，即通过串行排成 DEBCA，这是能够符合构建要求的；但是它的性能是比较低的，比如 D 和 E 是可以并行的，所以第二种方式就是通过把 DE 以及 BC 进行并行处理，这样便能把前面的 5 个步骤加速到 3 个步骤。但这种优化还不够极致，我们发现任务 C 呢，不依赖任务 E 的完成，但是方式二下，任务 C 却得等待 D 和 E 都完成才开始执行。 因此便引出第三种方式，在任务 E 完成后，D 和 C 便可以并行执行；考虑到并行执行涉及的子项目任务过多时，会导致 cpu 过载，因此还配套了并发控制能力，比如右侧的时序图，4 个需要执行的任务，最大的并发数为 2。 接下来看看多级缓存能力的实践，当 monorepo 体积增大以后，每一次开发或者上线都可能涉及到数个子项目，每次都需要对这些子应用进行重新构建，这将会极大地拖慢构建和部署时间。 我们提供了对构建产物进行缓存的能力，能够同时将产物缓存到本地和远程，当相关的子项目没有修改过代码时，将会复用之前的构建产物以减少构建时间。此外，除了构建任务，其它的操作，比如单测任务的执行，也会缓存结果，从而跳过单测任务。 按需构建能力方面的实践，我们支持按影响面执行 CI 流程，通过 diff 更改的代码并进行依赖分析，得到整个 Monorepo 中受影响的项目，并执行其 CI 流水线；否则每次 CI 都会完整的构建整个所有的子项目 此处以一个简单的 Monorepo 为例，依赖关系如图所示，三个 App，一个 Component，一个 Sdk 和工具库 如果一次性全量构建所有应用，耗时约为 17.72秒 如果仅仅改动了 component 模块，那么按需构建的话，我只需要构建 component, app1, app2，此时耗时 8.94秒，节约 50% 的时间 再来看看无缓存的情况下，我仅仅构建 App1, App2, App3 它们的耗时在 10.77秒到16.94秒之间 如果有缓存的话，比如 component, sdk, util 已经构建过，那么再单独构建 App1, App2, App3 时，构建耗时在 7.55秒到9.74秒 之间，大约节约 45% 的时间 因为 Monorepo 会让众多水平不一工程师在同一个仓库里开发，而且仓库每周新增的代码量都是巨大的，所以我们更需要去保障代码质量。 因此在代码防劣化方面，我们通过 Checker 机制进行各类规范的检查，实践有 支持『内置多种 Checker』，确保项目基本的可维护性 支持『自定义 Checker』，使用户能够编写基于各自团队的规范检查 支持『支持自动修复』，自动修复不符合规范的代码或配置 我们内置了多种 Checker，用于保证基本的可维护性，比如统一依赖版本，它会检查各项目的 package.json 下相同依赖版本是否一致等等 此处以统一依赖版本一致为例，我们都知道，如果多个项目使用相同依赖的不同版本容易导致问题，比如 components 和 app 的 react 版本不一致，就会有 react hooks 报错等问题，此时应用这里的第一个 Checker，就能检测这个问题，并自动修复为统一的版本。 如果内置的 Checker 不满足用户对于规范的需求呢？我们也提供了自定义 Checker 的能力，在每个 Checker 内部，我们注入当前 Monorepo 工程的配置和依赖图等上下文信息，Checker 开发者便能通过这些信息去 check 当前的工程是否符合规范。 当我们探测出问题后，其实有些不规范是能通过自动修复解决的，就像 eslint 一样，我们也支持自动修复代码 因为字节的业务线众多，号称 App 工厂，所以各个业务对 Monorepo 有这样或那样的扩展需求。 因此在面对这些不同业务的扩展需求方面，我们通过插件机制支持用户自定义命令，Checker，生成器，并暴露整个生命周期的钩子，以提供更灵活的扩展能力 以上就是自研方案在实践过程中遇到的一些问题，接下来简要说说整体的落地情况 在字节前端领域，我们的自研方案在所有的 Monorepo 工程中 仓库占比达 25%，采用自研方案的仓库绝对数量已经有上千个 月活占比 47%，拍平后的子应用总数接近多仓项目数量 每周的 npm 包下载量达 14万+ 在字节跳动的前端领域，无论从仓库占比，月活占比，以及下载量来看，我们的自研方案都是排名第一的 Monorepo 工具 过去通过自研方案的建设，拥有高效率、可扩展、开箱即用的 Monorepo 解决方案，良好地服务了字节跳动上千个 Monorepo 工程，颇受业务好评。当前的 Monorepo 方案，面临几个核心问题： 第一个，本地任务调度性能瓶颈，巨型 Monorepo 仓库开始出现依赖安装和构建速度问题，需要更极致的性能和体验优化 第二个，缺乏缓存管理能力，如何跟踪和解决缓存命中率偏低问题 第三个，依赖可读性问题，目前文本形式的依赖结构难以理解，这也导致难以对依赖进行有效的管理 我们对上述问题的解决方法如下： 针对性能，我们将会自研依赖安装逻辑，加速安装；任务调度性能加强（比如子项目任务调度能力，远程任务执行能力），加速构建 针对缓存，我们将会开发缓存 sdk，提供缓存的管理和复用能力，跟踪和提高缓存命中率 针对依赖管理，我们将提供开发辅助工具，用可视化的方式管理复杂的依赖关系 PPT 附件","link":"/2022/12/31/%E5%89%8D%E7%AB%AF%20Monorepo%20%E5%9C%A8%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"title":"前端八股知识点-速记脑图","text":"为面试而整理的前端八股文脑图！ RepositoryGithub repository Preview 阅读方式：在线点开每个子目录可以预览脑图下载本仓库，使用软件可打开 mindnode 文件","link":"/2021/04/06/%E5%89%8D%E7%AB%AF%E5%85%AB%E8%82%A1%E7%9F%A5%E8%AF%86%E7%82%B9-%E9%80%9F%E8%AE%B0%E8%84%91%E5%9B%BE/"},{"title":"命令行工具的监控告警建设","text":"本人五年的工作经验，历经三份工作，竟然每份都开发维护过前端命令行工具，大家对前端页面和服务端应用有监控告警这件事习以为常，其实这类工具也需要监控告警，本文将从错误处理到上报排查进行分享。 背景作为前端大家其实都习惯了前端页面有 sentry 类的应用进行错误监控；Node.js 应用打印日志，并在需要的时候使用类 kibana 的应用进行日志查询，且往往配套监控告警。而前端开发几乎每天都会打交道的命令行工具，却在每次报错时，要么联系开发者，要么去用户群咨询。 目的我们先来看看命令行工具的几个特点： 在用户的终端中执行，像一个客户端 运行在 Node.js 环境中，像服务端应用 用户是开发者，往往不能像一个真实产品一样被运营 基于如上几个特点，命令行工具的监控告警需要有上报处理；错误处理需要借鉴 Node.js 应用；开发者对 bug 的容忍度比较高，善于自行排查，命令行工具的维护者需要主动解决某一类问题，以减少开发者在排查中浪费时间；需要一个反馈机制，使得工具能够越来越 bug less。 作为命令行的维护者，借助监控告警的目的主要有： 主动发现异常，提前介入处理，而不是积累到一定程度，用户主动上门时才介入 大部分用户往往不会主动上门，如果有替代品或者不是必须使用的话，用户就流失了 即使用户没有流失，一个经常出错的命令行工具，会使得用户变得不信任 接入监控告警重点是发现『重复类』的错误，解决一批错误而不是偶现错误，从而迅速收敛错误 错误处理在阐述监控告警之前，有必要重点说明下如何科学地处理程序出现的错误。如果程序侧无法很好地处理和上报错误，那么监控告警将起不到有效的作用。 错误分类正确地区分错误分类，有助于我们分别采取不同的方式处理错误，错误主要分为： 操作性的错误(预期内) 程序员的错误(预期外) 这两类错误的区别： 『操作性的错误』是程序正常操作的一部分 『程序员的错误』是 Bug，往往由于程序员没有正确处理导致 举几个例子： 操作性的错误 连接不到服务器 无法解析主机名 无效的用户输入 请求超时 服务器返回500 套接字被挂起 程序员的错误 读取 undefined 的一个属性 调用异步函数没有指定回调 该传对象的时候传了一个字符串 该传IP地址的时候传了一个对象 不同类别的错误如何处理？操作性的错误 直接处理（处理完成后，继续执行） 向上层传错（及时向上层抛错，由上层处理） 重试操作（尝试重试，比如重发请求） 直接崩溃（崩溃退出进程，比如内存不足） 记录错误（仅记录或上报错误） 程序员的错误 无法处理（log &amp; crash） 错误上报我们往往能妥善地处理大部分『操作性的错误』，但是一旦出现无能为力的『操作性的错误』或者『程序员的错误』，此时我们能做的一般只有打印错误告知用户后退出应用，同时将错误上报。接着在服务端根据错误信息区分监控和告警，那么什么样的错误属于告警，什么样的错误属于监控呢？ 告警 当程序出现『程序员的错误』，需要紧急介入时，比如用户初始化模版时，进程异常退出 当程序出现『操作性的错误』，且最终无法被处理时，比如用户创建 gitlab 时，重试次数达到上限，且仍未成功时 监控 当程序出现『操作性的错误』，需要引起足够重视，比如用户输入了不合法的路径、用户创建 gitlab 时经常需要重试 2 次才能成功 当程序进入需要引起足够重视的逻辑（可以不是错误），比如出现了新用户、新部门、新工程 哪些地方需要上报预期外的错误12345678910process.on('unhandledRejection', error =&gt; { reportAlarm({ error }); console.error('Unhandled Rejection Error: ', error); setTimeout(() =&gt; process.exit(1), 1000);});process.on('uncaughtException', error =&gt; { reportAlarm({ error }); console.error('Unhandled Exception Error: ', error); setTimeout(() =&gt; process.exit(1), 1000);}); 预期内的错误1234567try { // 命令行处理逻辑} catch (error) { console.error('Handled Error: ', error); await reportAlarm({ error }); process.exit(1);} 错误排查有效的错误上报，对错误的监控告警处理起到了决定性的作用，我们来看看两则上报消息： Bad case1234567891011121314错误模块：模版初始化错误信息：输入路径非法错误栈：/Users/linleyang/code/temp/case.js:2 throw new Error('输入路径非法'); ^Error: 输入路径非法 at init (/Users/linleyang/code/temp/case.js:2:9) at Object.&lt;anonymous&gt; (/Users/linleyang/code/temp/case.js:5:1) at Module._compile (internal/modules/cjs/loader.js:999:30) at Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10) at Module.load (internal/modules/cjs/loader.js:863:32) at Function.Module._load (internal/modules/cjs/loader.js:708:14) at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:60:12) at internal/main/run_main_module.js:17:47 Good case12345678910111213141516错误模块：模版初始化错误信息：输入路径非法错误栈：/Users/linleyang/code/temp/case.js:2 throw new Error('输入路径非法'); ^Error: 输入路径非法 at init (/Users/linleyang/code/temp/case.js:2:9) at Object.&lt;anonymous&gt; (/Users/linleyang/code/temp/case.js:5:1) at Module._compile (internal/modules/cjs/loader.js:999:30) at Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10) at Module.load (internal/modules/cjs/loader.js:863:32) at Function.Module._load (internal/modules/cjs/loader.js:708:14) at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:60:12) at internal/main/run_main_module.js:17:47输入值：$/home/linleyang触发用户：林宜丙 这两个示例，下面那个除了上报错误消息和错误栈之外，还上报了足够多的错误上下文，我们称之为错误现场，有了错误现场，我们便可能自行复现，或者阅读源码就能解决问题，错误现场既然如此重要，我们来看看一般可以上报哪些现场信息： 错误发生时的入参 错误发生时的状态（关键变量） 触发人 错误信息/错误栈 环境信息（SCM，CI，本地）…… 我们正确处理了错误，正确上报了现场，这样服务端便可根据这些信息将错误分发到各个模块负责人，示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647模版初始化 | 载入模版失败 | 错误报警模块负责人@林宜丙基本信息Version: 1.0.1 Node: v12.22.5 Env: localProject: https://github.com/quanru/bagu用户信息姓名: @林某某首次使用的时间: 2021-08-31 05:00:29最近使用的时间: 2021-11-23 06:00:00历史执行次数: 111部门信息名称: XX部门首次使用的时间: 2021-05-24 04:03:43使用的总人数: 21信息链接: https://quanru.github.io/错误信息级别: 未设置信息:Cannot read property 'replace' of undefined近 24 小时内该用户出现该错误信息的次数: 4近 24 小时内该用户出现所有错误信息的次数: 4近三个月该用户出现该错误信息的次数: 4近三个月所有用户出现该错误信息的次数: 26错误栈:Error: 输入路径非法 at init (/Users/linleyang/code/temp/case.js:2:9) at Object.&lt;anonymous&gt; (/Users/linleyang/code/temp/case.js:5:1) at Module._compile (internal/modules/cjs/loader.js:999:30) at Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10) at Module.load (internal/modules/cjs/loader.js:863:32) at Function.Module._load (internal/modules/cjs/loader.js:708:14) at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:60:12) at internal/main/run_main_module.js:17:47上下文信息输入值：$/home/linleyang附加信息command: cra init hello-world 上报策略为了让命令行维护者更聚焦的处理错误，请切记『监控告警』： 只提供一个通道，用户需要酌情上报 都上报和都不上报区别不大 服务侧需提供默认的上报策略： 这些策略提供参数供上报侧设置 同一个人，半小时内仅上报一次 同一个人，首次无需上报 不存在上下文信息和附加信息的降级为监控 上报侧需要考虑： 调试流量/测试版本不上报 上报侧应主动区分监控和告警（比如：npm start） 用户目录下的代码改动，可以是监控 我们的模块代码出错，应该是告警 尽量主动处理预期内错误，减少抛到兜底处理（比如：某个 npm 包未安装） 按需配置『上报策略』 发生主动异常退出 (process.exit(1)) 时，使用 console.error 而不是 console.log，这样父进程能获取到 stderr 标准错误输出 spawnSync 子进程时，将标准错误 pipe 到父进程进行处理 参考 https://www.joyent.com/node-js/production/design/errors","link":"/2021/12/05/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7%E7%9A%84%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6%E5%BB%BA%E8%AE%BE/"},{"title":"基于 Gitlab Web Hook 的自动 Eslint 语法检查","text":"Eslint, 一个插件化的 Javascript 语法检查工具, 如何将其结合 Gitlab 并应用于开发呢? Gitlab Web HookGitlab Web Hook 提供如下事件的 Hook: Push events Tag push events Comments Issues events Merge Request events… 当对应事件发生时, 将触发预设的 URL (即 Web Hook), 并向其发送一个包含该事件详细信息的 POST 请求, 正是通过该 POST 请求从而对各个事件进行处理的. 例如 Merge Requests events: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879//Request body:{ &quot;object_kind&quot;: &quot;merge_request&quot;, &quot;user&quot;: { &quot;name&quot;: &quot;Administrator&quot;, &quot;username&quot;: &quot;root&quot;, &quot;avatar_url&quot;: &quot;http://www.gravatar.com/avatar/e64c7d89f26bd1972efa854d13d7dd61?s=40\\u0026d=identicon&quot; }, &quot;object_attributes&quot;: { &quot;id&quot;: 99, &quot;target_branch&quot;: &quot;master&quot;, &quot;source_branch&quot;: &quot;ms-viewport&quot;, &quot;source_project_id&quot;: 14, &quot;author_id&quot;: 51, &quot;assignee_id&quot;: 6, &quot;title&quot;: &quot;MS-Viewport&quot;, &quot;created_at&quot;: &quot;2013-12-03T17:23:34Z&quot;, &quot;updated_at&quot;: &quot;2013-12-03T17:23:34Z&quot;, &quot;st_commits&quot;: null, &quot;st_diffs&quot;: null, &quot;milestone_id&quot;: null, &quot;state&quot;: &quot;opened&quot;, &quot;merge_status&quot;: &quot;unchecked&quot;, &quot;target_project_id&quot;: 14, &quot;iid&quot;: 1, &quot;description&quot;: &quot;&quot;, &quot;source&quot;:{ &quot;name&quot;:&quot;Awesome Project&quot;, &quot;description&quot;:&quot;Aut reprehenderit ut est.&quot;, &quot;web_url&quot;:&quot;http://example.com/awesome_space/awesome_project&quot;, &quot;avatar_url&quot;:null, &quot;git_ssh_url&quot;:&quot;git@example.com:awesome_space/awesome_project.git&quot;, &quot;git_http_url&quot;:&quot;http://example.com/awesome_space/awesome_project.git&quot;, &quot;namespace&quot;:&quot;Awesome Space&quot;, &quot;visibility_level&quot;:20, &quot;path_with_namespace&quot;:&quot;awesome_space/awesome_project&quot;, &quot;default_branch&quot;:&quot;master&quot;, &quot;homepage&quot;:&quot;http://example.com/awesome_space/awesome_project&quot;, &quot;url&quot;:&quot;http://example.com/awesome_space/awesome_project.git&quot;, &quot;ssh_url&quot;:&quot;git@example.com:awesome_space/awesome_project.git&quot;, &quot;http_url&quot;:&quot;http://example.com/awesome_space/awesome_project.git&quot; }, &quot;target&quot;: { &quot;name&quot;:&quot;Awesome Project&quot;, &quot;description&quot;:&quot;Aut reprehenderit ut est.&quot;, &quot;web_url&quot;:&quot;http://example.com/awesome_space/awesome_project&quot;, &quot;avatar_url&quot;:null, &quot;git_ssh_url&quot;:&quot;git@example.com:awesome_space/awesome_project.git&quot;, &quot;git_http_url&quot;:&quot;http://example.com/awesome_space/awesome_project.git&quot;, &quot;namespace&quot;:&quot;Awesome Space&quot;, &quot;visibility_level&quot;:20, &quot;path_with_namespace&quot;:&quot;awesome_space/awesome_project&quot;, &quot;default_branch&quot;:&quot;master&quot;, &quot;homepage&quot;:&quot;http://example.com/awesome_space/awesome_project&quot;, &quot;url&quot;:&quot;http://example.com/awesome_space/awesome_project.git&quot;, &quot;ssh_url&quot;:&quot;git@example.com:awesome_space/awesome_project.git&quot;, &quot;http_url&quot;:&quot;http://example.com/awesome_space/awesome_project.git&quot; }, &quot;last_commit&quot;: { &quot;id&quot;: &quot;da1560886d4f094c3e6c9ef40349f7d38b5d27d7&quot;, &quot;message&quot;: &quot;fixed readme&quot;, &quot;timestamp&quot;: &quot;2012-01-03T23:36:29+02:00&quot;, &quot;url&quot;: &quot;http://example.com/awesome_space/awesome_project/commits/da1560886d4f094c3e6c9ef40349f7d38b5d27d7&quot;, &quot;author&quot;: { &quot;name&quot;: &quot;GitLab dev user&quot;, &quot;email&quot;: &quot;gitlabdev@dv6700.(none)&quot; } }, &quot;work_in_progress&quot;: false, &quot;url&quot;: &quot;http://example.com/diaspora/merge_requests/1&quot;, &quot;action&quot;: &quot;open&quot;, &quot;assignee&quot;: { &quot;name&quot;: &quot;User1&quot;, &quot;username&quot;: &quot;user1&quot;, &quot;avatar_url&quot;: &quot;http://www.gravatar.com/avatar/e64c7d89f26bd1972efa854d13d7dd61?s=40\\u0026d=identicon&quot; } }} 从这个请求体中可知该 Merge Request 的创建者, 分支, 源分支信息, 目标分支信息, 最近一次 Commit, 分配给哪个用户, 对应的状态等信息. Eslint作为插件化的 lint 方案, 逐渐打败了原有的 JSLint 和 JSHint, 并’吞并’了 JSCS, 其最大的优势就是标榜的可插件化, 并且有各种插件可以扩展, 使得开发人员能够灵活的配置规则, 如果这还不满足你的需求, 你还能方便的开发针对自己需求的插件, 此外它还支持 Es6, Jsx 语法, 前端程序员真是一群追求’时尚’的猿, 你不能阻止一个前端使用新工具 …… 编写自己的 config/plugin npm package两种方式: 第一种是类似于 eslint-google-config 的方式, 通过编写配置文件 npm package, 具体详见官方教程; 第二种以插件形式, 类似于eslint-plugin-react-native, 通过编写独立的 Eslint 插件; 本文采用第二种方式, 不仅方便配置现有的 rules, 也方便未来添加自己的 rules: 12345678910111213141516171819202122232425262728293031import ReactEslint from 'eslint-plugin-react';import ReactNativeEslint from 'eslint-plugin-react-native';const reactRules = ReactEslint.rules;const reactNativeRules = ReactNativeEslint.rules;const ReactNativeDemo = { rules: { 'split-platform-components': reactNativeRules['split-platform-components'], 'no-inline-styles': reactNativeRules['no-inline-styles'], 'no-did-mount-set-state': reactRules['no-did-mount-set-state'], 'no-did-update-set-state': reactRules['no-did-update-set-state'] }, configs: { recommended: { parserOptions: { ecmaFeatures: { jsx: true } }, rules: { 'react-native-demo/split-platform-components': 2, 'react-native-demo/no-inline-styles': 2, 'react-native-demo/no-did-mount-set-state': 2, 'react-native-demo/no-did-update-set-state': 2 } } }};export default MyEslinPlugin; 假设你的插件叫做: eslint-react-demo, 那么通过安装该插件(前提是你 npm publish了这个插件): 1npm install eslint-react-demo babel-eslint eslint --save-dev 并在项目根目录配置如下 .eslintrc 文件即可: 1234567{ &quot;parser&quot;: &quot;babel-eslint&quot;, &quot;plugins&quot;: [ &quot;react-demo&quot;, ], &quot;extends&quot;: [&quot;plugin:react-demo/recommended&quot;]} 使用 Node 编写 Gitlab Web Hook 接口现在插件有了, 还是聊聊怎么实现自动化 Eslint 检查吧, 由于目前采用 Issue 方式开发, 当猿们最终写完功能, 需要合并到 master 分支时, 必须提一个 Merge Request, 此时通过监听拦截 Merge Request 事件, 对应的 Web Hook 需要完成以下任务: 判断 Merge Request 事件的触发动作, 若为 open 或者 reopen, 则执行 Eslint 检查 通过 POST 请求体获取对应事件的 git 地址, 分支等信息, 将对应仓库拉到本地 执行 Eslint 检查 如果通过 Eslint 检查, 则什么都不做, 否则将 Eslint 检查结果作为评论回复在该 Merge Request 页面, 并关闭该 Merge Request(当然, 你也可以发邮件给对应的开发人员); 关键代码如下: 12345678910111213141516171819check(mr) { return new Promise((resolve, reject) =&gt; { // TODO fs.chmodSync(shellFilePath, 755); return execFile(shellFilePath, [mr.author, mr.project_path, mr.source_branch, mr.repo], { cwd: rootPath }, (err, stdout, stderr) =&gt; { // 此处不处理 err, 因为 eslint 不通过也算一种 err...... resolve(stdout); }); }) .then(async(ret) =&gt; { const projectService = new ProjectService(); if(ret) { await projectService.createMrNotes(mr, `\\`\\`\\`\\n ${ret} \\`\\`\\`\\n`); await projectService.updateMr(mr, 'close'); return Promise.resolve(false); } else { return Promise.resolve(true); } })} 其中用到的 shell 脚本内容如下: 1234567891011121314151617181920212223#!/bin/bashif [ ! -d eslint ]; thenmkdir eslintficd eslintif [ ! -d $1 ]; thenmkdir $1ficd $1if [ ! -d $2 ]; thengit clone -b $3 $4 &gt; /dev/nullcd $2elsecd $2git checkout $3 &gt; /dev/nullgit pull &gt; /dev/nullfi../../.././node_modules/eslint/bin/eslint.js . 剧终","link":"/2016/10/02/%E5%9F%BA%E4%BA%8E%20Gitlab%20Web%20Hook%20%E7%9A%84%E8%87%AA%E5%8A%A8%20Eslint%20%E8%AF%AD%E6%B3%95%E6%A3%80%E6%9F%A5/"},{"title":"如何打造一个满足快速定制能力的私有化部署系统？","text":"打造私有化部署系统过程中的一些和总结 场景我们来看一个场景，一个『规模不大』或者『精力有限』的中小型公司，接入三方公司的效率工具，比如 IM 工具（企业微信）、文档协作系统（语雀）、问卷系统（问卷星）、绩效系统等等。三方公司提供接入的主流方式为 SASS，通过用户分级的方式（普通用户、会员用户、企业用户）售卖自身的产品，这对三方公司来说，易于统一维护。 然而我这边举的几个例子往往关系到公司内部的敏感信息，比如高层员工间的对话、核心战略文档、eNps 调研数据、绩效信息等。这些数据如果保存在三方公司维护的服务器里，多少会有一些安全问题；此外『用户分级』的方式就像套餐，难以满足各类用户的定制需求。因此目前越来越多的三方公司支持私有化部署的方式，将自身的服务部署在客户指定的服务器上，比如企业内部机房、公有云和专有云（阿里云、华为云）。目前，我们对外提供的一款服务也遇到此类问题，当客户不满足于我们提供的『用户分级』套餐，且对『数据存储』十分敏感时，我们如何快速定制并私有化部署服务呢？ 现有问题直接将现有的系统，打包部署到客户指定的服务器上不就完事了？事情没有这么简单，它存在如下3个关键的问题： 一、依赖内部环境在公司内部维护了多年的系统，不少能力都强依赖了内部提供的基础设施，比如 CDN、数据库、缓存服务、动态配置、用户体系等。这些基础设施可能在用户指定的服务器上不一致，甚至不存在。 二、缺乏定制能力当开放用户对系统的定制能力后，现有的系统仅仅支持『用户分级』的方式，是难以支持用户『变幻莫测』的定制需求的，如何快速响应用户的定制需求，是需要全新的一套『模块组织方式』。 三、 并行维护成本如果支持用户私有化部署，那么不得不面临一个这样的问题，当 N 个公司接入时，我们会同时维护 N 套系统，那涉及到一系列在多套系统之间同步 bug 和 feature 的规范、多个版本并行『开发调试构建部署』的研发流程。 解决方法那么，如何打造一个满足快速定制能力的私有化部署系统呢？依旧从上述三点问题出发： 一、摆脱依赖内部环境配置化为了摆脱对内部环境的依赖，我们需要对现有系统进行配置化改造，支持对涉及到的基础设施进行配置，这些配置可以手动写在配置文件里、可构建时生成、运行时动态修改。最终做到根据 N 份配置，即可产出针对 N 个公司的部署包。 二、满足需求定制能力模块化抽离功能模块，通过『开关』、『配置』进行设置。那模块如何拆分和维护呢？拆分可考虑从『垂直』角度，比如『投放功能』、『个人中心』等；或者从『水平』角度，比如『登录』、『权限』等。 三、降低并行维护成本系统融合对现存的系统进行融合，维护一个基线版本的『基础版』，这个所谓的『基础版』有两个目的： 当有新的企业需要接入时，可使用本『基础版』快速定制开发 作为所有并行系统的上游代码分支（master），下游代码分支（各企业分支）定期从该分支 pick bugfix &amp; feature 流程改造我们的服务比较特殊（参考页面搭建类服务），有负责配置的 B 端，以及由 B 端生成的 C 端页面。这样我们就有 4 个工程，每次功能开发都可能涉及到 4 个工程的开发、调试、联调、构建、发布，同时 C 端的静态资源版本和服务地址又是自身决定的。这两点无疑增加了开发人员的心智负担。 首先，将 BC 两端的前后端各放入一个工程中维护，每次发布最大仅需要发布两次。同时 B 端既然作为配置侧，那么不仅 C 端页面本身的功能需要由 B 端决定，我们完全可以将诸如静态资源版本和服务地址这类配置也交由 B 端获取和设置。 接着，对应用初始化、开发、调试、联调、构建、发布进行规范，规范是一种约定，为了更好地将规范落地，有必要实现一套命令行工具或页面流程作为规范的载体。更进一步，可抽象应用概念，这样所有的流程都围绕着这类应用开展； BC 两端可以是这类应用，同时拆分的模块也可以是这类应用，此时我们的所有流程都是围绕着这个应用概念，也就是说，BC 两端以及拆分出的几十甚至上百的模块都共用一套初始化、开发、调试、联调、构建、发布流程。 配置化此外，配置化也有助于降低并行维护成本，我们通过维护配置而不是维护各类代码逻辑。避免诸如『如果是A公司，就执行B逻辑』这类逻辑，配置更关注功能，比如 A 公司的配置开启了『B逻辑』。 除了降低成本之外，配置化还用于应对未来的规划，由于目前所有的企业『定制需求』都是通过人工方式去组装 BC 端应用。在未来当我们接入的企业越来越多，沉淀出大量的『模块』应用之时，便可去探索如何摆脱手工组装，也即通过在线页面通过搭积木的方式组装，借助于配置化，我们的设计可以很好地满足这个需求。 结语此处仅简单讨论了下一个私有化部署系统所需要的改造，步骤三中的流程改造涉及到大量前端工程化相关的能力，还需进一步展开~","link":"/2021/04/05/%E5%A6%82%E4%BD%95%E6%89%93%E9%80%A0%E4%B8%80%E4%B8%AA%E6%BB%A1%E8%B6%B3%E5%BF%AB%E9%80%9F%E5%AE%9A%E5%88%B6%E8%83%BD%E5%8A%9B%E7%9A%84%E7%A7%81%E6%9C%89%E5%8C%96%E9%83%A8%E7%BD%B2%E7%B3%BB%E7%BB%9F/"},{"title":"字节跳动的前端工程化实践","text":"受邀参加 2023 51CTO 举办的『WOT全球技术创新大会』 分享会背景 受邀参加 2023 51CTO 举办的『WOT全球技术创新大会』 字节跳动的前端工程化实践，大幅提高巨型应用构建和维护效益-51CTO.COM 本主题简介 PPT 正文 大家早上好，我叫林宜丙，今天带来的分享主题是『字节跳动的前端工程化实践』。 简单介绍下我自己，我是一名来 Web Infra 部门的前端架构工程师，拥有多年前端工程化的经验，致力于帮助前端工程师更好地管理和治理工程；目前负责工程治理方向的方案设计和落地工作。 今天的分享包括以下四个部分 首先分析『当下前端开发领域的趋势』，并引出字节跳动前端开发所面临的新挑战； 其次分享我们目前通过哪些实践来应对这些新挑战； 接着分享自研的方案在字节跳动的整体落地情况； 最后总结和展望前端工程化的发展规律； OK，在分享之前，我们先来看看『什么是前端工程化？』 所谓前端工程化，就是在前端开发过程中，采用一系列的技术手段和工具，来提高开发效率、保证代码质量、提高代码复用性、实现自动化流程和促进团队协作等方面的目标，是现代前端开发不可或缺的一部分； 在这里呢我要特别说明一下，本次主题所要分享的并不是大而全的前端工程化，而是分享为了应对当前『新趋势』下的『新挑战』，我们所做的『新实践』，接下来一起看看，前端开发出现了哪些新趋势呢？ 首先是前端工种的趋势 第一个趋势是涉及的平台越来越多，Web，Node，客户端和跨平台等 第二个趋势是所能支撑的业务也越来越多，复杂度越来越大，特别是近年来前端侧涌现出不少重前端交互的应用 第三个的话，是随着上述两个趋势而来地、不可避免地使得前端团队的规模增大 上述三个趋势又客观上造成了前端工程的四个趋势，也就是： 第一个是，代码规模增大，内部已经出现代码量超过 10G 的大型工程，同时一个应用的上下游依赖所涉及的项目数量也是非常的多 第二个，维护人数增多，一个工程少则十来人，多则四五十人 第三个，应用体积增加，随着应用功能的迭代，体积越来越庞大 第四个，治理难度增高，复杂的依赖关系难以治理，复杂的构建产物难以诊断 那么，在上述这些趋势下，我们的前端开发面临了哪些新的挑战呢？主要有四个： 第一个是多项目维护成本比较高，项目基建重复、代码复用困难、工作流程割裂等 第二个是多人开发协作成本比较高，相互依赖的流程、级联的依赖升级，都增加了协作成本 第三个是巨型应用构建速度比较慢，构建耗时随着应用增大而变慢 第四个是大型应用劣化速度比较快，我们缺乏有效的防劣化手段 为了解决上述新挑战，我们投资如下工具来应对 第一个是自研 Monorepo 工具，用于降低多项目的维护成本 第二个是对原有的微前端框架进行升级，进一步降低多人开发的协作成本 第三个是开发 Bundler 和 Build System，来加快巨型应用的构建速度 第四个是提供诊断工具，来有效地防止应用劣化 接下来将从这四个主题展开聊聊，为了达到上述目标，我们是如何实践的 什么是 Monorepo？ 它是一种源代码管理的模式，其形式就是将多个项目集中到一个仓库中管理； 与之相对的是 Polyrepo 模式，这种模式下各个项目都有独立的仓库； 简而言之，Monorepo 就是将多个不同的项目以良好地组织关系，放到单个仓库中维护； 那么 Monorepo 是如何降低多项目的维护成本的？他通过 复用基建，让开发人员重新专注于应用本身 代码共享，能够低成本的做到代码复用 原子提交，使用自动化的多项目工作流 首先是通过复用基建，让开发人员重新专注于应用本身： 在传统的 Polyrepo 模式下，每个项目都需要有开发人员创建和维护；而在 Monorepo 中，只需要一两个开发人员负责建立 Monorepo 工程，所有的项目都能够在一个仓库中统一维护，通过复用一套基建（比如 CI 配置、Lint 规则、构建脚本等），从而降低多项目维护成本 此外，复用基建也使得统一改造和升级基建变得方便，比如想在 CI 流程中为所有项目加入类型检查，来提高下项目的质量，在 Polyrepo 下需要修改每一个项目，这样成本其实是很高的。而在 Monorepo 下，基建的调整和维护，能够很容易地应用到多个项目中。 其次是通过代码共享，让开发人员能够低成本地复用代码： Polyrepo 中，维护公用模块的成本比较高，首先是调试很繁琐，公用模块的调试需要手动执行 npm link，与当前调试的项目关联起来，如果公用模块较多的话，npm link 步骤将非常繁琐低效。 其次公用模块的升级很繁琐，需要手动管理这种依赖关系，先升级底层的模块，然后发布，最后再升级顶层模块，如果中途出错了，我还得再来一遍这个过程。 而在 Monorepo 中，可以直接一键创建公用模块，顶层的模块一键引入公用模块进行开发、调试，底层模块的更改能够直接被上层感知，甚至不需要经过 link 和 npm 发布，减少了很多重复的工作，大大降低了抽离新的『复用代码』的成本，这使得大家更愿意做这类抽离工作，这反过来又提高了代码复用率。 再次，是通过原子提交，让开发人员享受自动化的多项目工作流 如果我的业务需求要涉及到多个项目，在 Polyrepo 模式中，例如需要修改图中的三个项目，需要先修改提交底层模块，跑每个模块的 CI 流程，在处理顶层模块时，还得更新底层依赖，接着再跑一次 CI 流程，这一套流程非常繁琐且不连续。 而在 Monorpeo 中，我们可以直接一次性调整并提交多个项目，CI 和发布流程也都是一次搞定的，从而将多项目的工作流自动化。 我们简单的做一个总结： 在 Polyrepo 的模式下，每一个项目都有各自的一套基建，且代码复用困难、工作流程割裂，但是在 Monorepo 里面，我们可以让多个项目复用一套基建；方便地共享代码，使用一致的工作流。 很多时候一个团队的项目往往不是割裂的，而是相互联系的，Monorepo 可以很方便地将这些项目组织到一个大仓库中维护，从而极大地降低了多项目的维护成本。 接下来分享下 bunlder 和 build stystem 的工程化实践，因为无论是单仓项目还是多仓项目，随着代码规模和子应用数量的增加，都会导致构建性能下降，为了应对这种情况，我们分别建设了 Bundler 和 Build System 工具。 其中 Bundler 是为了解决单个巨石应用构建速度慢的问题； 而 Build System 则是为了解决 Monorepo 下构建速度慢的问题。 在前端领域，Bundler 是一种工具，用于将多个前端资源（例如 JS、CSS、图像等文件）打包到一个或多个文件中，从而让浏览器能够直接运行； 常见的 Bundler 工具有 Webpack、Rollup、Vite、Parcel、Esbuild。 接下来介绍一下我们自研的 Rspack Bundler，它是一个基于 Rust 语言的高性能构建引擎， 具备与 Webpack 生态系统的互操作性。 从这句介绍中，我们可以知道，Rspack 有两个特性，一个是高性能，另一个是与 Webpack 生态的兼容性。 首先第一个特性，即采用 Rust 语言实现，由于 JS 是单线程的，虽然在 JS 中也有些方式做到并发，但这些方法都有种带着镣铐跳舞的感觉，而在 Rust 中我们能很好地支持并发特性，所以我们将构建过程中的任务，利用并发的特性去执行，这极大提升了构建性能； 这两张图片是 Webpack 与 RSpack 在构建过程中的线程情况对比，我们可以明显地看出，Webpack 其实只是一个单线程，而 Rspack 则能充分发挥多核 CPU 的优势，极致地压榨出性能。 这是我们官网上的一个对比图，在相同的项目下，RSpack 只需要 4.2秒，而 Webpack 需要 34.8秒 Rspack 第二个特性就是与 webpack 部分兼容，目前的实现可以理解为是一个 webpack 的子集，这套子集里包含了大部分的常用配置，满足了日常的业务开发，那么，为什么要与 webpack 生态兼容？ 首先，webpack 的插件机制满足了项目对定制化的要求 其次，复用 webpack 丰富的生态，相当于用最小的成本优化巨型项目的开发体验 最后，存量的 webpack 项目非常多，兼容 webpack 能大大降低迁移成本 这是两个典型业务上的收益，两个项目原本 dev 启动耗时5分钟左右，使用 Rspack 后只需 20 秒左右，原本 hmr 需要 20 秒左右，使用 Rspack 后只需要 1 秒左右，性能收益基本都在 10 倍左右 简单介绍下 Build System，它通过处理 Monorepo 下的项目依赖关系图，并根据这个关系图调度构建任务； 为什么 Monorepo 下需要一个 Build System 呢？ 因为 Monorepo 不是简单的将多个项目直接维护到单个仓库而已，它还需要借助 Build System 来管理整个代码仓库中的多个项目，并根据项目之间的依赖关系进行构建； 常见的 Build System 工具有 Bazel、NX、Turborepo、Lage 等 接下来介绍下在我们自研的 Monorepo 工具中，如何实践 Build System，我们通过 支持『任务并行能力』，采用最大限度的并行任务加速 支持『多级缓存能力』，对构建任务实现了多级缓存 支持『按需构建能力』，根据代码更改的影响面来构建 如图所示，根据子项目的依赖关系转成一个任务依赖图，它的构建顺序必须符合这样一种要求，即上层的项目构建，需要等待底层项目构建的完成。 OK，我们看方式一，即通过串行排成 DEBCA，这是能够符合构建要求的；但是它的性能是比较低的，比如 D 和 E 是可以并行的，所以第二种方式就是通过把 DE 以及 BC 进行并行处理，这样便能把前面的 5 个步骤加速到 3 个步骤。 此时我们发现任务 C 呢，它不依赖任务 D 的完成，但是方式二下，任务 C 却得等待 D 和 E 都完成才开始执行。 因此便引出第三种方式，在任务 E 完成后，D 和 C 以并行方式执行； 当 monorepo 子项目规模增大后，每一次开发或者上线都会涉及多个子项目，如果每次都需要对这些子项目进行重新构建，这将会极大地拖慢构建和部署速度。 我们提供了对构建产物进行缓存的能力，能够同时将产物缓存到本地和远程，当相关的子项目没有修改过代码时，将会复用之前的构建产物以减少构建时间。 按需构建能力方面的实践，我们支持按影响面执行 CI 流程，通过 git diff 当前有改动的代码进行依赖分析，从而只构建受影响的项目，否则每次 CI 都会完整的构建所有的子项目。 此处以一个简单的 Monorepo 为例，依赖关系如图所示 如果一次性全量构建所有项目，耗时约为 17.72秒 如果仅仅改动了 component 模块，那么按需构建的话，我只需要构建 component, app1, app2，此时耗时 8.94秒，节约 50% 的时间 再来看看无缓存的情况下，我仅仅构建 App1, App2, App3 它们的耗时在 10.77秒到16.94秒之间 如果有缓存的话，比如 component, sdk, util 已经构建过，那么再单独构建 App1, App2, App3 时，构建耗时在 7.55秒到9.74秒之间，大约节约 45% 的时间 通过 Builder 和 Build System 的建设，我们分别借助 Rust 语言的高性能和远程构建缓存能力，极大地加速了巨型应用的构建速度，但这又不仅仅是开发速度上的提升，它同时给我们的业务带来两个巨大的收益 拉高了一个巨石应用上限：使得我们能够开发一个更巨型更强大的应用 加快迭代速度：使得我们可以更快更多地做 AB 测试和功能发布 所谓微前端，就是前端领域的应用分治解决方案，字节跳动的微前端实践也经过 iframe, spa, 框架阶段；在实践中遇到不少问题，为了进一步降低多人协作的成本，目前进入一个新方案的探索阶段。 接下来看下微前端的工程化实践，新方案是如何降低多人开发的协作成本的 首先，通过减轻基座负担，将基座应用与业务逻辑解耦 其次，采用细粒度的组合，在更细粒度的模块上独立开发、部署 最后，通过约定模块协议标准，我们搭建了模块中心，甚至可以结合低代码平台，从而带来更高的模块复用率；并支持模块级别的灰度和AB能力； 它是如何减轻基座负担的呢？传统的微前端，通过建立一个公共的基座来承载公共逻辑，这种复用方式，除了工具库以外，往往导致不少业务逻辑耦合到基座中，这导致基座更频繁的改动和发布，更容易出现更大的故障影响面和更容易失效的缓存； 这又让子应用从独立开发、部署重新回到某种程度上地相互依赖，因此，新方案通过两种方式消除了这类基座的存在，一个是消费机制，另一个是共享机制，前者一般用于复用业务逻辑，后者用于复用业务无关的工具库 那么，新方案的机制是如何起作用的？ 传统的微前端架构中，多个子应用之间是相对隔离的，往往都会通过一套沙箱机制来保证这种隔离性； 但随着前端子应用的规模增大，人员增多，如此粗粒度的隔离又会反过来制约每个子应用内外的人员协作； 因此，我们新的应对方式就是通过提供更细粒度的模块消费和共享方案，从而让开发人员能够以更小的单位进行独立开发、测试、部署 模块协议标准约定了模块的元信息，通过该协议在各个平台之间流转，来达到特定的功能 比如构建平台会根据配置文件构建出这份协议文件 比如部署平台会将元信息转成包含具体 cdn 地址的数据，直接回填到 html 里下发，这步操作也是能做到模块灰度和AB的前提 再比如应用运行时也会根据这份协议动态加载模块 有了细粒度组合和模块协议标准的特性，我们又能够十分方便地建立一个在线模块中心，无论是业务相关的组件，还是业务无关的工具库，都能以极低的成本在团队内部，甚至跨团队复用； 此外，基于这套机制，我们还尝试了与低代码平台的配合，通过低代码平台搭建出符合模块协议标准的组件，并注册为在线模块后，能够极大的提高业务开发的效率。 这是一个典型的接入业务，接入之后我们发现从构建耗时、部署耗时，甚至迭代速度和需求吞吐量都有了显著的提升效果。 接下来分享下诊断工具，市面上的工具大多面向构建产物进行诊断和分析，无法对构建过程进行更深入诊断和分析，作用十分有限 那么我们自研的诊断工具，是如何有效地防止应用劣化的呢？ 首先，提供面向构建过程的分析能力，由于记录了构建过程的数据，我们会有更细粒度和更丰富的分析； 其次，提供一套可扩展的规则机制，让不同的垂直场景和业务场景能够扩展自身的规则； 最后，通过与核心研发流程结合，让规则真正发生作用； 这是一份 statoscope 的分析结果页面，它是典型面向构建产物分析的，即通过消费 stats.json 来分析产物； 它没有 loader、resolver、plugin 等和构建过程相关的分析和诊断； 而我们的工具能做到如下一些能力： webpack loader 时序分析 webpack loader 分析 webpack resolver 分析 webpack plugin 分析 bundle 深度分析 那我们是如何做到的呢？我们通过监听插件钩子和劫持 Loader 等方式介入构建过程，并收集和生成专门用于诊断和分析场景的数据结构，比如依赖图、模块图、三方包图、源码、loader、plugin、resolver 等数据，从而获得更完善的构建上下文信息，以便我们后续进行深度的诊断和分析 我们提供了一些默认的诊断规则，比如 重复包检查 - Duplicate Packages 包规范语句匹配 - Default Import Check Loader 性能优化 - Loader Performance Optimization 此外，我们还提供一套可扩展的诊断规则机制，我们将重新生成的数据结构作为上下文传递给自定义的规则，便能轻松的做到如下能力： 依赖的引用方式检查 特定依赖的版本检查 禁止使用特定的语句 光有上述两个能力还不够，我们还谋求与核心研发流程结合，在 CI 中支持基于分支的 diff 拦截，从而让规则真正发生作用 总结一下，这是一些典型的业务收益情况 这是目前上述几个工具的整体落地情况，其中 Monorepo 工具接入工程x个，微前端活跃用户y个，Bundler 开源工具 Rspack z stars，诊断工具的周下载量达到n次 接下来做一个简单的总结和展望： 当上述一系列工具链支撑了更大的工程规模，更多的人员规模，更加快速的构建速度，更可维护的前端工程之后； 我相信未来一定会催生出更『强大』的前端应用，当这个更强大的前端应用继续增大工程规模，增加团队人员，渐渐地拉低了构建速度和可维护性 那未来必将会对这些工具提出更高的要求，从而带动整个前端工具链再一次革新 谢谢，这就是我今天带来的分享 PPT 附件","link":"/2023/06/17/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E7%9A%84%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%8C%96%E5%AE%9E%E8%B7%B5/"},{"title":"实习总结","text":"上接《不要叫我程序猿,我是前端开发工程狮》，话说我去了挖财实习，此处省略一万字，然后我就顺利转正，接着回学校撰写毕业论文了。 上接《不要叫我程序猿,我是前端开发工程狮》，话说我去了挖财实习，此处省略一万字，然后我就顺利转正，接着回学校撰写毕业论文了。 省略一万字（部分内容） 找呀找呀找工作当初做的选择是多么正确，一路顺风顺水就拿到offer，基友们都还没着落的时候，我已经决定留在挖财了（挖财决定留我了）。待遇挺满意，团队超nice，我就没有再出去找其他公司了。 当然，转正之前，我还是试了几家的；做过那么多家笔试，唯一进入面试的就是阿里了，然后你懂的，阿里实习宝宝们都拥抱“变化”了，接着宝宝们就出来拥抱我们了，并成功让我司一个前端实习生拥抱了“变化”。阿里的面试，尽问些前端工程化的问题，事实上，那时我还没接触到，一问三不知哈，反正就是去打酱油的。 内推进入面试的有两家，一家是蘑菇街，一家是51信用卡管家。蘑菇街顺利进入了二面，然后也没怎么问，期间问我职业规划什么的，我就blabla说了一大堆我想学的，然后我又没时间什么的，所以面试官问我你加班吗？我说不怎么加班，“那你回到家，为什么不自学”，就没然后了，其实我回家是有学习的，我当时想表达的是我想学那些，可是我在忙着学其他的，怪我咯。下楼问我何时能来实习，二逼如我：“明年六月份吧”，嗯，这下真的没有然后了。 51信用卡真是掉渣天，10点通知我到他那面试，然后10.30才匆匆忙忙跟我碰上头；跟我聊感想….然后我就发现这姐们是hr，所以没有技术面？然后也问了我什么时候能来实习，再续二逼：“明年六月份吧”，其实我主要在等挖财，可惜挖财转正面试太晚啦！ 挖财的转正面试，应该是走个过场，CTO面的，看我简历上写的关键字中有“Linux”，然后就去拿了台mac，问了一些常用命令，比如ps，比如vi，再比如grep；他问我平时是用Linux吗？我说是的，“前端屌丝不都用mac吗？”，内心os：人家才不是屌丝呢（我只是没钱），对了，我现在是我司唯一一个没有mac的前端工程师，随意感受下。 实习经历刚来挖财的时候，一切都是那么新鲜，我还是很屌的，什么git啊，svn啊，idea啊，vm啊，avalon啊，mvvm啊，gulp啊，webpack啊我都没听过，哈哈~还有一个有求必应的资深前端小哥在我旁边，随时都可以请教，进步是非常明显的。 期间做了很多有趣的项目，都不大，但是挺有成就感的，有个大咖秀项目的活动页面生成管理系统，现在又增加了有些小需求，打算做成发布各种类似活动的管理系统。还有一个让我累死累活的双十一活动主会场，倒不是有多难，主要是开发人员都在上海那边，而负责前端的我一个人在杭州，沟通极为不便，很多东西都是我第一次接触，又要兼容我司各个app，但是做成了，还是蛮好玩的。 第一笔实习工资入账的时候还是很激动的，心想：我也是能够养活自己了，还跟我妈说了，以后不用给我寄生活费啦。还有以后我就要扎根杭州啦~大爱杭城。 回校瞒着导师出来实习，已有整整四个月半了，四个月的时候，导师估计发现什么了，在电话里第一句就问我：“xx，实习得怎么样？”，看来到了不得不回校的时候了，于是请了一个长长长的假就回到我校，着手准备撰写“优秀毕业论文”哈，开玩笑~","link":"/2015/11/18/%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/"},{"title":"我给我的 Obsidian 实践写了一个插件","text":"为 一种实用新型 Obsidian 实践之构建我的第二大脑 🧠 实现 Obsidian 插件！ 本文内容已经过时，更多内容请参考官网 LifeOS！ 背景最近将这个我的 Obsidian 实践（一种实用新型 Obsidian 实践之构建我的第二大脑 🧠）分享后，帮助到了几个人 一个不认识的同事用上后，表示『像开启了新大陆』，并给我发了个红包，说请我喝咖啡 一个不认识的网友用上后，表示『满足了她对个人知识管理的全部需求』，甚至还为此写了一篇上手文档 当然，除了陌生人以外，我也推荐给不少熟悉的同事和朋友，收到了不少反馈，比如上手成本高、更新难以跟进、关系图谱中无法区分项目（所有都是 README.md）等。 目标因此考虑到上述问题，我决定实现一个插件，主要有如下几个目标 降低上手成本，特别针对非程序员群体 更新及时触达，无论是新功能还是问题修复 增加受众，以获得反馈，共建并完善流程 文末我还介绍了一种渐进的使用这个插件的方式！ 旧的方式在未使用插件的示例模版里，存在如下几类的『程序代码』 脚本类 日期解析 date 根据周期笔记的文件名，解析出日期 根据解析出的日期，获取日期范围 根据解析出的日期，获取文件列表 任务查询 task 根据日期范围，获取任务列表 项目查询 project 获取当前项目列表的快照 根据日期范围内，获取项目列表 计算日期范围内，项目的耗时及其占比 领域查询 area 获取当前领域列表的快照 根据日期范围内，获取领域列表 视图类 任务完成视图 taskDoneList 放到周期笔记中，可获取当前日期范围内完成的任务列表 任务记录视图 taskRecordList 放到周期笔记中，可获取当前日期范围内收集的任务列表 项目列表视图 projectList 放到周期笔记中，可获取当前日期范围内项目耗时的占比 领域列表视图 areaList 放到周期笔记中，可获取当前日期范围内领域列表 直接将代码跟随示例模版输出给用户，这大大增加了用户的上手成本，且用户无法更新这些脚本。 新的方式因此我决定实现一个插件封装上述两类『程序代码』，通过提供『markdown 代码块』的方式提供『视图』，这样用户只要会 Markdown，就能读懂并使用，那么我的插件实现了哪些视图呢？ 在上一篇文章中，提到我的实践有两个上下文，让我保持聚焦 一个是基于时间的（周期笔记），即我到达某个时间节点，我就基于对应周期笔记作业，且笔记中有足够的上下文； 另一个是基于主题的（PARA），即我想对某个主题进行调查研究的时候，我就基于对应主题的索引（README.md）作业，且笔记中已经收集了不少上下文； 因此，插件需要实现的视图也是基于这两个上下文，举个例子，比如一篇月记（2023-07），它所拥有的上下文应包含7月份所有的日记和周记内的任务（目前只有任务，但是我觉得可以有其它的）；再举个例子，比如一个 PARA 的项目（分享-2023-WOT 分享会），它所拥有的上下文应包含所有被打上 #WOT 标签的任务和记录； 目前插件提供的视图有如下三大类： 根据时间上下文查询即下面同一个代码块，放在不同的周期笔记中（月记、周记、日记），均会解析并获得对于时间范围内的『完成任务列表（TaskDoneListByTime）』、『记录任务列表（TaskRecordListByTime）』、『涉及的项目列表（ProjectListByTime）』、『涉及的领域列表（AreaListByTime）』。 123```PeriodicPARATaskDoneListByTime``` 123```PeriodicPARATaskRecordListByTime``` 123```PeriodicPARAProjectListByTime``` 123```PeriodicPARAAreaListByTime``` 根据 PARA 上下文查询即下面同一个代码块，放在不同的 PARA 目录的 README.md 中（比如 1. Projects/分享-2023-WOT 分享会/README.md），均会查询并获取 README.md 声明的 Metadata tags 字段，并根据这些 tag 查询出『打上该 tag 的任务列表（TaskListByTag）』、『打上该 tag 的记录列表（BulletListByTag）』 123```PeriodicPARATaskListByTag``` 123```PeriodicPARABulletListByTag``` 根据目录查询为了总览当前 PARA，还有一类基于目录的视图，比如查询『当前项目目录下的所有项目（ProjectListByFolder）』、『当前领域目录下的所有有领域（AreaListByFolder）』、『当前资源目录下的所有有资源（ResourceListByFolder）』、『当前归档目录下的所有有归档（ArchiveListByFolder）』 123```PeriodicPARAProjectListByFolder``` 123```PeriodicPARAAreaListByFolder``` 123```PeriodicPARAResourceListByFolder``` 123```PeriodicPARAArchiveListByFolder``` 除了上述视图，插件还提供了在周期笔记模版中使用的辅助函数，目前只有一个，即生成指定目录下的 README.md 文件列表，比如 模版中的如下语句 1&lt;% PeriodicPARA.Project.snapshot() %&gt; 将会替换为 121. [[1. Projects/x-project/README|x-project]]2. [[1. Projects/y-project/README|y-project]] 这样项目列表将在日记笔记创建时，被作为快照固化下来，有几个作用 保留每个项目都经历了哪些日子，而且能手动统计每日项目耗时 未来基于日记能统计周记、月记等时间范围内，都有哪些项目，也能统计每周、每月的项目耗时 如何渐进使用这套系统？在上一篇文章中，提到我的实践是有两套系统的，一个是周期笔记，另一个是 PARA；后来我发现，一上来就两个一起实践，其实上手成本挺高，特别是从未使用过 Obsidian，且不了解 PARA 的用户，我对这类用户的建议是，你大可先只使用周期笔记，把模版中关于 PARA 的一切都剔除，这样你仍能享受到『DailyLog』以及『基于时间的上下文』，即你只管每天记录日记，剩余的汇总和回顾都交给『周记』、『月记』、『季记』、『年记』，等你实践下来有自己的看法和感觉的时候，你可以考虑自顶向下在『周记』和『月记』中安排任务，在『季记』、『年记』设定目标！ 下一步？我本人在使用了这套流程后，真的收获很大，这也是我能日复一日地照着这个实践的原因！因此，我希望有更多的用户能尝试这套系统，一起共创，让这套系统更快地迭代，不仅能让我受益，也能让大家受益！ 下面是我发布 上一篇文章 文章后，收到的反馈所做的微小工作 开发插件，降低使用门槛，为周期笔记和 PARA 的提供查询视图，将所有复杂的查询语句屏蔽 关系图谱优化，即在 PARA 目录下支持 XXX.README.md 作为索引，否则所有的节点都将是 README 单一事实来源，用户只需要设置元信息，插件负责读取","link":"/2023/07/16/%E6%88%91%E7%BB%99%E6%88%91%E7%9A%84%20Obsidian%20%E5%AE%9E%E8%B7%B5%E5%86%99%E4%BA%86%E4%B8%80%E4%B8%AA%E6%8F%92%E4%BB%B6/"},{"title":"排查 Node.js 服务内存泄漏，没想到竟是它？","text":"团队最近将两个项目迁移至 degg 2.0 中，两个项目均出现比较严重的内存泄漏问题，此处以本人维护的埋点服务为例进行排查。服务上线后内存增长如下图，其中红框为 degg 2.0 线上运行的时间窗口，在短短 36 小时内，内存已经增长到 50%，而平时内存稳定在 20%-30%，可知十之八九出现了内存泄漏。 背景团队最近将两个项目迁移至 degg 2.0 中，两个项目均出现比较严重的内存泄漏问题，此处以本人维护的埋点服务为例进行排查。服务上线后内存增长如下图，其中红框为 degg 2.0 线上运行的时间窗口，在短短 36 小时内，内存已经增长到 50%，而平时内存稳定在 20%-30%，可知十之八九出现了内存泄漏。 排查思路由于两个接入 degg 2.0 的服务均出现内存泄漏问题，因此初步将排查范围锁定在 degg 2.0 引入或重写的基础组件上，重点怀疑对象为 nodex-logger 组件；同时为了排查内存泄漏，我们需要获取服务运行进程的堆快照（heapsnapshot），获取方式可参看文章：Node 案发现场揭秘 —— 快速定位线上内存泄漏。 排查过程一、获取堆快照使用 alinode 获取堆快照，服务启动后，使用小流量预热一两分钟便记录第1份堆快照（2020-4-16-16:52），接着设置 qps 为 125 对服务进行施压，经过大约一个小时（2020-4-16-15:46）获取第2份堆快照。使用 Chrome dev 工具载入两份堆快照，如下图所示，发现服务仅短短运行一小时，其堆快照文件就增大了 45MB，而初始大小也不过 39.7MB；我们按 Retained Size 列进行排序，很快就发现了一个『嫌疑犯』，即 generator；该项占用了 55% 的大小，同时 Shallow Size 却为 0%，一项一项地展开，锁定到了图中高亮的这行，但是继续展开却提示 0%，线索突然断了。 盯着 generator 进入思考，我的服务代码并没有 generator 语法，为什么会出现 generator 对象的内存泄漏呢？此时我把注意力转到 node_modules 目录中，由于最近一直在优化 nodex-kafka 组件，有时直接在 node_modules 目录中修改该组件的代码进行调试，因此几乎每个文件头部都有的一段代码引起了我的注意： 12345678910&quot;use strict&quot;;var __awaiter = (this &amp;&amp; this.__awaiter) || function (thisArg, _arguments, P, generator) { function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); } return new (P || (P = Promise))(function (resolve, reject) { function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } } function rejected(value) { try { step(generator[&quot;throw&quot;](value)); } catch (e) { reject(e); } } function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); } step((generator = generator.apply(thisArg, _arguments || [])).next()); });}; 这个代码是 typescript 源码编译后的产出，由于代码使用了 async/await 语法，因此都编译成 __awaiter 的形式，在源码中使用 async 函数的地方，在编译后都使用 __awaiter 进行包裹： 12345678910111213// 编译前(async function() { await Promise.resolve(1); await Promise.resolve(2);})()// 编译后(function () { return __awaiter(this, void 0, void 0, function* () { yield Promise.resolve(1); yield Promise.resolve(2); });})(); 同时一个关于 generator 内存泄漏的 #30753 generator functions - memory leak 也引起了我的注意，该 issue 遇到的问题无论从 Node.js 的版本和内存泄漏的表现都和我遇到的问题十分相似。所以我在工程的 node_modules 中搜索所有 __awaiter 字符串，发现了 3 个模块编译出了上述代码，分别是： nodex-logger nodex-kafka nodex-apollo 由于模块的 tsconfig.json 的 target 字段将目标产出为 es6，因此才会使用 generator 去模拟 async/await 语法，但是从 Node.js v8.10.0 开始已经 100% 支持了 ES2017 的所有特性，所以本不该编译 async/await 语法，此处遂将这 3 个模块的目标产出配置改为 es2017，这样 tsc 就不会编译 async/await 语法。 二、验证重复之前获取堆快照的步骤，惊奇地发现即使过了一天，内存也没有增长，而且 generator 也没有持有未释放的内存： 至此，内存泄漏问题已经解决！那么如何避免遇到这个问题呢？ 如何避免一、解决步骤步骤一该问题仅在特定的 Node.js 版本中存在，请使用版本区间 [v11.0.0 - v12.16.0) 之外的 Node.js，从而防止二方 npm 组件、三方 npm 组件的 generator 语法使你的服务出问题 步骤二将自己的 typescript 的目标环境（target）编译为 es2017 及以上，同时应尽量使用 async/await 语法而不是 generator 语法，从而防止别人使用 [v11.0.0 - v12.16.0) 版本时，引入你的 npm 组件而导致内存泄漏 二、详细说明前文说了从 Node.js v8.10.0 开始就已经支持了 async/await 语法，经查该版本于 2018-03-06 发布，由于所有服务也不可能一下全切换到新版本，因此为了兼容 Node.js v6 版本的环境，需要将代码编译到 es6。但是站在现在这个 LTS 版本已经是 v12 的时间节点，完全可以排查现有使用 typescript 的 npm 组件是否都编译到 es2017，甚至探讨编译到 es2019 的可能。 此外这个内存泄漏问题是从哪个版本开始有的，现在是否解决了呢？编写可验证的内存泄漏的代码如下： 1234567891011121314151617181920// node-leak.jsconst heapdump = require('heapdump')class Async { async run() { return null; }}const run = async () =&gt; { for (let index = 0; index &lt; 10000000; index++) { if (index % 1000000 === 0) console.log(Math.floor(process.memoryUsage().heapUsed / 10000), index); const doer = new Async(); await doer.run(); } heapdump.writeSnapshot((err, filename) =&gt; { console.log(&quot;Heap dump written to&quot;, filename); });};run(); 123456789101112131415161718192021222324252627282930// leak.js 由 node-leak.js 编译得来var __awaiter = (this &amp;&amp; this.__awaiter) || function (thisArg, _arguments, P, generator) { function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); } return new (P || (P = Promise))(function (resolve, reject) { function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } } function rejected(value) { try { step(generator[&quot;throw&quot;](value)); } catch (e) { reject(e); } } function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); } step((generator = generator.apply(thisArg, _arguments || [])).next()); });};class Async { run() { return __awaiter(this, void 0, void 0, function* () { return null; }); }}const run = () =&gt; __awaiter(this, void 0, void 0, function* () { const now = Date.now(); console.log('循环总次数: ', 10000000); for (let index = 0; index &lt; 10000000; index++) { if (index % 1000000 === 0) { console.log('第 %d 次循环，此时内存为 %d', index, Math.floor(process.memoryUsage().heapUsed / 1000000)); } const instance = new Async(); yield instance.run(); } console.log('总耗时: %d 秒', (Date.now() - now) / 1000);});run(); 经过二分排查，发现该泄漏问题从 v11.0.0 引入，在 v12.16.0 解决；内存泄漏版本执行脚本时，内存占用逐步递增直到 crash，而未泄漏版本则会及时回收内存。 根本原因根本原因是 v8 的一个 bug，相关链接： v8 issue: https://bugs.chromium.org/p/v8/issues/detail?id=10031 v8 commit: https://chromium.googlesource.com/v8/v8.git/+/d3a1a5b6c4916f22e076e3349ed3619bfb014f29 node issue: https://github.com/nodejs/node/issues/30753 node commit: https://github.com/nodejs/node/pull/31005/files 改进后的代码，在分配新增 WeakArrayList 数组时，即使返回没有空闲数组的标记（ kNoEmptySlotsMarker ），仍需要调用 ScanForEmptySlots 方法重新扫描一次数组，因为该数组元素有可能有被 GC 回收，这些被回收的元素是可以重复使用的；仅当返回 kNoEmptySlotsMarker 且数组中没有被 GC 回收的元素，才真正执行新增逻辑： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// https://github.com/targos/node/blob/cceb2a87295724b7aa843363460ffcd10cda05b5/deps/v8/src/objects/objects.cc#L4042// staticHandle&lt;WeakArrayList&gt; PrototypeUsers::Add(Isolate* isolate, Handle&lt;WeakArrayList&gt; array, Handle&lt;Map&gt; value, int* assigned_index) { int length = array-&gt;length(); if (length == 0) { // Uninitialized WeakArrayList; need to initialize empty_slot_index. array = WeakArrayList::EnsureSpace(isolate, array, kFirstIndex + 1); set_empty_slot_index(*array, kNoEmptySlotsMarker); array-&gt;Set(kFirstIndex, HeapObjectReference::Weak(*value)); array-&gt;set_length(kFirstIndex + 1); if (assigned_index != nullptr) *assigned_index = kFirstIndex; return array; } // If the array has unfilled space at the end, use it. if (!array-&gt;IsFull()) { array-&gt;Set(length, HeapObjectReference::Weak(*value)); array-&gt;set_length(length + 1); if (assigned_index != nullptr) *assigned_index = length; return array; } // If there are empty slots, use one of them. int empty_slot = Smi::ToInt(empty_slot_index(*array)); if (empty_slot == kNoEmptySlotsMarker) { // GCs might have cleared some references, rescan the array for empty slots. PrototypeUsers::ScanForEmptySlots(*array); empty_slot = Smi::ToInt(empty_slot_index(*array)); } if (empty_slot != kNoEmptySlotsMarker) { DCHECK_GE(empty_slot, kFirstIndex); CHECK_LT(empty_slot, array-&gt;length()); int next_empty_slot = array-&gt;Get(empty_slot).ToSmi().value(); array-&gt;Set(empty_slot, HeapObjectReference::Weak(*value)); if (assigned_index != nullptr) *assigned_index = empty_slot; set_empty_slot_index(*array, next_empty_slot); return array; } else { DCHECK_EQ(empty_slot, kNoEmptySlotsMarker); } // Array full and no empty slots. Grow the array. array = WeakArrayList::EnsureSpace(isolate, array, length + 1); array-&gt;Set(length, HeapObjectReference::Weak(*value)); array-&gt;set_length(length + 1); if (assigned_index != nullptr) *assigned_index = length; return array;}// staticvoid PrototypeUsers::ScanForEmptySlots(WeakArrayList array) { for (int i = kFirstIndex; i &lt; array.length(); i++) { if (array.Get(i)-&gt;IsCleared()) { PrototypeUsers::MarkSlotEmpty(array, i); } }} 不止内存泄漏在我测试内存泄漏时，有一个发现，执行发生内存泄漏时的代码（前文的 leak.js）和未发生内存泄漏时的代码（前文的 node-leak.js）时，即使在已经修复该问题的 Node.js v12.16.2 版本下，generator 语法仍然有两个问题： 内存回收效率低，导致执行完后，仍有相当大的内存占用； 执行效率非常慢，async/await 版本仅需要 0.953 秒，而 generator 却需要 17.754 秒； 这说明，相比 generator 语法，async/await 语法无论从执行效率还是内存占用方面都有压倒性优势。那么执行效率对比如何呢？上 benchmark 工具比划比划： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// benchmark.jsconst __awaiter = (this &amp;&amp; this.__awaiter) || function (thisArg, _arguments, P, generator) { function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); } return new (P || (P = Promise))(function (resolve, reject) { function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } } function rejected(value) { try { step(generator[&quot;throw&quot;](value)); } catch (e) { reject(e); } } function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); } step((generator = generator.apply(thisArg, _arguments || [])).next()); });};const Benchmark = require('benchmark');const suite = new Benchmark.Suite;suite .add('generator', { defer: true, fn: function (deferred) { (function () { return __awaiter(this, void 0, void 0, function* () { yield Promise.resolve(1); yield Promise.resolve(2); // 测试完成 deferred.resolve(); }); })(); } }) .add('async/await', { defer: true, fn: function(deferred) { (async function() { await Promise.resolve(1); await Promise.resolve(2); // 测试完成 deferred.resolve(); })() } }) .on('cycle', function(event) { console.log(String(event.target)); }) .run({ 'async': false }); Node.js v12.16.2 的结果： 12generator x 443,891 ops/sec ±4.12% (75 runs sampled)async/await x 4,567,163 ops/sec ±1.96% (79 runs sampled) generator 每秒执行了 516,178 次操作，而 async/await 每秒执行了 4,531,357 次操作，后者是前者的 10 倍多！我们看看其它 Node.js 版本表现如何： 电脑配置：MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports) Node.js 版本 generator async/await 倍数 12.16.2 443,891 4,567,163 10.29 11.15.0 424,073 680,795 1.60 10.20.1 427,799 669,910 1.57 9.11.2 275,526 500,487 1.82 8.17.0 281,571 535,317 1.90 二者执行效率和 Node.js 版本成正比，而 Node.js v12 来了一次大跃进，直接高了一个数量级，这个得益于 v8 7.2 的一个新特性，官网用了整整一篇文章说明，有兴趣的可以看看。 Chrome 也中招了吗？ 目前最新版：版本 81.0.4044.113（正式版本） （64 位） 已经修复这个问题 既然是 v8 的问题，那么 chrome 浏览器也是有这个问题的，打开空白标签页，执行前文给出的 leak.js 代码： 可发现，chrome 下也会有内存泄漏问题，只不过 chrome 页面上的代码运行一般不会有密集地、重复地执行某段『导致内存泄漏』的代码，因此该问题在 chrome 端不容易被察觉。 总结没想到一个小小的语法转译也会造成如此严重的内存泄漏问题，且一个 V8 底层小小的 bug 在上层使用时会被放大得如此严重。但是只要我们不放过每一个可疑点，深入排查总会有意想不到的收获！","link":"/2020/04/20/%E6%8E%92%E6%9F%A5%20Node.js%20%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%EF%BC%8C%E6%B2%A1%E6%83%B3%E5%88%B0%E7%AB%9F%E6%98%AF%E5%AE%83%EF%BC%9F/"},{"title":"排查守候在零点两分的 bug","text":"最近接手泛前端团队的服务稳定性治理，遇到一些很有特点的线上问题，这边记录一次『有趣的 bug』排查 背景故事的开始是由老板的一个艾特开始的： 当时排查了下没啥思路，就放弃了（以为偶现，过几天它能自己好起来！）。直到某一天我又收到了同样的告警，我回想了下最近好几天都有这个告警。 抬头一看： 心里一惊：别搞出事故啊！就开始了我的排查之路。 排查思路根据告警错误栈显示，这是一个 “unhandledRejection”： 1234567891011[ERROR][2020-09-16T23:59:59.582+0800][default:process.&lt;anonymous&gt; at /home/xxx/xxx/xxx/lib/app.js:49:10] _undef||traceid=64594b155f6231298ae0e2b114a1a02||spanid=38197e8a96a6d96a||pid=1431||msg=on unhandledRejection, error: { Error: ERR invalid expire time in set at JavascriptReplyParser.parseResult (/home/xxx/xxx/xxx/node_modules/redis-parser/lib/javascript.js:90:16) at JavascriptReplyParser.tryParsing (/home/xxx/xxx/xxx/node_modules/redis-parser/lib/javascript.js:117:21) at JavascriptReplyParser.run (/home/xxx/xxx/xxx/node_modules/redis-parser/lib/javascript.js:131:22) at JavascriptReplyParser.execute (/home/xxx/xxx/xxx/node_modules/redis-parser/lib/javascript.js:112:10) at Socket.&lt;anonymous&gt; (/home/xxx/xxx/xxx/node_modules/redis/index.js:223:27) at emitOne (events.js:116:13) at Socket.emit (events.js:211:7) at addChunk (_stream_readable.js:263:12) at readableAddChunk (_stream_readable.js:250:11) at Socket.Readable.push (_stream_readable.js:208:10) command: 'SET', code: 'ERR' } 所以一层一层往上找，是找不到抛错的源头的！猜测了一通无果，于是我去翻告警群的记录，我发现了一个惊人的规律，该告警只要出现，必定是每天凌晨 00:02： 因此排查思路锁定在以下几个： 存在每天定时任务设置某个 redis 值的超时时间？ 服务器时间存在误差？ 排查过程剧透下，并不是上述两个原因。 经过多番搜索代码，发现了几处设置 redis 值的代码，同时结合 Google，有人指出 Redis 设置的超时时间不能为小于 0。经过本地验证，的确发现超时时间不能为 0： 于是排查方向转为代码中哪里出现了设置超时时间小于 0 的逻辑。可疑代码如下： 12345setRedisKey( redisTeamKey, data, 24 * 3600 - getPastTimeOfToday() / 1000,) 那么这个值 24*3600 - getPastTimeOfToday() / 1000 可能为 0 或者负数吗？我们来看看完整逻辑： 12345678const getPastTimeOfToday = () =&gt; { const date = new Date() const year = date.getFullYear() const month = strPadding(date.getMonth() + 1, '0', 2) const day = strPadding(date.getDate(), '0', 2) const todayStartTs = +new Date(`${year}/${month}/${day} 00:00:00`) return +new Date() - +new Date(todayStartTs)} 这个值代表当前这个时间点，离今天结束还有多少秒的时间。可是这个值不可能小于 0，我甚至猜测是否执行上述代码第二行时是昨天，而第七行的时候是今天，这样能验证我们的猜测，即这个函数返回的值大于 24*3600，那 24*3600 - getPastTimeOfToday() / 1000 就小于 0 了。不过这个可能性比较低，于是我转而去服务器复现这个 bug，编写如下代码并执行： 12345const { setRedisKey } = require('./lib/xxx/xxx/redis')process.on('unhandledRejection', console.log)setRedisKey('abc', 'value', 0) 执行结果如下，与告警的错误栈一致： 既然复现了错误，回头继续找 bug，深入到 setRedisKey 代码： 123const setRedisKey = (key, value, expireTime = DEFAULT_EXPIRETIME) =&gt; { return redis.set(key, value, 'EX', Math.floor(expireTime))} 过期时间 expireTime 被 Math.floor 包裹，也就是说当 0 &lt; expireTime &lt; 1 时，Math.floor(expireTime) 的值为 0。那么当服务器时间到无限接近 00:00:00 时，getPastTimeOfToday 将返回 (24*3600-x)*1000 ，因为无限接近 00:00:00，因此 x 的值介于 0 - 1 之间（毕竟时间戳的最小单位是毫秒，想象下『当天剩余毫秒数』还有不到 1000 的情况）。 为什么 unhandledRejection对了，为什么该错误没有被捕获？如果一开始被捕获，也就有完整的错误栈，那么排查过程肯定会顺利很多。业务代码如下（已脱敏）： 123Promise.resolve().then(() =&gt; { Promise.reject(new Error('出错了！'))}).catch(console.error) 大家发现了吧，then 中的 Promise 没有返回，那么我返回了就一定会被捕获吗？在这种写法下是会的，大家可以尝试下，但是你写成下面这样，还是会 unhandledRejection： 123456789101112new Promise((resolve, reject) =&gt; { if (false) { resolve('对了！') } if (true) { return Promise.reject(new Error('出错了！')) } reject(new Error('兜底逻辑！'))}).catch(console.warn) 为什么呢？因为需要手动调用 reject 才可抛错，这就需要层层将 resolve 和 reject 传入可能报错的 Promise 才行，珍爱生命远离 Promise，大家还是尽量用 async/await 语法（相同功能下）。 解决方式原因既然依旧找到，解决方式也就出来了： 向下取整 Math.floor 改为向上取整 Math.ceil（不过极端情况下还是存在当天剩余毫秒数恰好为 0 的情况） 判断 expireTime 值小于等于 0 时，赋值为 1（续一秒，蛤蛤蛤），同时记录 warn 警告日志 排查未 return 的 Promise，统一返回 尝试找了下 typescript 限制 number 为正整数的方式，没有找到，有的话求大神告知？","link":"/2020/09/18/%E6%8E%92%E6%9F%A5%E5%AE%88%E5%80%99%E5%9C%A8%E9%9B%B6%E7%82%B9%E4%B8%A4%E5%88%86%E7%9A%84%20bug/"},{"title":"消失的 1px","text":"之前本人写过一篇文章，是关于1px边框的：从line-height到0-5px。文中提到用缩放的方法固然可行，但是在使用rem或者百分比单位时，时常会造成1px边框在某些机型下消失；而使用border-image方案则不会出现消失的情况；本文将探索该1px边框消失的原因以及后者为何能正常显示。 前因之前本人写过一篇文章，是关于1px边框的：从line-height到0-5px。文中提到用缩放的方法固然可行，但是在使用rem或者百分比单位时，时常会造成1px边框在某些机型下消失；而使用border-image方案则不会出现消失的情况；本文将探索该1px边框消失的原因以及后者为何能正常显示。 LayoutUnit在LayoutUnit中提到了两种将亚像素(即小数点像素)转换为真实物理像素的两种方法，示意图如下： enclosingIntRect x: floor(x) y: floor(y) maxX: ceil(x + width) maxY: ceil(y + height) width: ceil(x + width) - floor(x) height: ceil(y + height) - floor(y) pixelSnappedIntRect 采用该方式，最终形成的物理大小将会超过原来的小，使得盒模型出现溢出的风险。 pixelSnappedIntRect x: round(x) y: round(y) maxX: round(x + width) maxY: round(y + height) width: round(x + width) - round(x) height: round(y + height) - round(y) 采用这种方式的好处是能够保证最终渲染的物理大小不超过原来的大小，使得在屏幕等分出现小数的情况也不会溢出到下一行。本人将七个div等分整个屏幕宽度，在不同的分辨率下并没有发生溢出的情况，因此猜测浏览器采用了这套方案。 转换时的相互影响如果每个DOM节点都各自独立地采用上述方案之一，那么就不会出现1px消失的情况，然而事实上在文档流中，前一个节点所占用的大小经过亚像素转换之后，还会影响后一个节点的大小，从而影响后者进行亚像素转换。 此处以iPhone4屏幕大小为例，将其七等分： .box { font-size: 10px; width: 14.2857%; height: 14.2857%; background: pink; float: left; } .box:nth-child(2n) { background: gray; } 并打印计算宽度与实际渲染宽度： 1234$.each($(&quot;.box&quot;), function(index, val) {var computedWid = getComputedStyle(val).width;var wid = val.offsetWidth;$(val).html(computedWid + '&lt;br&gt;' + wid + 'px'); 显示效果如下： 其计算规则如下： 第一个box的大小为45.7031px，进位46px大小，导致其覆盖了后一个box，覆盖宽度为1-0.7031=0.2969px； 第二个box的大小此时缩减为45.7031-0.2969=45.4062px，因此舍入为45px，此时舍弃的0.4062px则合并到下一个box的上； 第三个box的大小此时扩大为45.7031+0.4062=46.1093px，因此舍入为46px，合并到下一个box的宽度为0.1093px； 第四个box的大小扩大为45.7031+0.1093px=45.8124px，因此进位为46px，覆盖下一个节点宽度0.1876px； 第五个box的大小缩减为45.7031-0.1876=45.5155px，因此进位为46px，覆盖后一个节点宽度0.4845px； 第六个box的大小缩减为45.7031-0.4845=45.2186px，因此舍入45px，合并到下一个节点宽度为0.2186px； 第七个box的大小扩大为45.7031+0.2186=45.9217px，因此舍入为46px； 可发现计算结果与显示效果吻合，读者可以通过调整不同的分辨率来测试，根据这个规则都能预测正确结果。 结论 采用scale、zoom、viewport等缩放方案实现的1px，由于实际上为0.5px的CSS像素，导致其有可能被上一个DOM节点所覆盖，从而导致其大小小于0.5px，进而导致其被舍入为0px，所以才会消失不见。 而采用border-image方式则不会消失，由于border-image方案的大小为1px的CSS像素，上一个DOM节点无论如何覆盖，最大不过0.499999px，即不超过0.5px，因此即使被覆盖0.499999px，其大小仍为0.511111px，最终效果也是进位到1px，因此该方案实现的1px边框会始终显示。","link":"/2016/04/17/%E6%B6%88%E5%A4%B1%E7%9A%841px/"},{"title":"深入浅出 Node.js の笔记","text":"前端开发人员上手 Node.js 还是需要课补一些服务端知识的。 章一，章二 单线程，使得 Node 不需要像多线程编程那样处处在意状态的同步问题（没有死锁，没有线程上下文交换） exports 对象用于导出当前模块的方法或变量，此外，在模块中还存在一个 module 对象，它代表模块自身，而 exports 是 module 的属性 exports 和 module.exports 的区别: module.exports 初始值为一个空对象 {} exports 是指向的 module.exports 的引用 require() 返回的是 module.exports 而不是 exports require 模块时优先从缓存中加载，而 Node 缓存的是编译和执行之后的对象 require 模块时，可省略扩展名，Node 会依次补足 .js, .json, .node 进行尝试，建议后缀为 .json 和 .node 时带上扩展名 在模块编译过程中，Node 对获取的 JavaScript 文件进行了头尾包装，使其执行在一个包含特定变量（exports，require，module，__filename，__dirname）的闭包中，该闭包返回模块的 exports 属性 章三 阻塞 IO: 等到系统内核层面完成所有操作之后，调用随之结束 非阻塞 IO: 调用之后立即返回（不携带处理完成的数据，因为尚未生成），之后还得通过文件描述符再次读取数据 非阻塞 IO 返回之后，CPU 时间片可以用于处理其它事务，但为了获取完整数据需要轮询，以判断操作是否完成 轮询满足了非阻塞 IO 确保获取完整数据的需求，但是对于应用程序而言，它仍然是一种同步，因为应用程序仍然需要等待 IO 完全返回 Node 的单线程仅仅是指 JavaScript 执行在单线程中，内部另有线程池完成 IO 任务 Node 经典的调用方式: 从 JavaScript 调用 Node 的核心模块；核心模块调用 C++ 内建模块；内建模块通过 libuv 进行系统调用（其中 libuv 作为封装层，有两个平台的实现） Node 的异步: 第一部分：JavaScript 调用立即返回；接着在进行系统调用过程中，创建了一个请求对象，该对象包含了从JavaScript 层传入的参数和当前方法，以及回调函数；接着将该对象推入系统的线程池中等待执行，而不用在乎该操作是否阻塞 IO 第二部分：线程池中的 IO 操作调用完毕之后，将 IO 结果通知并归还线程；此时在每次 Tick 的执行中，会调用特定方法检查线程池中是否有执行完的请求；若存在则将请求对象加入到 IO 观察者的队列中，然后将其当做事件处理 setTimeout 与 setInterval 的定时并不精确，相比 process.nextTick 更为浪费性能 process.nextTick 在每轮循环中将数组中的回调函数全部执行完毕，而 setImmediate 则在每轮循环中执行链表中的一个回调函数 章四 雪崩: 在高访问量，大并发量的情况下缓存失效的情景，此时大量的请求同时涌入数据库中数据库无法同时承受如此大的查询请求，进而往前影响到网站整体的响应速度 使用 once 解决雪崩问题: 1234567891011const proxy = new events.EventEmitter();let status = 'ready';const select = function(callback) { proxy.once('selected', callback); if(status === 'ready') { status = 'pending'; db.select('SQL', (ret) =&gt; { proxy.emit('selected', ret); status = 'ready'; }); }}; 将所有请求回调都压入事件队列中，使用 once 保证每个回调只会被执行一次，对于相同的 SQL 语句，保证只查询一次；SQL 在进行查询时，新到来的相同调用只需在队列中等待就绪即可，一旦查询结束，得到的结果可以被这些调用共同使用（因为都监听了 ‘selected’ 事件） 尾触发: next() 机制 章五 解除引用：delete 操作和重新赋值具有相同的效果，在 V8 中通过 delete 删除对象的属性有可能干扰V8的优化，所以建议使用赋值方式。 无法立即回收的内存有闭包和全局变量引用这两种情况。由于V8的内存限制，要十分小心此类变量是否无限制地增加，因为它会导致老生代中的对象增多。 堆中的内存总是小于进程的内存，这意味着 Node 中的内存使用并非都是通过 V8 进行分配的。那些不是通过 V8 分配的内存称为堆外内存。例如 Buffe r对象，它不经过V8的内存分配机制，所以也不会有堆内存的大小限制。 由于模块的缓存机制，模块是常驻老生代的。在设计模块时，要十分小心内存泄漏的出现。在下面的代码，每次调用 leak() 方法时，都导致局部变量 leakArray 不停增加内存的占用，且不被释放： 1234var leakArray = [];exports.leak = function () { leakArray.push(&quot;leak&quot; + Math.random());}; 如果模块不可避免地需要这么设计，那么请添加清空队列的相应接口，以供调用者释放内存。 ​ 深度的解决方案应该是监控队列的长度，一旦堆积，应当通过监控系统产生报警并通知相关人员。 另一个解决方案是任意异步调用都应该包含超时机制，一旦在限定的时间内未完成响应，通过回调函数传递超时异常，使得任意异步调用的回调都具备可控的响应时间，给消费速度一个下限值。 由于V8的内存限制，我们无法通过fs.readFile()和fs.writeFile()直接进行大文件的操作，而改用fs.createReadStream()和fs.createWriteStream()方法通过流的方式实现对大文件的操作。 章六 如果需要超过 8 KB 的 Buffer 对象，将会直接分配一个 SlowBuffer 对象作为 slab 单元，这个 slab 单元将会被这个大 Buffer 对象独占。 上面提到的 Buffer 对象都是 JavaScript 层面的，能够被 V8 的垃圾回收标记回收。但是其内部的 parent 属性指向的 SlowBuffer 对象却来自于 Node 中 C++ 层面上的 Buffe r对象，所用内存不在 V8 的堆中。 当进行小而频繁的 Buffer 操作时，采用 slab 的机制进行预先申请和事后分配，使得 JavaScript 到操作系统之间不必有过多的内存申请方面的系统调用。对于大块的 Buffer 而言，则直接使用 C++ 层面提供的内存，而无需细腻的分配操作。 buffer += chunk; 这句代码里隐藏了 toString() 操作，它等价于如下的代码： 1buffer = buffer.toString() + chunk.toString(); 通过预先转换静态内容为 Buffer 对象，可以有效地减少 CPU 的重复使用，节省服务器资源。在 Node 构建的 Web 应用中，可以选择将页面中的动态内容和静态内容分离，静态内容部分可以通过预先转换为 Buffer 的方式，使性能得到提升。由于文件自身是二进制数据，所以在不需要改变内容的场景下，尽量只读取 Buffer，然后直接传输，不做额外的转换，避免损耗。 章七 TCP 针对网络中的小数据包有一定的优化策略：Nagle算法。Nagle算法要求缓冲区的数据达到一定数量或者一定时间后才将其发出，所以小数据包将会被Nagle算法合并，以此来优化网络。这种优化虽然使网络带宽被有效地使用，但是数据有可能被延迟发送。 在 Node 中，TCP 默认启用了 Nagle 算法，调用 socket.setNoDelay(true) 关闭 Nagle 算法，使得write() 可以立即发送数据到网络中。另一个需要注意的是，尽管在网络的一端调用 write() 会触发另一端的 data 事件，但是并不意味着每次 write() 都会触发一次 data 事件，在关闭掉 Nagle 算法后，另一端可能会将接收到的多个小数据包合并，然后只触发一次 data 事件。 UDP 与 TCP 同属于网络传输层。TCP 中连接一旦建立，所有的会话都基于连接完成，客户端如果要与另一个 TCP 服务通信，需要另创建一个套接字来完成连接。但在 UDP 中，一个套接字可以与多个 UDP 服务通信，常常应用在那种偶尔丢一两个数据包也不会产生重大影响的场景，比如音频、视频等。UDP 目前应用很广泛，DNS 服务即是基于它实现的。 报文体部分则抽象为一个只读流对象，如果业务逻辑需要读取报文体中的数据，则要在这个数据流结束后才能进行操作，如下所示： 123456789101112function (req, res) { // console.log(req.headers); var buffers = []; req.on('data', function (trunk) { buffers.push(trunk); }).on('end', function () { var buffer = Buffer.concat(buffers); // TODO res.end('Hello world'); });} HTTP响应对象：它封装了对底层连接的写操作，可以将其看成一个可写的流对象。它影响响应报文头部信息的 API 为 res.setHeader() 和 res.writeHead()。在上述示例中： res.writeHead(200, {‘Content-Type’: ‘text/plain’}); 其分为 setHeader() 和 writeHead() 两个步骤。它在 http 模块的封装下，实际生成如下报文： &lt; HTTP/1.1 200 OK&lt; Content-Type: text/plain 我们可以调用 setHeader 进行多次设置，但只有调用 writeHead 后，报头才会写入到连接中。除此之外，http模块会自动帮你设置一些头信息，如下所示： &lt; Date: Sat, 06 Apr 2013 08:01:44 GMT&lt; Connection: keep-alive&lt; Transfer-Encoding: chunked​ 报文体部分则是调用 res.write() 和 res.end() 方法实现，差别在于 res.end() 会先调用 write() 发送数据，然后发送信号告知服务器这次响应结束。 响应结束后，HTTP 服务器可能会将当前的连接用于下一个请求，或者关闭连接。值得注意的是，报头是在报文体发送前发送的，一旦开始了数据的发送，writeHead() 和 setHeader() 将不再生效。另外，无论服务器端在处理业务逻辑时是否发生异常，务必在结束时调用res.end()结束请求，否则客户端将一直处于等待的状态。 同时 http 模块提供了一个底层 API：http.request(options, connect)，用于构造 HTTP 客户端。 为了重用 TCP 连接，http 模块包含一个默认的客户端代理对象 http.globalAgent。它对每个服务器端（host + port）创建的连接进行了管理，默认情况下，通过 ClientRequest 对象对同一个服务器端发起的 HTTP 请求最多可以创建 5 个连接。 除此之外，WebSocket 与传统 HTTP 有如下好处： 客户端与服务器端只建立一个TCP连接，可以使用更少的连接。 WebSocket服务器端可以推送数据到客户端，这远比HTTP请求响应模式更灵活、更高效。 有更轻量级的协议头，减少数据传送量。 章八基础功能与数据上传 RESTful 类 Web 服务中请求方法： PUT 代表新建一个资源 POST 表示要更新一个资源 GET 表示查看一个资源 而 DELETE 表示删除一个资源 我们可以通过请求方法来决定响应行为，如下所示： 12345678910111213141516function (req, res) { switch (req.method) { case 'POST': update(req, res); break; case 'DELETE': remove(req, res); break; case 'PUT': create(req, res); break; case 'GET': default: get(req, res); }} ​ var url = require('url'); var querystring = require('querystring'); var query = querystring.parse(url.parse(req.url).query); //更简洁的方法是给url.parse()传递第二个参数，如下所示： var query = url.parse(req.url, true).query; // 它会将foo=bar&amp;baz=val解析为一个JSON对象，如下所示： { foo: 'bar', baz: 'val' } 12345678910113. 要注意的点是，如果查询字符串中的键出现多次，那么它的值会是一个数组，如下所示： ```js // foo=bar&amp;foo=baz var query = url.parse(req.url, true).query; // { // foo: ['bar', 'baz'] // } 业务的判断一定要检查值是数组还是字符串，否则可能出现TypeError异常的情况。 Cookie 的处理分为如下几步： 服务器向客户端发送 Cookie。 浏览器将 Cookie 保存。 之后每次浏览器都会将 Cookie 发向服务器端。 HTTP_Parser 会将所有的报文字段解析到 req.headers 上，那么 Cookie 就是 req.headers.cookie。 响应的 Cookie 值在 Set-Cookie 字段中，规范中对它的定义如下所示：Set-Cookie: name=value; Path=/; Expires=Sun, 23-Apr-23 09:01:35 GMT; Domain=.domain.com;其中 name=value 是必须包含的部分，其余部分皆是可选参数。 path 表示这个 Cookie 影响到的路径，当前访问的路径不满足该匹配时，浏览器则不发送这个 Cookie。 HttpOnly 告知浏览器不允许通过脚本 document.cookie 去更改这个 Cookie 值，事实上，设置HttpOnly 之后，这个值在 document.cookie 中不可见。但是在 HTTP 请求的过程中，依然会发送这个Cookie到服务器端。 Secure，当Secure值为true时，在HTTP中是无效的，在HTTPS中才有效，表示创建的Cookie只能在HTTPS连接中被浏览器传递到服务器端进行会话验证，如果是HTTP连接则不会传递该信息，所以很难被窃听到。 如果在域名的根节点设置 Cookie，将使得几乎所有子路径下的请求都会带上这些Cookie。 解决方法：为静态组件使用不同的域名 为不需要 Cookie 的组件换个域名减少无效 Cookie 的传输。 同时还可以突破浏览器下载线程数量的限制，因为域名不同，可以将下载线程数翻倍。 缺点是域名转换为 IP 需要进行 DNS 查询，多一个域名就多一次 DNS 查询。 为了解决 Cookie 敏感数据的问题，Session 的数据只保留在服务器端，使数据的安全性得到一定的保障。 两种方式： 第一种：基于Cookie来实现用户和数据的映射 第二种：通过查询字符串来实现浏览器端和服务器端数据的对应 注: 用户访问 http://localhost/pathname 时，如果服务器端发现查询字符串中不带 session_id 参数，就会将用户跳转到 http://localhost/pathname?session_id=12344567 这样一个类似的地址。如果浏览器收到302状态码和Location报头，就会重新发起新的请求。 有的服务器在客户端禁用Cookie时，会采用这种方案实现退化。通过这种方案，无须在响应时设置Cookie。但是这种方案带来的风险远大于基于Cookie实现的风险，因为只要将地址栏中的地址发给另外一个人，那么他就拥有跟你相同的身份。Cookie的方案在换了浏览器或者换了电脑之后无法生效，相对较为安全。 为了解决性能问题和 Session 数据无法跨进程共享的问题，常用的方案是将 Session 集中化，将原本可能分散在多个进程里的数据，统一转移到集中的数据存储中。工具有 Redis、Memcached 等，通过这些高效的缓存，Node 进程无须在内部维护数据对象，垃圾回收问题和内存限制问题都可以迎刃而解，并且这些高速缓存设计的缓存过期策略更合理更高效，比在Node中自行设计缓存策略更好。 采用第三方缓存来存储 Session 引起的一个问题是会引起网络访问。理论上来说访问网络中的数据要比访问本地磁盘中的数据速度要慢，因为涉及到握手、传输以及网络终端自身的磁盘I/O等，尽管如此但依然会采用这些高速缓存的理由有以下几条： Node 与缓存服务保持长连接，而非频繁的短连接，握手导致的延迟只影响初始化。 高速缓存直接在内存中进行数据存储和访问。 缓存服务通常与Node进程运行在相同的机器上或者相同的机房里，网络速度受到的影响较小。 ETag: 由服务端生成，服务端还可以决定它的生成规则，例如根据文件内容生成 Hash 值 数据上传与安全： 内存限制：1. 限制上传内容的大小，一旦超过限制，停止并响应 400 状态码；2. 通过流式解析，将数据流导向磁盘中，Node 只保留文件路径等小数据 CSRF：跨站请求伪造，可为每个请求的用户在 Session 中赋予一个随机值 路由解析与中间件 MVC Controller，一组行为的集合 Model，数据相关的操作和封装 View，视图的渲染 require 的缓存机制使得只有在首次 require 时是阻塞的 RESTful：将服务器端提供的内容看做一个资源，对该资源的操作只要体现在 HTTP 请求方法上： POST /user/tihu DELETE /user/tihu PUT /user/tihu GET /user/tihu Connect 中间件使用 next() 进行尾触发 中间件与性能： 编写高效的中间件：提高单个处理单元的处理速度，以尽早调用 next()；缓存需要重复计算的结果；避免不必要的计算 合理使用路由：例如只处理静态资源的中间件，可限定路由（app.use(‘/public’, statcFile))，从而避免对整站都处理 章九 创建子进程：child_process 模块的四种方法 spawn() 启动一个子进程来执行命令 exec() 与 spawn 不同，它多了一个用于获知子进程状况的回调 execFile() 启动一个子进程来执行可执行文件 fork() 与 spawn 类似，不同点在于创建的子进程只需指定 JavaScript 文件模块 注：其中 2，3 可设置超时 子进程对象的 send() 方法支持发送以下五种类型的句柄 net.Socket，TCP 套接字 net.Server， TCP 服务器 net.Native， C++ 层面的 TCP 套接字或 IPC 管道 dgram.Socket，UDP 套接字 dgram.Native， C++ 层面的 UDP 套接字 多个应用监听相同端口时，文件描述符同一时间只能被某个进程所用，即抢占式的，因此只有一个进程能够抢到连接 除了 send() 外，还能通过 kill() 方法给子进程发送消息，kill() 方法只是给子进程发送了一个系统信号，进程在收到这些系统信号时，做出约定的行为 自杀信号：当所有进程都处于等待退出状态，并停止接收新连接，如果等到进程退出才重启，则此时新来的请求可能存在没有工作进程为新用户服务的情景，从而丢失请求；解决方法是在子进程决定退出前主动向父进程发送一个 “自杀信号” 通知父进程，使得父进程能够及时创建新进程。 限量重启：当重启过于频繁时，主动通知进程放弃重启 状态共享，使用第三方进行数据存储，比如数据库，磁盘文件，缓存服务（如 Redis），因此需要一种机制通知各个子进程：各个子进程向第三方进行定时轮询 状态共享，主动通知，可设计一个进程只进行轮询和通知，如果想跨越多台服务器，可采用 UDP 或 TCP 方案 章十 单测原则 单一职责 接口抽象 层次分离 TDD：测试驱动开发；BDD：行为驱动开发；二者差别： 关注点不同，TDD 关注所有功能是否被正确实现，每个功能对应一个测试用例；BDD 关注整体行为是否符合预期，适合自顶向下的设计方式 表达方式不同，TDD 偏向于功能说明书风格，而 BDD 更接近与自然语言的习惯 BDD：before 与 after 分别在进入和退出 describe 时触发，beforeEach 与 afterEach 分别在 describe 中的每个测试用例执行前和执行后触发 性能测试 基准测试，统计在多少时间内执行了多少次某个方法 压力测试，对网络接口进行压测的指标主要有吞吐率，响应时间和并发数","link":"/2016/12/25/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%20Node.js%20%E3%81%AE%E7%AC%94%E8%AE%B0/"},{"title":"第十一届 D2 会议的观后感与总结","text":"生平第一次参加这种技术会议，围观各种前端界网红，感觉知乎和微博里的网红都触手可及，记下这篇观后感与总结。 Weex 在双十一会场的大规模应用 鬼道 淘宝 相比去年双十一，weex 在今年双十一的主会场中几乎都使用了 weex DOM 嵌套不要太深，否则将导致性能问题 长列表滚动-性能问题，H5 lazy load 体验不好，原生好于 weex 帧率方面，weex 在 Android 与 iOS 上表现一致，而 H5 在低端机上（特别是 Android）表现较差 iOS 中打开多个 weex 页面，使得内存占用过大而导致 crash 解决方案： 主会场单实例 push 页面设置上限（双十一中上限为 5） H5 降级方案，不过几乎没开启 iOS JSCore 内存，通过分析 JS Heap，发现是 JS 引用持有导致的内存不释放（好奇 Android 为什么没有问题，是安卓内存够大不在乎吗？） 通过预加载提高页面秒开率： a. App 启动预先下载 JS； b. 通过长连通道 push； c. 本地 IO； d. 平台自动化对接； framework &amp; bundle 分离，共有逻辑前置 + 异步执行 Node &amp; Tree 渲染结合 Node: 最小颗粒度渲染 解析 DOM 后立即显示 不会长时间阻塞主线程 可能造成多次冗余的 Layout Tree: 块级渲染 只需 Layout 一次 可能阻塞主线程 weex 存在的缺点 发布无灰度 缺失兼容性与性能验收 线上监控未到人 组件生态相比 RN，差距大 富交互能力 常用 API 未来会丰富组件市场，同时添加研发支撑（上述1-3） XCore —蘑菇街移动端动态跨平台开发框架 蘑菇街 王兴楠 移动端业务场景定制的一套动态跨平台开发框架，但是目前并不开源 XCore 封装了底层，自称新造了轮子，而不需要改变现有框架的编码方式(Vue.js，React，DOM)，即车不需要变 与 weex / react-native 差异，三棵树，DOM tree，Shadow tree ，Native tree 以 Web 的方式快速开发与部署可运行于三端（H5，Android，iOS）的代码 实现了一套标准的 Web 子集 对比 RN，Weex： a. 定位不同，解决特定业务场景的问题，并不是一个业界通用的开源方案 b. 目标不同，采用了一个浏览器的架构，不绑定固定的前端框架 只要使用的 DOM 层面或者是模板的层面，在 XCore 模板范围内就可以. 基于云端真机的无线调试解决方案 淘宝 肖焉 真机放置在机房中，远程申请进行调试，只对阿里集团内部开放，对外开放已在计划中。 这种核心竞争力大概不会对外开放吧。 蚂蚁财富的 BFF 实践 蚂蚁金服 汤 尧 BFF: Backends for Frontends，复杂环境的必然产物 核心思想 在前端页面与后端 Java 间加入一层 Node，使 Node 直接调用 Java jar 包(hessian)，Node 层完成多端 API 接入、裁剪、格式化、聚合编排，从而控制接口数量，规范数据格式，只把用户关心的数据输出给界面，同时也方便了数据的 mock 业务现状 服务层 API 相对稳定 体验者 API 经常变化 场景 多APP适配： Error code 统一管理 数据一致性 免登 业务日志… 聚合 简化客户端逻辑，减少网络开销（毕竟有些后端接口的数据，前端可能只需要获取其中的一部分） 敏感信息过滤（我朝你懂的） Node.js 在 YunOS 中的最佳实践 YunOS 逸臻 把 Node.js 引入到 YunOS 的基础架构中 对 IO 优先的 Looper 机制的改造和优化 Node.js 的消息循环机制完全由 IO 事件来驱动 终端设备的场景在 UI 渲染及非 IO 任务的及时响应的需求 解决：将系统任务与 Node.js 的 IO 任务融合，二者地位基本相当，系统任务优先级略高；系统任务通过异步事件接入 Node.js 的队列中 性能优化 将系统级 JS 模块合并加载，减少 IO 次数 Code cache，预先将 JavaScript 代码编译成 cache 文件，运行时直接加载 cache 文件，调过编译过程 require 的懒加载，require 时不真正加载模块，而是在第一次访问对象的时候进行加载，从而减轻启动时的压力，优化启动时间 提出模块可卸载概念，不过这样的话，是否顶部 require 的模块并不是都用的了？因为无法获知该模块是否在某个地方被卸载了 NW.js: 集成DOM和Node.js的编程方式 Intel 王文睿 NW.js (原名 node-webkit )通过集成 Chromium 和 Node.js感觉枯燥，讲的不好，回答问题也不诚心 在 DOM 中直接调用 Node.js 介绍 NW 的主要功能和最新进展 个人比较看好 electron，成熟应用比较多（Atom, VSCode)，社区支持比较好 从 React 到 ClojureScript 饿了么 题叶 主要介绍了一些函数式编程的概念 JavaScript 有种通过自我阉割来实现函数式编程 介绍性较多，走马观花 大数据下的前端优化实战 UC 庞锦贵 借助 UC 浏览器的海量用户产生的大数据，针对特定场景的一些优化（场景性特别强，有点 hack） 通过在浏览器内核打点的方式进行数据分析（希望我司未来的客户端 webview 实现类似的打点分析）","link":"/2016/12/24/%E7%AC%AC%E5%8D%81%E4%B8%80%E5%B1%8A%20D2%20%E4%BC%9A%E8%AE%AE%E6%80%BB%E7%BB%93/"},{"title":"给你心爱的 npm 包上个『北京户口』","text":"记录解决如何批量将 npm 包从一个 npm 源迁移到另一个源 背景一切从我司有两个 npm 源说起，一个叫杭州源，另一个叫北京源。本来各用各的相安无事，直到有一天我们想依赖另一个源的 npm 包时，就相互拉取不到对方的源了。杭州源这边也有做过兼容，当拉取的源不存在时，就尝试去北京源拉取，看似解决了这个问题。但是这只解决了我们依赖北京源的 npm 包的问题，而我们有大量的包需要推广到其它部门，因此我们决定切换到使用人数更多的北京源，目前团队正处于集体转向使用北京源的阶段，不过经常会遇到如下问题： 项目拉取下来，执行 npm install，提示某些包不存在，然后查看当前源，切换到另一种源，继续 npm install； 老项目使用杭州源，但是依赖了北京源的 npm 包，虽然杭州源会在拉取不到包时主动向北京源拉取，但是这个机制经常出问题，有时会阻塞服务的构建，得解决这个依赖问题才能重新构建项目； 新项目使用北京源，但是依赖了杭州源的 npm 包，这个场景比较棘手，北京源没有向杭州源拉取包的机制，因此需要将杭州源的包重新在北京源上发布，如果采用手动发布，版本多的包会很繁琐还容易出错；如果只发布部分包，会导致两边不一致，导致无法拉取到未发布的包； 发布一个包，只向杭州源发布，那使用北京源的项目很可能更新不到你的新版本，反之亦然； 咋整？ 解决思路为了加快结束这个过渡期，需要我们全面切换到北京源，有两个问题需要解决： 团队内的杭州源 npm 包的各个版本需要在北京源重新发布一次，除了版本外，dist-tag 也要同步到北京源； 通知团队成员将项目切换到北京源，并不再使用杭州源，如有遗漏未同步到北京源的 npm 包告知我将其同步； 那么，我们团队有多少个包呢？已知的 @kd scope 下的包有 49 个，而更多的包都在 @dd scope 下，目前收集到的有 50 个。假设平均一个包有 10 个版本，那么粗略估计有上千个版本需要重新发布，所以手动是不可能手动的，得想法子通过自动化的方式批量同步。 来看看到底咋整吧！ 实现方式作为一个前端开发，当然选择 Node.js 来编写脚本或命令行工具解决这种重复劳动了！先来回顾下如何发布一个 npm 包：只要进入 npm 包的根目录，执行 npm publish 即可。所以我们只要找到一个方式能获取到每个包的每个版本的所有代码即可！那有这种方式吗？ 有的，有的！回忆下你使用 npm view 查询一个 npm 包时的情形： 聪明的你肯定发现 .tarball: https://registry.npmjs.org/koa/-/koa-2.13.0.tgz，这个包正是当前最新版本 2.13.0 的 koa 包。下载解压后发现这正是我们要的，只要进入该目录执行 npm publish 即可： 那我们能获取指定版本的 tgz 包吗？当然，使用 npm view koa@1.0.0 即可查看指定版本。等等，我没法知道当前 koa 包有哪些版本诶？别慌，试试这个命令 npm view koa --json，答案就藏在 versions 字段里： 此外 dist-tags 字段也需要同步（ 默认不同步 tag，这有可能导致杭州源的 tag 覆盖了北京源的 tag，而杭州源的 tag 可能不是最新的）。 目前万事俱备，离搞定只差一个程序猿了！核心逻辑即是根据包名拉取所有版本的 tgz 压缩包，同时解压所有 tgz 压缩包，进入对应目录执行 npm publish： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192// 获取该包的所有版本let resulttry { result = await execa.command(`npm view ${npmPacakgeName} --json --registry=${from}`)} catch (error) { // 查询失败的日志写入文件，方便追踪 console.log(chalk.red(`获取列表失败：${npmPacakgeName}`)) const content = await fs.readFile(errorLogFile) await fs.writeFile(errorLogFile, `${content}\\n获取列表失败：${npmPacakgeName}: ${error.stack}`) return}const json = result.stdout ? eval(`temp = ${result.stdout}`) : {}const tagTaskList = Object.keys(json['dist-tags']).map(npmPackageTag =&gt; { return async () =&gt; { try { await execa.command(`npm dist-tag add ${npmPacakgeName}@${json['dist-tags'][npmPackageTag]} ${npmPackageTag} --registry=${to}`) console.log(chalk.green(`成功同步 tag：${npmPacakgeName}@${npmPackageTag}`)) } catch (error) { // 发布失败的日志写入文件，方便追踪 console.log(chalk.red(`tag 添加失败：${npmPacakgeName}@${json['dist-tags']}`)) const content = await fs.readFile(errorLogFile) await fs.writeFile(errorLogFile, `${content}\\ntag 添加失败：${npmPacakgeName}@${json['dist-tags']}: ${error.stack}`) } }})const publishTaskList = json.versions &amp;&amp; json.versions.map(npmPacakgeVersion =&gt; { // 该包每个版本的处理逻辑 return async () =&gt; { try { // 如果 ${to}已经存在该版本，则不处理 // FIXME: 处理 ${to}不存在该包的情况 let result try { result = await execa.command(`npm view ${npmPacakgeName}@${npmPacakgeVersion} --registry=${to}`) if (result.stdout) { console.log(chalk.green(`已存在于 ${to}，无需同步：${npmPacakgeName}@${npmPacakgeVersion}`)) return } } catch (error) { if (!error.message.includes('npm ERR! code E404')) { throw error } } // 去除形如 @scope 的字符串 const packagePath = path.join(__dirname, npmPacakgeName, `${npmPacakgeName.replace(/@.*\\//, '')}-${npmPacakgeVersion}`) const tgzPath = `${packagePath}.tgz` // 下载包 await download( `${from}/${npmPacakgeName}/download/${npmPacakgeName}-${npmPacakgeVersion}.tgz`, npmPacakgeName ) // 解压包 await compressing.tgz.uncompress( tgzPath, packagePath ) // 删除压缩包 await execa.command(`rm ${tgzPath}`) // 发布 await execa.command(`npm publish --tag=sync --registry=${to}`, { cwd: path.join(packagePath, 'package') }) console.log(chalk.green(`${npmPacakgeName}@${npmPacakgeVersion} 成功同步 ${to}！`)) } catch (error) { // 发布失败的日志写入文件，方便追踪 console.log(chalk.red(`version 发布失败：${npmPacakgeName}@${npmPacakgeVersion}`)) const content = await fs.readFile(errorLogFile) await fs.writeFile(errorLogFile, `${content}\\nversion 发布失败：${npmPacakgeName}@${npmPacakgeVersion}: ${error.stack}`) } }})// 限制并行数为 5，防止 npm 网站报错await Promise.all(publishTaskList.map(pLimit(5)))// 修复所有 tag，并行会导致不成功，此处改为串行// 默认不同步 tag，有可能导致${from}的 tag 覆盖了 ${to}的 tag，而${from}的 tag 可能不是最新的if (options.syncTag) { await Promise.all(tagTaskList.map(pLimit(1)))} 请留意并行问题，并发量太大 npm 网站会报错（估计被拦截了）。 如何使用本来仅提供脚本执行方式，为了让大家也能方便使用（不要再叫我同步了）。贴心的我已把这个脚本实现为一个命令行工具，使用方式如下： 内部工具，此处不提供，大伙可自行实现 安装命令行npm i @dd/npm-sync -g 执行版本同步npm-sync packageName 执行版本和tag同步npm-sync packageName --syncTag=true 更多使用方式npm-sync --help TODOS 欢迎提交代码 支持『发布前有编译钩子』的包 登录前置校验","link":"/2020/09/27/%E7%BB%99%E4%BD%A0%E5%BF%83%E7%88%B1%E7%9A%84%20npm%20%E5%8C%85%E4%B8%8A%E4%B8%AA%E5%8C%97%E4%BA%AC%E6%88%B7%E5%8F%A3/"},{"title":"redux 中间件入门到编写，到改进，到出门","text":"春江花月夜调 bug，孤独寂寞日著 blog。吾母昨夜托梦，吾儿只身在外，读书撩妹两手要抓，前端知识切莫荒废。如无对象可面向，可以学学函数式。多写代码少睡觉，还有周五的周报。redux 要会，middleware 能写，有空记得写博客，写好发给我看看。 惊醒之余，其敦敦教诲不敢忘，乃正襟危坐，挑灯写下这篇博客，感动~ redux 中间件redux 提供了类似 Web 开发的中间件机制，Web 中经过中间件的是一个个请求，而 redux 中经过中间件的是一个个 action，使得开发人员能够在中间件中针对特定 action 进行各种统一的处理，比如日志打印，数据请求，错误处理等。 如何使用redux 提供 applyMiddleware 方法，通过如下方式即可应用中间件： 1234567891011121314151617import { createStore, applyMiddleware, compose,} from 'redux'import nextAndRequest from './middleware/redux-next-and-request'import errorCatcher from './middleware/redux-error-catcher'import reducer from '../reducer'const createStoreWithMiddleware = compose( applyMiddleware( nextAndRequest, errorCatcher, ), DevTools.instrument(), window.devToolsExtension(),)(createStore) 实现原理compose 函数此处有个神奇的函数，即 compose，该函数在 applyMiddleware 中也是核心代码，正是它实现了 redux 中间件的机制，我们来看看 compose 的源码： 12345678910111213141516171819202122/** * Composes single-argument functions from right to left. The rightmost * function can take multiple arguments as it provides the signature for * the resulting composite function. * * @param {...Function} funcs The functions to compose. * @returns {Function} A function obtained by composing the argument functions * from right to left. For example, compose(f, g, h) is identical to doing * (...args) =&gt; f(g(h(...args))). */export default function compose(...funcs) { if (funcs.length === 0) { return arg =&gt; arg } if (funcs.length === 1) { return funcs[0] } return funcs.reduce((a, b) =&gt; (...args) =&gt; a(b(...args)))} 从注释中可知，它的作用是从右到左将多个函数组合成一个新函数，其中最右边的函数消耗了该新函数的参数，并逐级向左作为参数依次执行； 执行 compose(f1, f2, f3) 可得 (…args) =&gt; f1(f2(f3(…args)))；核心操作为 reduce，详细使用方式可参看文档 12345678910111213141516第一次 reduce previousValue: f1; currentValue: f2; returnValue: (...args1) =&gt; f1(f2(...args1)) // 记为 R1第二次 reduce previousValue: R1 currentValue: f3; returnValue: (...args2) =&gt; R1(f3(...args2)) // 记为 R2其中 R2:(...args2) =&gt; ((...args1) =&gt; f1(f2(...args1)))(f3(...args2))此时传入 args 执行 R2：第一步： 得到 ((...args1) =&gt; f1(f2(...args1)))(f3(args)) // 记为 R3第二步：f3(args) 即是 R3 的参数 (...args1)，继续执行可得 f1(f2(f3(args))) 其实之前这个 compose 方法不是使用 reduce 实现的，而是使用 reduceRight 实现 composeRight，因此对比新版实现，比较好理解，原来的版本为： 新版 Merge Request新版的方式使用惰性求值，性能有提升 1234567891011121314151617181920212223242526/** * Composes single-argument functions from right to left. The rightmost * function can take multiple arguments as it provides the signature for * the resulting composite function. * * @param {...Function} funcs The functions to compose. * @returns {Function} A function obtained by composing the argument functions * from right to left. For example, compose(f, g, h) is identical to doing * (...args) =&gt; f(g(h(...args))). */export default function compose(...funcs) { if (funcs.length === 0) { return arg =&gt; arg } funcs = funcs.filter(func =&gt; typeof func === 'function') if (funcs.length === 1) { return funcs[0] } const last = funcs[funcs.length - 1] const rest = funcs.slice(0, -1) return (...args) =&gt; rest.reduceRight((composed, f) =&gt; f(composed), last(...args))} 让我们回到： 12345678const createStoreWithMiddleware = compose( applyMiddleware( nextAndRequest, errorCatcher, ), DevTools.instrument(), window.devToolsExtension(),)(createStore) createStoreWithMiddleware 的最终值为： 1applyMiddleware(nextAndRequest,errorCatcher)(DevTools.instrument()(window.devToolsExtension()(createStore))) applyMiddleware 函数其源码如下： 12345678910111213141516171819202122232425262728293031323334353637import compose from './compose'/** * Creates a store enhancer that applies middleware to the dispatch method * of the Redux store. This is handy for a variety of tasks, such as expressing * asynchronous actions in a concise manner, or logging every action payload. * * See `redux-thunk` package as an example of the Redux middleware. * * Because middleware is potentially asynchronous, this should be the first * store enhancer in the composition chain. * * Note that each middleware will be given the `dispatch` and `getState` functions * as named arguments. * * @param {...Function} middlewares The middleware chain to be applied. * @returns {Function} A store enhancer applying the middleware. */export default function applyMiddleware(...middlewares) { return (createStore) =&gt; (reducer, preloadedState, enhancer) =&gt; { const store = createStore(reducer, preloadedState, enhancer) let dispatch = store.dispatch let chain = [] const middlewareAPI = { getState: store.getState, dispatch: (action) =&gt; dispatch(action) } chain = middlewares.map(middleware =&gt; middleware(middlewareAPI)) dispatch = compose(...chain)(store.dispatch) return { ...store, dispatch } }} 一个 redux 中间件的结构： 123store =&gt; next =&gt; action =&gt; { // 中间件逻辑代码} 假设有三个中间件 M1, M2, M3，应用 applyMiddleware(M1, M2, M3) 将返回一个闭包函数，该函数接受 createStore 函数作为参数，使得创建状态树 store 的步骤在这个闭包函数内执行;接着将 store 重新组装成 middlewareAPI 作为新的 store，也就是我们编写的中间件最外层函数的参数 store，这样中间件就可以根据状态树进行各种操作了。 可以发现重新组装之后的 store 只有两个方法，一个是用户获取 state 的 getState 方法，另一个是用于分发 action 的 dispatch，而 setState、subscribe、replaceReducer 等方法则不提供，setState 在设置状态时重新 render 可能会触发新的 action 而导致死循环；setState 本身就是用于订阅每个 dispatch 操作，此时 dispatch 就在你手上（next)，根本不需要订阅；replaceReducer 用于动态加载新的 reducer，我猜你用不到。 将中间件数组中的函数逐一传入参数 middlewareAPI 并执行，从而得到 chain 数组，此时 chain 数组中的每个函数长这样： 123next =&gt; action =&gt; { // 中间件逻辑代码} 核心代码解读 dispatch = compose(…chain)(store.dispatch) 假设 chain 是包含 C1, C2, C3 三个函数的数组，那么 compose(…chain)(store.dispatch) 即是 C1(C2(C3(store.dispatch))), 因此易知： applyMiddleware 的最后一个中间件 M3 中的 next 就是原始的 store.dispatch; M2 中的 next 为 C3(store.dispatch); M1 中的 next 为 C2(C3(store.dispatch)); 最终将 C1(C2(C3(store.dispatch))) 作为新的 dispatch 挂在 store 上返回给用户，因此这就是用户切实调到的 dispatch 方法，既然层层执行了 C3，C2, C1，那么一个中间件已经被拆解为： 123action =&gt; {} 触发 action 的完整流程有了这个 dispatch 方法和被扒光的中间件，我们来梳理一遍当用户触发一个 action 的完整流程： 手动触发一个 action：store.dispatch(action)； 即调用 C1(C2(C3(store.dispatch)))(action)； 执行 C1 中的代码，直到遇到 next(action)，此时 next 为 M1 中的 next，即：C2(c3(store.dispatch))； 执行 C2(c3(store.dispatch))(action)，直到遇到 next(action)，此时 next 为 M2 中的 next，即：C3(store.dispatch)； 执行 C3(store.dispatch)(action)，直到遇到 next(action)，此时 next 为 M3 中的 next，即：store.dispatch； 执行 store.dispatch(action)，store.dispatch 内部调用 root reducer 更新当前 state； 执行 C3 中 next(action) 之后的代码 执行 C2 中 next(action) 之后的代码 执行 C1 中 next(action) 之后的代码 即：C1 -&gt; C2 -&gt; C3 -&gt; store.dispatch -&gt; C3 -&gt; C2 -&gt; C1 洋葱模型有没有!!! 如何编写讲了这么多，终于切入正题，开始写中间件了，目标是实现中间件，使得异步请求，错误处理都能经由中间件处理；而不需要每次手动繁琐的发起异步请求，同时每个异步请求语句之后都手动处理错误代码。 先从简单的错误处理中间件开始~ 错误处理中间件通过检测 action 上是否存在 error 字段，来决定是否抛出错误 1234567891011121314151617import { notification } from 'antd'export default store =&gt; next =&gt; async action =&gt; { try { if(action.error) { throw new Error(action.error) } else { next(action) } } catch (err) { notification.error({ message: '错误信息', description: err.message }); throw err }} 当发现 action 中有 error 字段，则抛出错误，这个字段可由上游中间件出错后，将对应的错误信息挂在 action.error 上，使得本中间件能够处理这个错误，由于项目基于 antd，此处将所有错误都通过 notification 组件在右上角弹窗显示； 如果做成通用的错误处理的话，可以再包一层函数，传入错误处理函数，便能够自定义错误处理函数了： 123456789101112export default handler =&gt; store =&gt; next =&gt; action =&gt; { try { if(action.error) { throw new Error(action.error) } else { next(action) } } catch (err) { handler &amp;&amp; handler(err) throw err }} 则使用方式变为： 123456789101112const createStoreWithMiddleware = compose( applyMiddleware( nextAndRequest, errorHandler(err =&gt; { notification.error({ message: '错误信息', description: err.message }) }), ), window.devToolsExtension)(createStore) 异步请求处理中间件版本一通过判断 action 字段上是否用 url 字段来判断是否需要发起异步请求，同时将请求结果挂在 action 的 result 字段上，供下一个中间件或 reducer 使用。 123456789101112131415161718192021222324252627282930313233343536373839404142import request from './request'export default store =&gt; next =&gt; async action =&gt; { if (action.url) { try { const execAction = async act =&gt; { if(act.url) { const { code, data, error, } = await request({ url: act.url, method: act.method || 'get', data: act.data || {}, }) if (code !== 0) { throw new Error(error || '未知错误！') } else { return data } } } const result = await execAction(action) next({ result, ...action }) } catch (error) { next({ error: error.message, }) } } else { next(action) }} 版本二由于本项目大部分情况需要在执行一个异步 action 之后，再重新执行一个异步 action，达到更新当前列表的目的。 例如删除或添加一条记录后，希望更新当前列表信息 因此做如下更改，在 action 上增加一个 nextAction 字段，使得能够在执行当前 action 之后，接着执行一个 action： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import request from './request'export default store =&gt; next =&gt; async action =&gt; { if (action.url || action.nextAction) { try { const execAction = async act =&gt; { if(act.url) { const { code, data, error, } = await request({ url: act.url, method: act.method || 'get', data: act.data || {}, }) if (code !== 0) { throw new Error(error || '未知错误！') } else { return data } } } const result = await execAction(action) next({ result, ...action }) if (action.nextAction) { const act = action.nextAction const nextAction = typeof act === 'function' ? await act(result, action) : act const nextResult = await execAction(nextAction) next({ result: nextResult, lastResult: result, ...nextAction }) } } catch (error) { next({ error: error.message, }) } } else { next(action) }} 为了方便执行一些额外的操作，此处 nextAction 也可以是一个函数，该函数必须返回一个 action，同时将当前 action 的返回值作为回调传入这个函数，nextAction 执行之后，除了将请求结果作为 result 字段挂在 action 之外，还加入了一个 lastResult 字段保存首次 action 的值。 版本三目前只能支持一级 nextAction，如果要支持多级的话，可以传入数组，数组中可以是一个普通的 action，也可以是返回一个 action 的函数，完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// index.jsimport execAction from './exec-action'import execNextAction from './exec-next-action'import isFunction from './is-function'import isArray from './is-array'export default () =&gt; next =&gt; async action =&gt; { if (action.url || action.nextAction) { try { const result = await execAction(action) next({ result, ...action }) if (action.nextAction) { let nextAction = action.nextAction let lastResult = result let lastAction = action if(isFunction(nextAction)) { nextAction = await nextAction(lastResult, lastAction) await execNextAction(nextAction, lastResult, next) } else if(isArray(nextAction)) { let currentAction for( let i = 0; i &lt; nextAction.length; i++ ) { lastAction = nextAction[i - 1] ? nextAction[i - 1] : lastAction currentAction = isFunction(nextAction[i]) ? await nextAction[i](lastResult, lastAction) : nextAction[i] await execNextAction(currentAction, lastResult, next) } } else { await execNextAction(nextAction, lastResult, next) } } } catch (error) { next({ error: error.message, }) } } else { next(action) }} 123// is-array.jsexport default param =&gt; Array.isArray(param) 123// is-function.jsexport default param =&gt; typeof param === 'function' 1234567891011121314151617181920212223242526272829// request.jsimport reqwest from 'reqwest'export default async opts =&gt; { const defaultOpts = { type: 'json', url: `/routers${opts.url}`, } const finalOpts = { ...opts, ...defaultOpts, } let ret try { ret = await reqwest(finalOpts) return ret } catch (e) { try { ret = JSON.parse(e.response) } catch (e) { ret = e.message } return ret }} 123456789101112131415161718192021222324// exec-action.jsimport request from './request'export default async act =&gt; { if(act.url) { const { code, data, error, } = await request({ url: act.url, method: act.method || 'get', data: act.data || {}, }) if (code !== 0) { throw new Error(error || '未知错误！') } else { return data } }} 12345678910111213// exec-next-action.jsimport execAction from './exec-action'export default async (nextAct, lastResult, next) =&gt; { const result = await execAction(nextAct) next({ result, lastResult, ...nextAct })} 如此这般，便能开心的写页面了~好了，我要发给我妈看看。","link":"/2017/03/18/%E7%BC%96%E5%86%99%20redux%20%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"title":"编写可维护的 JavaScriptの笔记","text":"第一部分主要涉及编程风格，为了让团队的代码看起来如出一人之手；第二部分为编程实践，学到了很多关于 JavaScript 的编程经验；第三部分是关于自动化的，介绍的工具链，在 2016 年底看来已经相当过时。 编写可维护的 JavaScriptの笔记第一部分 基本格式化 建议在每个流程控制语句（比如 if 和 for 语句）之前添加空行 变量命名应当以名词为前缀，而函数命名则以动词为前缀，常用动词约定： 动词 含义 can 返回布尔值 has 返回布尔值 is 返回布尔值 get 返回非布尔值 set 用于保存一个值 常量约定使用大写字母书写，单词之间使用下划线分隔 使用 null 的场景（理解为对象的占位符） a. 初始化一个变量，该变量可能赋值为一个对象 b. 用来和一个已经初始化的变量（可以是也可以不是一个对象）比较 c. 当函数的期望是对象时，用作参数传入 d. 当函数的返回值期望是对象时，用作返回值传出 null == undefined // true typeof null === object // true 注释前留出空行，文件头部除外 不符常理，有意为之的代码最好注释 声明语句之后留出空行 原始包装类型：String、Boolean、Number，原始值本身并不具有对象特性 第二部分 使用单全局变量，将变量统一挂载到一个具体的全局变量上 可在全局对象的第一层级创建对象以作为命名空间 使用函数包装器创建零全局变量场景： 1234(function(win) { let doc = window.document; // your code}(window)); 隔离应用逻辑，例如将事件处理程序封装到一个全局对象中 123456789const MyApp = { handleClick: e =&gt; { this.showPopup(e); }, showPopup: e =&gt; { // 事件逻辑代码 }，}； 不要分发事件对象，仅分发需要的参数 123456789const MyApp = { handleClick: e =&gt; { this.showPopup(e.clientX, e.clientY); }, showPopup: (x, y) =&gt; { // 事件逻辑代码 }，}； 尽量让事件处理程序成为接触到 event 对象的唯一函数，经过4、5、6，使得代码更易于测试 123456789101112const MyApp = { handleClick: e =&gt; { e.preventDefault(); e.stopPropagation(); this.showPopup(e.clientX, e.clientY); }, showPopup: (x, y) =&gt; { // 事件逻辑代码 }，}； frame A 中的对象传入 frame B，则相当于每个 frame 都有一份拷贝，使得： 123456// trueframeAObjInstance instanceof frameAObj//falseframeAObjInstance instanceof frameBObj// 同样适用于 Function 与 Array，因为每个 frame 中都个各自的构造函数 Array.isArray 判断数组 判断某个对象是否存在某个属性值（避免属性假值，不建议 if(obj[‘count’])，使用 in 操作符，或者 hasOwnProperty() 方法 抽离配置数据（硬编码值） 抛出错误就像给自己留下为什么出错的便签 包装器模式和适配器模式唯一的不同是前者创建新接口，后者实现已存在的接口 避免使用特性推断和浏览器推断，酌情使用特性检测和用户代理检测","link":"/2016/11/27/%E7%BC%96%E5%86%99%E5%8F%AF%E7%BB%B4%E6%8A%A4%E7%9A%84%20JavaScript%20%E3%81%AE%E7%AC%94%E8%AE%B0/"},{"title":"编译&amp;构建现代JS应用","text":"给合作部门的客户端同学科普如何《编译&amp;构建现代JS应用》！","link":"/2020/05/30/%E7%BC%96%E8%AF%91&%E6%9E%84%E5%BB%BA%E7%8E%B0%E4%BB%A3JS%E5%BA%94%E7%94%A8/"},{"title":"记一次 Webview Jsbridge 接口封装","text":"由于我司原有的 Jsbridge 方案调用繁琐, 参数多层嵌套, 并在多个 APP 上存在兼容性问题, 引来我司前端开发人员的一致吐槽… 前因客户端开发人员站在自己的角度上, 根据前端需要使用的功能设计出了一套 Jsbridge 交互方案, 该方案虽然能很好地完成 native 与 h5 页面的相互调用对方的函数(方法), 但是由于没有充分考虑到前端开发的使用习惯, 导致该方案难以推广, 作为我司前端的一员, 恰好又被分在了公共组, 觉得十分有必要封装一层对前端开发人员友好的 API. 新旧对比旧的方式 同步接口 1234const data = { &quot;action&quot;: &quot;doLogin&quot;}bridge.send(JSON.stringify(data)); 异步接口 123456const data = { &quot;action&quot;: &quot;isLogin&quot;}bridge.send(JSON.stringify(data), (response) =&gt; { alert(response === 'true');}); 优缺点 优点: 1.接口统一使用bridge.send方式执行 缺点: 1.参数为多层JSON格式, 比较繁琐, 2.参数众多, 容易写错, 3.语义化不足, 调用起来不直观 例如:closeWebView 关闭当前 webview ，并向下一个打开的 webview 传递 message 信息 123456789101112131415161718192021222324//发送参数const data = { &quot;action&quot;: &quot;closeWebView&quot;, &quot;params&quot;: { &quot;message&quot;: { &quot;action&quot;: &quot;prePageClose&quot;, &quot;params&quot;: { &quot;message&quot;: &quot;data form previous page&quot; } } }}bridge.send(JSON.stringify(data));//在上一个 webview 中需要初始化 bridge.init 来监听消息bridge.init((data) =&gt; { data === JSON.stringify({ &quot;action&quot;: &quot;prePageClose&quot;, &quot;params&quot;: { &quot;message&quot;: &quot;data form previous page&quot; } });}); 新的方式 同步 123bridge.doLogin(); 异步 1234567891011jsbridge.isLogin().then((res) =&gt; { if(res) { alert(&quot;已登录!&quot;); } else { alert(&quot;未登录!&quot;); }}).catch((err) =&gt; { alert(err);}); 优缺点 优点: 1. 使用 Promise 实现, 符合前端习惯; 2. 同步的接口也返回 Promise, 方便后续异步化; 3. API 方法名直接体现action, 比较直观; 3. catch函数能处理异常 新的实现方式 native 与 h5交互h5通过如下函数向native发送消息 123web.handleMessageFromJs 无回调api: 直接使用上述函数向native发送消息 有回调api: 除了发送消息外, 还需发送一个 unique id 标记回调函数, 当客户端执行完毕, 便可根据 unique id 执行该回调函数, 并提供超时销毁机制 动态生成 API 通过一个 action 列表, 生成对应的 API 函数, 方便动态地扫描新增的 API, 并添加到 Jsbridge 库中 1234567891011121314151617181920212223242526272829303132333435363738394041424344 // action 列表 const actionList = [ {&quot;action&quot;: &quot;isLogin&quot;, &quot;hasCallback&quot;: true}, {&quot;action&quot;: &quot;doLogin&quot;, &quot;hasCallback&quot;: false}, {&quot;action&quot;: &quot;test&quot;, &quot;hasCallback&quot;: true} ]; // 根据列表生成 API 函数 factory(actionList) { for (let value of actionList) { this[`${value.action}`] = this.generator(value); }} // API 生成器, 根据是否有回调函数, 分别生成同步 API 和 异步 API generator(action) { return function(params) { action['params'] = params; if (!action.hasCallback) { return new Promise((resolve, reject) =&gt; { this.sendAction(action); resolve(); }); } else { return new Promise((resolve, reject) =&gt; { let callbackId = this.generateId(); this.responseCallbackList[callbackId] = (data) =&gt; { clearTimeout(destoryCallback); resolve(data); } this.sendAction(action, callbackId); // 超时销毁回调 let destoryCallback = setTimeout(() =&gt; { delete this.responseCallbackList[callbackId]; reject(new Error('TIME EXCEED')); }, 5000); }); } }} 关于 generator 函数最近函数式编程越来越火, 从某种意义上来说该函数就是传说中的 curry 函数: 它接受一个函数 返回一个只接收一个参数的函数 好屌, 高阶函数有木有!","link":"/2016/10/02/%E8%AE%B0%E4%B8%80%E6%AC%A1%20Webview%20Jsbridge%20%E6%8E%A5%E5%8F%A3%E5%B0%81%E8%A3%85/"},{"title":"记一次难忘的前后端分离","text":"重构公司多年的 JSP 项目，加入 Node.js 层 可维护代码规范接入并使用 eslint 之前改过一个项目，满屏报错啊 属于接了 eslint，但是没按配置文件进行开发，如果这样不接会更好点 es6 module 配合 webpack2 可支持 tree shake 注释与变量命名 仅注释必要的场景：比如不合常理的写法、有很强 background 信息的、步骤性的 1234567// 1const getNextPageUrlAfterCheckPhone = () =&gt; ()const nextPageUrl = getNextPageUrlAfterCheckPhone()// 2const getUrl = () =&gt; () // 验证手机后，返回下一步的页面地址const url = getUrl() // 获取接下来需要跳转的页面地址 代码可能几经人手，大家都改过，逻辑变了，很可能注释却保持不动，最终注释反而影响了阅读代码的人 关于缺憾文档1. 项目对外文档 使用在线的 API 文档中心维护，便于查看 毕竟文档是给别人看的 2. 与后端对接的文档 同样使用在线的 API 文档中心维护，便于查看 接口文档修改会有通知，便于同步 mock 数据可同步到本地 utils 边界 一旦使用了名为 utils 的 componets，不管前期如何明了，到后来都不可避免的越写越臃肿 使用更具体的命名，比如用于表单验证的工具：formValidationUtils，用于 url 操作的工具：URLUtils 样式改动 已将样式的命名统一为小写，连接符为 - 而不是驼峰 原项目的移动端样式可维护性比较高，大部分都按页面级别拆分了 PC 端虽然重构后把独立的样式抽离，不过公共部分的样式实在太大了，且类名短，很难运用批量替换的方式重构 过时的 jQuery 开了历史的倒车，将一些使用 vue 的页面改回 jQuery 了 大量组件都直接依赖了 jQuery，改写困难重重 比如项目中的 vue 版本支持混用 jQuery，而最新版的 vue 直接篡改了 dom 节点，导致 jQuery 的一些 DOM 操作方法失效","link":"/2017/12/15/%E8%AE%B0%E4%B8%80%E6%AC%A1%E9%9A%BE%E5%BF%98%E7%9A%84%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB/"},{"title":"论如何科学地治理老项目的 eslint","text":"行之有效，可行性高的治理方式 背景总有那么些老项目，当老板叫你上去开发需求时，你会有一种不祥的预感，生怕改完一发布，线上就崩了。但是你还是慌兮兮的介入开发，想着开发完马上抽身。这些除了一些贴药膏式的业务逻辑外，让你十分缺乏安全感的就是项目没有接入 eslint，一不小心就是一个低级语法造成 crash；更可怕的是接入了大家都不遵守，想象下编辑器满屏红色错误，黄色警告的画面，此时的你，已经对报错不敏感，以致同流合污。有一个犯罪心理学上的理论可以套用在软件工程中，即『破窗效应』： 此理论认为环境中的不良现象如果被放任存在，会诱使人们仿效，甚至变本加厉。以一幢有少许破窗的建筑为例，如果那些窗不被修理好，可能将会有破坏者破坏更多的窗户。最终他们甚至会闯入建筑内，如果发现无人居住，也许就在那里定居或者纵火。一面墙，如果出现一些涂鸦没有被清洗掉，很快的，墙上就布满了乱七八糟、不堪入目的东西；一条人行道有些许纸屑，不久后就会有更多垃圾，最终人们会视若理所当然地将垃圾顺手丢弃在地上。 因此，随着时间的推移，此类项目会变得愈发难以维护。当然，这种难以维护不仅仅是 eslint 的问题，会有一系列约定和规范的问题，约定和规范制定很简单，但是如何保证这些约定和规范执行才是关键。本文聚焦 eslint 治理，将以一个仍然在持续维护的项目为例，阐述如何治理这种老项目。 现状描述当前存在的 eslint 错误数和告警数： 其中 5697 个错误和 65 个警告可被自动修复，修复后仍剩余 6468 个。经发现，原有项目使用 Airbnb 等规则集，由于该规则十分严格，且本身没有可靠手段阻止不符合规范的代码入库，使得开发更不愿意遵守，错误数有越来越多的趋势，即使自动修复，也仍有 6000+ 的错误。因此选择和制定一个大家都能良好遵守的规则集就很重要，切忌进入『越严格越牛逼』的误区，时刻记住我们的目的是保证代码符合一定的规范，从而避免低级错误。 解决方式最后经过调研和讨论，我们采用如下的规则集： 12345678extends: [ 'standard', 'plugin:@typescript-eslint/recommended', 'plugin:react/recommended', 'plugin:react-native/all', 'prettier', 'prettier/@typescript-eslint', ] 此处我们接入了 prettier 和 lint-staged，前者用于保证同一份代码仅输出一个确定的格式；后者用于保证不会有新的 eslint 错误引入，并对修改过的文件强制校验，从而逐步减少错误数。 结果eslint 默认的结果仅支持文件维度的统计，这在错误数非常多的项目是不适合的，几乎所有文件都有相当数量的错误。此处使用工具 eslint-formatter-stats，使得我们可以从错误类型的维度进行分析。运行结果如下： 接入后未修复的 eslint 结果 10959 个错误和警告 接入并修复的 eslint 结果 837 个错误和警告 分析错误数接入并修复之后，错误数仅有 800+，老板说对，放低点要求，我们 eslint 做得还挺好。 排查错误数量排名靠前的大部分是一些不影响程序功能的，而且为了保证线上代码的稳定性，除了自动修复以外，本次治理尽量不去更改其它错误。因此决定重点看看几个造成影响或可能造成影响的高危错误： no-undef，即使用了未定义的变量，这种错误极有可能造成 crash，目前有 13 处 import-no-unresolved，即导入了不存在的模块，目前有 6 处 react-no-key，即数组元素未使用 key，可能造成卡顿，目前有 4 处 react-native-no-unused-styles，即存在未使用的样式，造成应用体积变大，目前有 45 处 no-dupe-keys 和 no-duplicate-case，即重复定义的对象 key 或者 switch case，增加了不确定性，目前有 7 处 react/no-direct-muttion-state，即直接修改 this.state，有可能导致状态更新不生效，目前有 21 处 此外，诸如变量驼峰，双等号这种错误如果数量巨大，建议改成 warn 级别或者直接关闭，毕竟无关痛痒。经过上述修复之后，剩余的就交给 lint-staged 即可。","link":"/2020/09/28/%E8%AE%BA%E5%A6%82%E4%BD%95%E7%A7%91%E5%AD%A6%E5%9C%B0%E6%B2%BB%E7%90%86%E8%80%81%E9%A1%B9%E7%9B%AE%E7%9A%84%20eslint/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"LifeOS","slug":"LifeOS","link":"/tags/LifeOS/"},{"name":"PKM","slug":"PKM","link":"/tags/PKM/"},{"name":"Periodic-Note","slug":"Periodic-Note","link":"/tags/Periodic-Note/"},{"name":"Task-Management","slug":"Task-Management","link":"/tags/Task-Management/"},{"name":"Goal-Management","slug":"Goal-Management","link":"/tags/Goal-Management/"},{"name":"Time-Management","slug":"Time-Management","link":"/tags/Time-Management/"},{"name":"Javascript","slug":"Javascript","link":"/tags/Javascript/"},{"name":"Canvas","slug":"Canvas","link":"/tags/Canvas/"},{"name":"CSS","slug":"CSS","link":"/tags/CSS/"},{"name":"函数式编程","slug":"函数式编程","link":"/tags/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B/"},{"name":"Monorepo","slug":"Monorepo","link":"/tags/Monorepo/"},{"name":"工程治理","slug":"工程治理","link":"/tags/%E5%B7%A5%E7%A8%8B%E6%B2%BB%E7%90%86/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"知识管理","slug":"知识管理","link":"/tags/%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86/"},{"name":"周期笔记","slug":"周期笔记","link":"/tags/%E5%91%A8%E6%9C%9F%E7%AC%94%E8%AE%B0/"},{"name":"任务管理","slug":"任务管理","link":"/tags/%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86/"},{"name":"目标管理","slug":"目标管理","link":"/tags/%E7%9B%AE%E6%A0%87%E7%AE%A1%E7%90%86/"},{"name":"时间管理","slug":"时间管理","link":"/tags/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/"},{"name":"自学历程","slug":"自学历程","link":"/tags/%E8%87%AA%E5%AD%A6%E5%8E%86%E7%A8%8B/"},{"name":"命令行工具","slug":"命令行工具","link":"/tags/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/"},{"name":"单元测试","slug":"单元测试","link":"/tags/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"前端八股","slug":"前端八股","link":"/tags/%E5%89%8D%E7%AB%AF%E5%85%AB%E8%82%A1/"},{"name":"监控告警","slug":"监控告警","link":"/tags/%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/"},{"name":"私有化部署","slug":"私有化部署","link":"/tags/%E7%A7%81%E6%9C%89%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"name":"Micro Frontend","slug":"Micro-Frontend","link":"/tags/Micro-Frontend/"},{"name":"Bundler","slug":"Bundler","link":"/tags/Bundler/"},{"name":"Diagnostics Tool","slug":"Diagnostics-Tool","link":"/tags/Diagnostics-Tool/"},{"name":"实习历程","slug":"实习历程","link":"/tags/%E5%AE%9E%E4%B9%A0%E5%8E%86%E7%A8%8B/"},{"name":"内存泄漏","slug":"内存泄漏","link":"/tags/%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"name":"分享会","slug":"分享会","link":"/tags/%E5%88%86%E4%BA%AB%E4%BC%9A/"},{"name":"Redux","slug":"Redux","link":"/tags/Redux/"},{"name":"源码解析","slug":"源码解析","link":"/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"},{"name":"可维护性","slug":"可维护性","link":"/tags/%E5%8F%AF%E7%BB%B4%E6%8A%A4%E6%80%A7/"},{"name":"Compiler","slug":"Compiler","link":"/tags/Compiler/"},{"name":"Webview","slug":"Webview","link":"/tags/Webview/"},{"name":"JsBridge","slug":"JsBridge","link":"/tags/JsBridge/"},{"name":"重构","slug":"重构","link":"/tags/%E9%87%8D%E6%9E%84/"},{"name":"前后端分离","slug":"前后端分离","link":"/tags/%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB/"}],"categories":[{"name":"面试","slug":"面试","link":"/categories/%E9%9D%A2%E8%AF%95/"},{"name":"第二大脑","slug":"第二大脑","link":"/categories/%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/"},{"name":"工作记录","slug":"工作记录","link":"/categories/%E5%B7%A5%E4%BD%9C%E8%AE%B0%E5%BD%95/"},{"name":"读书笔记","slug":"读书笔记","link":"/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"工程化","slug":"工程化","link":"/categories/%E5%B7%A5%E7%A8%8B%E5%8C%96/"},{"name":"Node.js 服务端","slug":"Node-js-服务端","link":"/categories/Node-js-%E6%9C%8D%E5%8A%A1%E7%AB%AF/"},{"name":"前端学习","slug":"前端学习","link":"/categories/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"开发工具","slug":"开发工具","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"总结","slug":"总结","link":"/categories/%E6%80%BB%E7%BB%93/"}],"pages":[{"title":"404 Not Found：该页无法显示","text":"","link":"/404.html"},{"title":"","text":"article.article .article-meta { overflow-x: hidden; }","link":"/css/index.css"},{"title":"About","text":"一个热爱工具和规则的 JavaScript 攻城狮！","link":"/about/index.html"}]}